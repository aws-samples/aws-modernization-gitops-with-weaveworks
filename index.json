[
{
	"uri": "/",
	"title": "GitOps on EKS with Weaveworks",
	"tags": [],
	"description": "",
	"content": " Introduction to GitOps on EKS with Weaveworks 15 years ago, Git changed the way software teams collaborate and develop software. For new declarative software systems such as Kubernetes, Git can play a key role in deploying, configuring, updating and managing infrastructure as code. GitOps relies on Git as the single source of truth for declarative infrastructure and applications. With Git at the center of delivery pipelines, developers can make pull requests to accelerate and simplify application deployments and operations tasks to Kubernetes.\nIn these workshop modules Weaveworks will teach and demonstrate:\n The 4 principles of GitOps How to boost stability and reliability in Kubernetes environments How to incorporate a robust security model from the start\n Using GitOps for High Availability and Disaster Recovery on EKS Managing governance, risk and compliance (GRC) for Kubernetes on EKS with GitOps Accelerating Software Development with GitOpos and EKS Managing Machine Learning and Artificial Intelligence models w/GitOps on EKS  Who should take these workshops:  Anyone who has interest in GitOps \u0026amp; EKS Application teams Architects \u0026amp; Developers Technical leads Operations Engineers Infrastructure Teams Technical Business Leaders  Prerequisites  Basic understanding of Kubernetes concepts and architecture Basic understanding of infrastructure and application monitoring Familiarity with basic unix commands  "
},
{
	"uri": "/50_workshop_5_accelerating_sdlc/00_prerequisites.md.html",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": " Prerequisites Before proceeding through this workshop, please ensure that you have installed the following to your Cloud9 environment:\n Cloud9 Workspace Eksctl Kubectl Fluxctl Kustomize AWS IAM Authenticator Add IAM role Workspace IAM  "
},
{
	"uri": "/60_workshop_6_ml/00_prerequisites.md.html",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": " Prerequisites Before proceeding through this workshop, please ensure that you have installed the following to your Cloud9 environment:\n Cloud9 Workspace Eksctl Kubectl Fluxctl Kustomize AWS IAM Authenticator Add IAM role Workspace IAM  "
},
{
	"uri": "/40_workshop_4_hipo-teams/01_prerequisites.html",
	"title": "Workshop Prerequisites",
	"tags": [],
	"description": "",
	"content": " Prerequisites Please note that in order to complete this workshop, you should have:\n Completed Workshop 1: Introduction to GitOps on EKS Setup you environment using Prerequisites for all GitOps Workshops  Once you have completed both of these, please return here and run the following command in the Cloud9 terminal:\naws sts get-caller-identity --query Arn | grep TeamRole -q \u0026amp;\u0026amp; echo \u0026#34;IAM role valid\u0026#34; || echo \u0026#34;IAM role NOT valid\u0026#34; If the IAM role is not valid, DO NOT PROCEED. Go back and confirm the steps on this page.\nIf you are using your own AWS account, you will need permissions to create EKS clusters plus admin rights within your EKS cluster to configure configuration rules and install agents. Ensure you have authority within your organization to do this in your tenant.\n "
},
{
	"uri": "/50_workshop_5_accelerating_sdlc/00_prerequisites.md/01_create_workspace.html",
	"title": "AWS Workshop Portal",
	"tags": [],
	"description": "",
	"content": " Login to AWS Workshop Portal This workshop creates an AWS account, EKS Cluster, ELB and Route 53 environments that will be managed by eksctl. You will need the Participant Hash provided upon entry, and your email address to track your unique session.\nConnect to the portal by clicking the button or browsing to https://dashboard.eventengine.run/. The following screen shows up.\nEnter the provided hash in the text box. The button on the bottom right corner changes to Accept Terms \u0026amp; Login. Click on that button to continue.\nClick on AWS Console on dashboard.\nTake the defaults and click on Open AWS Console. This will open AWS Console in a new browser tab.\nIn the AWS Console, click on Cloud9\nIf a Cloud9 environment has not been set up:\n Click on the Create Environment button Enter a name, such as \u0026ldquo;GitOps Workshop\u0026rdquo;, for this Cloud9 environment Click Next Step Change the Cost saving setting to After one hour and click Next Step Leave the other default values as are, and click Create Environment  If a Cloud9 environment has been set up:\n Click on \u0026ldquo;Open IDE\u0026rdquo;  Next step Once you have completed the step above, you can leave the AWS console open. You can now move to the ekstctl Setup section.\n"
},
{
	"uri": "/60_workshop_6_ml/00_prerequisites.md/01_create_workspace.html",
	"title": "AWS Workshop Portal",
	"tags": [],
	"description": "",
	"content": " Login to AWS Workshop Portal This workshop creates an AWS account, EKS Cluster, ELB and Route 53 environments that will be managed by eksctl. You will need the Participant Hash provided upon entry, and your email address to track your unique session.\nConnect to the portal by clicking the button or browsing to https://dashboard.eventengine.run/. The following screen shows up.\nEnter the provided hash in the text box. The button on the bottom right corner changes to Accept Terms \u0026amp; Login. Click on that button to continue.\nClick on AWS Console on dashboard.\nTake the defaults and click on Open AWS Console. This will open AWS Console in a new browser tab.\nIn the AWS Console, click on Cloud9\nIf a Cloud9 environment has not been set up:\n Click on the Create Environment button Enter a name, such as \u0026ldquo;GitOps Workshop\u0026rdquo;, for this Cloud9 environment Click Next Step Change the Cost saving setting to After one hour and click Next Step Leave the other default values as are, and click Create Environment  If a Cloud9 environment has been set up:\n Click on \u0026ldquo;Open IDE\u0026rdquo;  Next step Once you have completed the step above, you can leave the AWS console open. You can now move to the ekstctl Setup section.\n"
},
{
	"uri": "/60_workshop_6_ml/05_setup.html",
	"title": "Cloud9 Setup",
	"tags": [],
	"description": "",
	"content": " Cloud9 Setup We will be setting up our git client environment on cloud9 and resizing the cloud9 instance filesystem\n"
},
{
	"uri": "/22_workshop_1/10_create_cluster.html",
	"title": "Create a Cluster",
	"tags": [],
	"description": "",
	"content": " eksctl Setup To complete the workshop, you first need to install eksctl. Instructions for doing this can be found at eksctl.io. eksctl (pronounced \u0026ldquo;eks control\u0026rdquo;) is a simple CLI tool for creating clusters on EKS. It is written in Go, uses CloudFormation, was created by Weaveworks and it welcomes contributions from the community.\nIf you are using your own AWS account, you will need permissions to create EKS clusters plus admin rights within your EKS cluster to configure configuration rules and install agents. Ensure you have authority within your organization to do this in your tenant.\n "
},
{
	"uri": "/22_workshop_1/20_gitops_enable.html",
	"title": "GitOps Enable Your Cluster",
	"tags": [],
	"description": "",
	"content": " Getting ready for gitops gitops is a way to do Kubernetes application delivery. It works by using Git as a single source of truth for Kubernetes resources and everything else. With Git at the center of your delivery pipelines, you and your team can make pull requests to accelerate and simplify application deployments and operations tasks to Kubernetes.\n"
},
{
	"uri": "/05_introduction.html",
	"title": "Welcome",
	"tags": [],
	"description": "",
	"content": " GitOps: Modern Operations for Kubernetes GitOps is a standardized workflow for how to deploy, configure, monitor, update and manage Kubernetes, its components and all the applications that run on it.\nGitOps relies on Git as a single source of truth for declarative infrastructure and applications. With Git at the center of delivery pipelines, developers can make pull requests to accelerate and simplify application deployments and operations tasks to Kubernetes.\nGitOps - an operating model for cloud native GitOps can be summarized as these two things:\n An operating model for Kubernetes and other cloud native technologies, providing a set of best practices that unify deployment, management and monitoring for containerized clusters and applications. A path towards a developer experience for managing applications; where end-to-end CICD pipelines and git workflows are applied to both operations, and development.  For further reading we recommend:  Whitepaper: Implementing a Kubernetes Strategy in your Organization Whitepaper: Automating Kubernetes with GitOps Whitepaper: Guide to a product ready Kubernetes cluster  Checklist: Production Readiness for Kubernetes  Case Study: Fidelity Case Study: Mettle Tutorial: A practical guide to GitOps\n Weaveworks is the EKS certified Advanced partner that built eksctl. Founded in 2014, Weaveworks makes it fast and simple for developers and DevOps teams to build and operate powerful containerized applications. Weaveworks minimizes the complexity of operating workloads in Kubernetes by providing automated continuous delivery pipelines, observability and monitoring. One of the first members of the Cloud Native Computing Foundation, Weaveworks also contributes to several open source projects, including Weave Scope, Weave Cortex and Weave Flux.\n  "
},
{
	"uri": "/25_workshop_2_ha-dr/01_prerequisites.html",
	"title": "Workshop Prerequisites",
	"tags": [],
	"description": "",
	"content": " Prerequisites Please note that in order to complete this workshop, you should have completed Prerequisites for all GitOps Workshops. Once you have completed the Introduction to GitOps Workshop, please return to perform this workshop.\nIf you are using your own AWS account, you will need permissions to create EKS clusters plus admin rights within your EKS cluster to configure configuration rules and install agents. Ensure you have authority within your organization to do this in your tenant.\n "
},
{
	"uri": "/30_workshop_03_grc/150_iam-groups/10_intro.html",
	"title": "Kubernetes Authentication",
	"tags": [],
	"description": "",
	"content": "In this workshop we will run through how to map IAM roles to rbac.\nIf you have different teams which needs different kind of cluster access, it would be difficult to manually add or remove access for each EKS clusters you want them to give or remove access from.\nWe can leverage on AWS IAM Groups to easilly add or remove users and give them permission to whole cluster, or just part of it depending on which groups they belong to.\nIn this lesson, we will create 3 IAM roles that we will map to 3 IAM groups.\n"
},
{
	"uri": "/10_aws_prerequisites/10_aws_event.html",
	"title": "At an AWS Event",
	"tags": [],
	"description": "",
	"content": " Only complete this section if you are at an AWS hosted event (such as re:Invent, Kubecon, Immersion Day, or any other event hosted by an AWS employee). If you are running the workshop on your own, goto: Start the workshop on your own.\n Login to AWS Workshop Portal This workshop creates an AWS account, EKS Cluster, ELB and Route 53 environments that will be managed by eksctl. You will need the Participant Hash provided upon entry, and your email address to track your unique session.\nConnect to the portal by clicking the button or browsing to https://dashboard.eventengine.run/. The following screen shows up.\nEnter the provided hash in the text box. The button on the bottom right corner changes to Accept Terms \u0026amp; Login. Click on that button to continue.\nClick on AWS Console on dashboard.\nTake the defaults and click on Open AWS Console. This will open AWS Console in a new browser tab.\nIn the AWS Console, click on Cloud9\nIf a Cloud9 environment has not been set up:\n Click on the Create Environment button Enter a name, such as \u0026ldquo;GitOps Workshop\u0026rdquo;, for this Cloud9 environment Click Next Step Leave the default values unchanged, and click Next Step Again, leave the default values, and click Create Environment  If a Cloud9 environment has been set up:\n Click on \u0026ldquo;Open IDE\u0026rdquo;  Next step Once you have completed the step above, you can leave the AWS console open. You can now move to the ekstctl Setup section.\n"
},
{
	"uri": "/25_workshop_2_ha-dr/40_gitops_enable_clusters/10_checkkubectl.html",
	"title": "Check kubectl",
	"tags": [],
	"description": "",
	"content": "With two terminal windows open, you\u0026rsquo;ll have to be careful when executing commands to ensure you are working with the correct cluster. In each terminal window, verify that you are connecting to the correct cluster by executing:\nkubectl config get-contexts This should produce out similar to this. It is difficult to see because the cluster names and credentials are long. You may have more than one line listed. However, the line that begins with the \u0026ldquo;*\u0026rdquo; is the current context (the cluster kubectl will be connecting to) for the kubectl command.  CURRENT NAME CLUSTER AUTHINFO NAMESPACE * paul.curtis@weave.works@ha-1.us-east-2.eksctl.io ha-1.us-east-2.eksctl.io paul.curtis@weave.works@ha-1.us-east-2.eksctl.io\n In this case, the kubectl command will be executed on the ha-1.us-east-2.eksctl.io cluster.\nCheck in your two terminal windows to ensure that each terminal session is connecting to your two different clusters.\n"
},
{
	"uri": "/100_cleanup/10_cleanup.html",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": " AWS Cleanup In order to prevent charges to your account we recommend cleaning up the infrastructure that was created. If you plan to keep things running so you can examine the workshop a bit more please remember to do the cleanup when you are done. It is very easy to leave things running in an AWS account, forget about it, and then accrue charges.\n In the AWS console, go to CloudFormation, click ModernizationWorkshop-EKS stack and then Delete. You do not need to choose the stacks for the workshop with the Nested tag, they will automatically be deleted as the ModernizationWorkshop-EKS stack is.\nThis process will take 15-30 minutes, to be sure to verify that none of the ModernizationWorkshop-EKS stacks are listed in CloudFormation and you are done.\n Weaveworks Cleanup You have a 15 day trial, so keep using it to monitor and manage your infrastructure and applications.\nHere are some additional resources to checkout:\n Learn more about your tenant and install more OneAgents Add other users to your tenant YouTube Videos More Support resources  "
},
{
	"uri": "/22_workshop_1/10_create_cluster/10-create-cluster.html",
	"title": "Create a Cluster with eksctl",
	"tags": [],
	"description": "",
	"content": "eksctl makes it simple to provision Kubernetes clusters in EKS. For this workshop, we will create a defauklt three node EKS cluster. With eksctl, this is a single command line:\neksctl create cluster --name eksworkshop You will see a number of messages scroll, ending with the kubeconfig message\n [ℹ] eksctl version 0.18.0 [ℹ] using region us-west-2 ..... [ℹ] deploying stack \"eksctl-unique-unicorn-1588181335-nodegroup-ng-d61e2b06\" [✔] all EKS cluster resources for \"unique-unicorn-1588181335\" have been created [✔] saved kubeconfig as \"/home/ec2-user/.kube/config\"  You should now be able to view your cluster with kubectl\nkubectl get nodes"
},
{
	"uri": "/50_workshop_5_accelerating_sdlc/10_create_cluster.html",
	"title": "Create Cluster",
	"tags": [],
	"description": "",
	"content": " Create the EKS Cluster We will be creating our EKS cluster using eksctl.\neksctl is the official CLI tool for creating clusters on Amazon EKS. It is an open source project created by Weaveworks in partnership with AWS.\n"
},
{
	"uri": "/60_workshop_6_ml/10_create_cluster.html",
	"title": "Create Cluster",
	"tags": [],
	"description": "",
	"content": " Create the EKS Cluster We will be creating our EKS cluster using eksctl.\neksctl is the official CLI tool for creating clusters on Amazon EKS. It is an open source project created by Weaveworks in partnership with AWS.\n"
},
{
	"uri": "/40_workshop_4_hipo-teams/30_gitops_enable_clusters/10_create-github.html",
	"title": "Create GitHub Repository",
	"tags": [],
	"description": "",
	"content": "We will be using a GitHub repo to declare the desired state of our EKS cluster. This repo will eventually include your workload manifests, HelmReleases etc.\nYou can start with an empty repository and push that to GitHub, or use the one you intend to deploy to the cluster. As a suggestion create a blank public repo on GitHub called my-eks-config and make sure you tick Initialize this Repository with a README:\nFor instructions on how to create a repository on GitHub follow these steps..\nIn the Cloud9 environment, a user SSH key pair is not automatically created. Use these instructions to create an SSH key pair for use as your git repository deploy keys.\n You will also want to add your ssh. Instructions to do so are here.\nTo create a new ssh key, run the following. Substitute your github email address where specified.\nssh-keygen -t rsa -b 4096 -C \u0026#34;your_email@example.com\u0026#34; Start the ssh-agent in the background.\neval \u0026#34;$(ssh-agent -s)\u0026#34; Add your SSH private key to the ssh-agent and store your passphrase in the keychain. If you created your key with a different name, or if you are adding an existing key that has a different name, replace id_rsa in the command with the name of your private key file.\nssh-add ~/.ssh/id_rsa Then run:\ncat ~/.ssh/id_rsa.pub You can then create a deploy key in your repository by going to: Settings \u0026gt; Deploy keys. Click on Add deploy key, and check Allow write access. Then paste your public key and click Add key. This will allow us to clone this repo to our Cloud9 environment.\n"
},
{
	"uri": "/40_workshop_4_hipo-teams/40_install-app-mesh/10_create_namespace.html",
	"title": "Create the namespace",
	"tags": [],
	"description": "",
	"content": "All the AWS App Mesh components will be live in the appmesh-system namespace in our EKS cluster. We will be managing all namespaces via GitOps. This includes the actual creation of the namespace and also the resources contained with it.\nEnsure you have cloned your repository into your Cloud9 environment.\n# Make sure you are in your environment directory cd ~/environment # Clone your repo git clone git@github.com:yourname/my-eks-config.git # Change into the repos directory cd my-eks-config Create a folder in your repo called namespaces and then create a file within that folder called appmesh-system.yaml.\nPaste the contents of the following into the new file:\n--- apiVersion: v1 kind: Namespace metadata: labels: name: appmesh-system name: appmesh-system Your repository structure should be:\n. ├── namespaces │ └── appmesh-system.yaml └── README.md Add and then commit the appmesh-system.yaml file and push the the changes to your GitHub repo.\nFlux will now see that the desired state has changed in Git and will apply the namespace to our cluster. This will take up to 1 minute to apply.\nCheck that that the namespace has been created by running the following command:\nkubectl get ns You should see the appmesh-system namespace listed. For example:\nNAME STATUS AGE appmesh-system Active 26s default Active 76m flux Active 2m18s kube-node-lease Active 76m kube-public Active 76m kube-system Active 76m If you don\u0026rsquo;t see it listed, give it a few minutes longer and try again.\n"
},
{
	"uri": "/40_workshop_4_hipo-teams/50_install_container_insights/10_create_namespace.html",
	"title": "Create the namespace",
	"tags": [],
	"description": "",
	"content": "Create a file within the namespaces that folder called amazon-cloudwatch.yaml.\nPaste the contents of the following into the new file:\n--- apiVersion: v1 kind: Namespace metadata: labels: name: amazon-cloudwatch name: amazon-cloudwatch Your repository structure should be:\n. ├── appmesh-system │ ├── appmesh-controller.yaml │ ├── appmesh-inject.yaml │ ├── appmesh-prometheus.yaml │ └── crds.yaml ├── namespaces │ ├── amazon-cloudwatch.yaml │ └── appmesh-system.yaml └── README.md Add and then commit the amazon-cloudwatch.yaml file and push the the changes to your GitHub repo.\nFlux will now see that the desired state has changed in Git and will apply the namespace to our cluster. This will take up to 1 minute to apply.\nCheck that that the namespace has been created by running the following command:\nkubectl get ns You should see the amazon-cloudwatch namespace listed. For example:\nNAME STATUS AGE amazon-cloudwatch Active 5s appmesh-system Active 3h2m default Active 4h18m flux Active 3h4m kube-node-lease Active 4h18m kube-public Active 4h18m kube-system Active 4h18m If you don\u0026rsquo;t see it listed, give it a few minutes longer and try again.\n"
},
{
	"uri": "/25_workshop_2_ha-dr/50_add_yamls/10_alb_ingress.html",
	"title": "Install an Application Load Balancer Ingress Controller (RBAC)",
	"tags": [],
	"description": "",
	"content": "This manifest creates the role based authorization (RBAC) for the ingress controller. Copy and paste the following into a file called alb-rbac.yaml in your repository:\n--- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: app.kubernetes.io/name: alb-ingress-controller name: alb-ingress-controller rules: - apiGroups: - \u0026#34;\u0026#34; - extensions resources: - configmaps - endpoints - events - ingresses - ingresses/status - services verbs: - create - get - list - update - watch - patch - apiGroups: - \u0026#34;\u0026#34; - extensions resources: - nodes - pods - secrets - services - namespaces verbs: - get - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: labels: app.kubernetes.io/name: alb-ingress-controller name: alb-ingress-controller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: alb-ingress-controller subjects: - kind: ServiceAccount name: alb-ingress-controller namespace: kube-system --- apiVersion: v1 kind: ServiceAccount metadata: labels: app.kubernetes.io/name: alb-ingress-controller name: alb-ingress-controller namespace: kube-system ..."
},
{
	"uri": "/50_workshop_5_accelerating_sdlc/00_prerequisites.md/10_install_eksctl.html",
	"title": "Install eksctl",
	"tags": [],
	"description": "",
	"content": "For this workshop you will use eksctl. Once you install eksctl, you will be ready to get started.\nAt the terminal command prompt, enter the following two commands:\ncurl --silent --location \u0026#34;https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\u0026#34; | tar xz -C /tmp \u0026amp;\u0026amp; \\ sudo mv /tmp/eksctl /usr/local/bin This will install eksctl in your Cloud9 environment. To test to make sure the command is installed properly, execute the command:\neksctl get cluster You should get a \u0026ldquo;No clusters found\u0026rdquo; message.\neksctl version For the current workshops, we will be using eksctl 0.19-rc.1 or newer. Please verify the version, as some features are only available in the 0.19 version of eksctl\n"
},
{
	"uri": "/60_workshop_6_ml/00_prerequisites.md/10_install_eksctl.html",
	"title": "Install eksctl",
	"tags": [],
	"description": "",
	"content": "For this workshop you will use eksctl. Once you install eksctl, you will be ready to get started.\nAt the terminal command prompt, enter the following two commands:\ncurl --silent --location \u0026#34;https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\u0026#34; | tar xz -C /tmp \u0026amp;\u0026amp; \\ sudo mv /tmp/eksctl /usr/local/bin This will install eksctl in your Cloud9 environment. To test to make sure the command is installed properly, execute the command:\neksctl get cluster You should get a \u0026ldquo;No clusters found\u0026rdquo; message.\neksctl version For the current workshops, we will be using eksctl 0.19-rc.1 or newer. Please verify the version, as some features are only available in the 0.19 version of eksctl\n"
},
{
	"uri": "/40_workshop_4_hipo-teams/60_install-pod-info/10_about_podinfo.html",
	"title": "Introducing Podinfo",
	"tags": [],
	"description": "",
	"content": "Podinfo is a tiny web application made with Go that showcases best practices of running microservices in Kubernetes. It provides a simple web interface that allows you verify the operation of ingresses and/or service meshes. Podinfo was written by Stefan Prodan of Weaveworks, and is freely available on github. There are many options, and URLs are available to provide a wealth of information. More information on Podinfo is available here.\nFor our workshop, we will be using podinfo to show how to use App Mesh and the observability it gives us.\nWe will be deploying podinfo in the following logical architecture:\n"
},
{
	"uri": "/20_weaveworks_prerequisites/10_workspace.html",
	"title": "Open Workspace",
	"tags": [],
	"description": "",
	"content": " AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. It includes a code editor, debugger, and terminal. Cloud9 comes prepackaged with essential tools for popular programming languages, including JavaScript, Python, PHP, and more, so you don’t need to install files or configure your development machine to start new projects.\n Ad blockers, javascript disablers, and tracking blockers should be disabled for the cloud9 domain, or connecting to the workspace might be impacted. Cloud9 requires third-party-cookies. You can whitelist the specific domains.\n  In the AWS console, navigate to the cloud9 service and click Create Environment For the Name type in WorkshopIDE and click Next Step Leave all the defaults and click Next Step once again On the Review page click Create Environment  Your environment should deploy and you should see something similar to the image below\n"
},
{
	"uri": "/70_workshop_7_multicloud/10_prerequisites/10_workspace.html",
	"title": "Open Workspace",
	"tags": [],
	"description": "",
	"content": " AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. It includes a code editor, debugger, and terminal. Cloud9 comes prepackaged with essential tools for popular programming languages, including JavaScript, Python, PHP, and more, so you don’t need to install files or configure your development machine to start new projects.\n Ad blockers, javascript disablers, and tracking blockers should be disabled for the cloud9 domain, or connecting to the workspace might be impacted. Cloud9 requires third-party-cookies. You can whitelist the specific domains.\n  In the AWS console, navigate to the cloud9 service and click Create Environment For the Name type in WorkshopIDE and click Next Step Leave all the defaults and click Next Step once again On the Review page click Create Environment  Your environment should deploy and you should see something similar to the image below\nShow Hidden Files  Show hidden files in Cloud9 environment by going to Settings \u0026gt; User Settings \u0026gt; Tree and Go Panel, then set the Hidden File Pattern to *.pyc, __pycache__  "
},
{
	"uri": "/70_workshop_7_multicloud/10_prerequisites.html",
	"title": "Prerequisites for Workshop",
	"tags": [],
	"description": "",
	"content": " Setup All the Required Tools \u0026amp; Permissions To complete these workshops, you will first need to install eksctl. Instructions for doing this can be found at eksctl.io. eksctl (pronounced \u0026ldquo;eks control\u0026rdquo;) is a simple CLI tool for creating clusters on EKS. It is written in Go, uses CloudFormation, was created by Weaveworks and it welcomes contributions from the community.\nIf you are using your own AWS account, you will need permissions to create EKS clusters plus admin rights within your EKS cluster to configure configuration rules and install agents. Ensure you have authority within your organization to do this in your tenant.\n "
},
{
	"uri": "/10_aws_prerequisites.html",
	"title": "Start the Workshop",
	"tags": [],
	"description": "",
	"content": " Getting Started To start the workshop, you first need an AWS account and to provision the resources for the workshop. Please choose the link from one of the following options:\nOption 1: Running the workshop on your own (in your own AWS account)\nOption 2: Attending an AWS hosted event (using AWS provided hashes)\n"
},
{
	"uri": "/05_introduction/10_introduction.html",
	"title": "Why EKS?",
	"tags": [],
	"description": "",
	"content": " Modern cloud environments need a different approach to observability Conventional application performance monitoring (APM) emerged when software was mostly monolithic and update cycles were measured in years, not days. Manual instrumentation and performance baselining, though cumbersome, were once adequate—particularly since fault patterns were generally known and well understood.\nAs monoliths get replaced by cloud-native applications, that are rapidly growing in size, traditional monitoring approaches are no longer enough. Rather than instrumenting for a predefined set of problems, enterprises need complete visibility into every single component of these dynamically scaling microservice environments. This includes multi-cloud infrastructures, container orchestration systems like Kubernetes, service meshes, functions-as-a-service and polyglot container payloads.\nSuch applications are more complex and unpredictable than ever. System health problems are rarely well understood from the outset and IT teams spend a significant amount of time manually solving problems and putting out fires after the fact. The challenge with modern cloud environments is to address the unknown unknowns—the kind of unique glitches that have never occurred in the past. These are the growing pains that the concept of observability attempts to tackle.\nKubernetes Kubernetes is open source software that allows you to deploy and manage containerized applications at scale.\nKubernetes manages clusters of compute instances and runs containers on those instances with processes for deployment, maintenance, and scaling. Using Kubernetes, you can run any type of containerized applications using the same toolset on-premises and in the cloud.\nYou can read more about Kubernetes here\nAmazon Elastic Kubernetes Service AWS makes it easy to run Kubernetes. You can choose to manage Kubernetes infrastructure yourself with Amazon EC2 or get an automatically provisioned, managed Kubernetes control plane with Amazon EKS. Either way, you get powerful, community-backed integrations to AWS services like VPC, IAM, and service discovery as well as the security, scalability, and high-availability of AWS.\nAmazon EKS runs Kubernetes control plane instances across multiple Availability Zones to ensure high availability. Amazon EKS automatically detects and replaces unhealthy control plane instances, and it provides automated version upgrades and patching for them.\nAmazon EKS is also integrated with many AWS services to provide scalability and security for your applications, including the following:\n Elastic Load Balancing for load distribution IAM for authentication Amazon VPC for isolation Amazon ECR for container images  For this workshop you are going to use Amazon EKS managed service that makes it easy for you to run Kubernetes on AWS without needing to stand up or maintain your own Kubernetes control plane.\n "
},
{
	"uri": "/30_workshop_03_grc/010_prerequisites.html",
	"title": "Workshop Prerequsites",
	"tags": [],
	"description": "",
	"content": " Prerequisites Please note that in order to complete this workshop, you should have completed Workshop 1: Introduction to GitOps on EKS. Once you have completed the Introduction to GitOps Workshop, please return to perform this workshop.\nIf you are using your own AWS account, you will need permissions to create EKS clusters plus admin rights within your EKS cluster to configure configuration rules and install agents. Ensure you have authority within your organization to do this in your tenant.\n "
},
{
	"uri": "/25_workshop_2_ha-dr/50_add_yamls/11_alb_ingress.html",
	"title": "Install an Application Load Balancer Ingress Controller",
	"tags": [],
	"description": "",
	"content": "This manifest actually creates the ingress controller. Copy and paste the following into a file called alb-ingress.yaml in each of your terminal windows. We will be creating two files, one for each cluster, as we must place the actual cluster name in this file. Replace the XXXXXXXX on the cluster-name option with the name of your cluster.\nOnce you have edited these two file, use kubectl apply -f alb-ingress.yaml in each of your terminal sessions to install the ingress controllers in each of your clusters. In more advanced GitOps usage, the different cluster names would be handled with a template, and the manifest templating tool like helm or using kustomize templates. For simplicity, we\u0026rsquo;re doing this installation directly.\n# Application Load Balancer (ALB) Ingress Controller Deployment Manifest. # This manifest details sensible defaults for deploying an ALB Ingress Controller. # GitHub: https://github.com/kubernetes-sigs/aws-alb-ingress-controller apiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/name: alb-ingress-controller name: alb-ingress-controller # Namespace the ALB Ingress Controller should run in. Does not impact which # namespaces it\u0026#39;s able to resolve ingress resource for. For limiting ingress # namespace scope, see --watch-namespace. namespace: kube-system spec: selector: matchLabels: app.kubernetes.io/name: alb-ingress-controller template: metadata: labels: app.kubernetes.io/name: alb-ingress-controller spec: containers: - name: alb-ingress-controller args: # Limit the namespace where this ALB Ingress Controller deployment will # resolve ingress resources. If left commented, all namespaces are used. # - --watch-namespace=your-k8s-namespace # Setting the ingress-class flag below ensures that only ingress resources with the # annotation kubernetes.io/ingress.class: \u0026#34;alb\u0026#34; are respected by the controller. You may # choose any class you\u0026#39;d like for this controller to respect. - --ingress-class=alb # REQUIRED # Name of your cluster. Used when naming resources created # by the ALB Ingress Controller, providing distinction between # clusters. - --cluster-name=XXXXXXXXXX # AWS VPC ID this ingress controller will use to create AWS resources. # If unspecified, it will be discovered from ec2metadata. # - --aws-vpc-id=vpc-xxxxxx # AWS region this ingress controller will operate in. # If unspecified, it will be discovered from ec2metadata. # List of regions: http://docs.aws.amazon.com/general/latest/gr/rande.html#vpc_region # - --aws-region=us-west-1 # Enables logging on all outbound requests sent to the AWS API. # If logging is desired, set to true. # - --aws-api-debug # Maximum number of times to retry the aws calls. # defaults to 10. # - --aws-max-retries=10 # env: # AWS key id for authenticating with the AWS API. # This is only here for examples. It\u0026#39;s recommended you instead use # a project like kube2iam for granting access. #- name: AWS_ACCESS_KEY_ID # value: KEYVALUE # AWS key secret for authenticating with the AWS API. # This is only here for examples. It\u0026#39;s recommended you instead use # a project like kube2iam for granting access. #- name: AWS_SECRET_ACCESS_KEY # value: SECRETVALUE # Repository location of the ALB Ingress Controller. image: docker.io/amazon/aws-alb-ingress-controller:v1.1.4 serviceAccountName: alb-ingress-controller"
},
{
	"uri": "/20_weaveworks_prerequisites/11_install_eksctl.html",
	"title": "Install eksctl",
	"tags": [],
	"description": "",
	"content": "For this workshop you will use a eksctl. Once you install eksctl, you will be ready to get started.\nAt the terminal command prompt, enter the following two commands:\ncurl --silent --location \u0026#34;https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\u0026#34; | tar xz -C /tmpsudo mv /tmp/eksctl /usr/local/bin This will install eksctl in your Cloud9 environment. To test to make sure the command is installed properly, execute the command:\neksctl get cluster You should get a \u0026ldquo;No clusters found\u0026rdquo; message.\neksctl version For the current workshops, we will be using eksctl 0.19-rc.1 or newer. Please verify the version, as some features are only available in the 0.19 version of eksctl\n"
},
{
	"uri": "/70_workshop_7_multicloud/10_prerequisites/11_install_eksctl.html",
	"title": "Install eksctl",
	"tags": [],
	"description": "",
	"content": "For this workshop you will use a eksctl. Once you install eksctl, you will be ready to get started.\nAt the terminal command prompt, enter the following two commands:\ncurl --silent --location \u0026#34;https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\u0026#34; | tar xz -C /tmp sudo mv /tmp/eksctl /usr/local/bin This will install eksctl in your Cloud9 environment. To test to make sure the command is installed properly, execute the command:\neksctl get cluster You should get a pre-installed cluster. If you see \u0026ldquo;No clusters found\u0026rdquo; message, contact us to provision a new cluster.\neksctl version For the current workshops, we will be using eksctl 0.19-rc.1 or newer. Please verify the version, as some features are only available in the 0.19 version of eksctl\n"
},
{
	"uri": "/20_weaveworks_prerequisites/12_install_kubectl.html",
	"title": "Install kubectl",
	"tags": [],
	"description": "",
	"content": "The Kubernetes command-line tool, kubectl, allows you to run commands against Kubernetes clusters. You can use kubectl to deploy applications, inspect and manage cluster resources, and view logs.\nAt the terminal command prompt, enter the following two commands:\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectlchmod +x ./kubectlsudo mv ./kubectl /usr/local/bin/kubectl This will install kubectl in your Cloud9 environment. To test to make sure the command is installed properly, execute the command:\nkubectl version --client You should see the kubectl version message.\n"
},
{
	"uri": "/70_workshop_7_multicloud/10_prerequisites/12_install_kubectl.html",
	"title": "Install kubectl",
	"tags": [],
	"description": "",
	"content": "The Kubernetes command-line tool, kubectl, allows you to run commands against Kubernetes clusters. You can use kubectl to deploy applications, inspect and manage cluster resources, and view logs.\nAt the terminal command prompt, enter the following two commands:\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl chmod +x ./kubectl sudo mv ./kubectl /usr/local/bin/kubectl This will install kubectl in your Cloud9 environment. To test to make sure the command is installed properly, execute the command:\nkubectl version --client You should see the kubectl version message.\n"
},
{
	"uri": "/20_weaveworks_prerequisites/13_install_helm.html",
	"title": "Install helm",
	"tags": [],
	"description": "",
	"content": "Helm describes itself as a \u0026lsquo;package manager for kubernetes\u0026rsquo; and can be used to deploy resources to Kubernetes.\nYou package your application as a chart which can contain templated files (usually Kubernetes resources) and default configuration values too use when rendering the template. Charts are reusable and values can be overriden for specific environments.\nWe are using Helm v3 in the workshops. So there is no tiller. If you are using your own environment and not the workshops Cloud9 environment and you have Helm v2 the commands should work ok.\n At the terminal command prompt, enter the following command to download Helm:\ncurl -L https://get.helm.sh/helm-v3.1.2-linux-amd64.tar.gz -o helm.tar.gz We then want to extract helm, make it executable and copy to a location in the path:\ntar xvfz helm.tar.gz linux-amd64/helm chmod +x ./linux-amd64/helm sudo mv ./linux-amd64/helm /usr/local/bin This will install helm in your Cloud9 environment. To test to make sure the command is installed properly, execute the command:\nhelm version You should see the helm version message.\nBefore we leave we should tidy up by executing these commands:\nrm -rf ./linux-amd64 rm helm.tar.gz"
},
{
	"uri": "/70_workshop_7_multicloud/10_prerequisites/13_install_helm.html",
	"title": "Install helm",
	"tags": [],
	"description": "",
	"content": "Helm describes itself as a \u0026lsquo;package manager for kubernetes\u0026rsquo; and can be used to deploy resources to Kubernetes.\nYou package your application as a chart which can contain templated files (usually Kubernetes resources) and default configuration values too use when rendering the template. Charts are reusable and values can be overriden for specific environments.\nWe are using Helm v3 in the workshops. So there is no tiller. If you are using your own environment and not the workshops Cloud9 environment and you have Helm v2 the commands should work ok.\n At the terminal command prompt, enter the following command to download Helm:\ncurl -L https://get.helm.sh/helm-v3.1.2-linux-amd64.tar.gz -o helm.tar.gz tar xvfz helm.tar.gz linux-amd64/helm chmod +x ./linux-amd64/helm sudo mv ./linux-amd64/helm /usr/local/bin rm -rf ./linux-amd64 rm helm.tar.gz This will install helm in your Cloud9 environment. To test to make sure the command is installed properly, execute the command:\nhelm version You should see the helm version message.\n"
},
{
	"uri": "/20_weaveworks_prerequisites/14_install_fluxctl.html",
	"title": "Install fluxctl",
	"tags": [],
	"description": "",
	"content": " Install Fluxctl Fluxctl is a CLI tool that is able to talk to Weave Flux.\nInstall by running this command:\ncurl -Ls https://fluxcd.io/install | sh \u0026amp;\u0026amp; \\ sudo mv $HOME/.fluxcd/bin/fluxctl /usr/local/bin/fluxctl Verify the installation:\nfluxctl version"
},
{
	"uri": "/70_workshop_7_multicloud/10_prerequisites/14_install_fluxctl.html",
	"title": "Install fluxctl",
	"tags": [],
	"description": "",
	"content": " Install Fluxctl Fluxctl is a CLI tool that is able to talk to Weave Flux.\nInstall by running this command:\ncurl -Ls https://fluxcd.io/install | sh \u0026amp;\u0026amp; \\ sudo mv $HOME/.fluxcd/bin/fluxctl /usr/local/bin/fluxctl Verify the installation:\nfluxctl version"
},
{
	"uri": "/50_workshop_5_accelerating_sdlc/15_deployment_strategies.html",
	"title": "Deployment Strategies",
	"tags": [],
	"description": "",
	"content": " Deployment Strategies In this section, we will visit some deployment strategies. They include:\n Recreate Rolling Update Blue / Green Canary  Today\u0026rsquo;s workshop will focus on Canary deployments.\n"
},
{
	"uri": "/20_weaveworks_prerequisites/15_install_kustomize.html",
	"title": "Install kustomize",
	"tags": [],
	"description": "",
	"content": " Install Kustomize Kustomize lets you customize raw, template-free YAML files for multiple purposes, leaving the original YAML untouched and usable as is.\nInstall kustomize for Linux:\ncurl --silent --location --remote-name \\ \u0026#34;https://github.com/kubernetes-sigs/kustomize/releases/download/kustomize/v3.2.3/kustomize_kustomize.v3.2.3_linux_amd64\u0026#34; \u0026amp;\u0026amp; \\ chmod a+x kustomize_kustomize.v3.2.3_linux_amd64 \u0026amp;\u0026amp; \\ sudo mv kustomize_kustomize.v3.2.3_linux_amd64 /usr/local/bin/kustomize Verify the install with:\nkustomize version"
},
{
	"uri": "/70_workshop_7_multicloud/10_prerequisites/15_install_kustomize.html",
	"title": "Install kustomize",
	"tags": [],
	"description": "",
	"content": " Install Kustomize Kustomize lets you customize raw, template-free YAML files for multiple purposes, leaving the original YAML untouched and usable as is.\nInstall kustomize for Linux:\ncurl --silent --location --remote-name \\ \u0026#34;https://github.com/kubernetes-sigs/kustomize/releases/download/kustomize/v3.2.3/kustomize_kustomize.v3.2.3_linux_amd64\u0026#34; \u0026amp;\u0026amp; \\ chmod a+x kustomize_kustomize.v3.2.3_linux_amd64 \u0026amp;\u0026amp; \\ sudo mv kustomize_kustomize.v3.2.3_linux_amd64 /usr/local/bin/kustomize Verify the install with:\nkustomize version"
},
{
	"uri": "/70_workshop_7_multicloud/15_setup_git_repo.html",
	"title": "Set up git repo",
	"tags": [],
	"description": "",
	"content": " Setup git repo  Create new repo Github called aws-gitops-multicloud and turn on \u0026ldquo;Initialize this repository with a README\u0026rdquo; Clone the repo on your machine Create two directories in the repo: mkdir flux-mgmt and mkdir flux-ec2  Github SSH key  Add ssh key to your Github account  On Cloud9, run: ssh-keygen -t rsa -b 4096 and accept defaults Run cat ~/.ssh/id_rsa.pub and copy the output Go to Github \u0026gt; Settings \u0026gt; SSH Keys \u0026gt; New SSH key and paste your public key   Clone Workshop Repo to use examples We\u0026rsquo;ll use some examples from the below repo\n Clone the workshop repo\ngit clone git@github.com:saada/gitops-cluster-management.git  "
},
{
	"uri": "/20_weaveworks_prerequisites/16_install_aws-iam.html",
	"title": "Install AWS IAM",
	"tags": [],
	"description": "",
	"content": "Amazon EKS uses IAM to provide authentication to your Kubernetes cluster through the AWS IAM authenticator for Kubernetes. You can configure the stock kubectl client to work with Amazon EKS by installing the AWS IAM authenticator for Kubernetes and modifying your kubectl configuration file to use it for authentication.\nTo install aws-iam-authenticator on Cloud9\nDownload the Amazon EKS-vended aws-iam-authenticator binary from Amazon S3:\ncurl -o aws-iam-authenticator https://amazon-eks.s3.us-west-2.amazonaws.com/1.15.10/2020-02-22/bin/linux/amd64/aws-iam-authenticator Apply execute permissions to the binary:\nchmod +x ./aws-iam-authenticator And move it into a common directory:\nsudo mv ./aws-iam-authenticator /usr/local/bin Test that the aws-iam-authenticator binary works:\naws-iam-authenticator help"
},
{
	"uri": "/70_workshop_7_multicloud/10_prerequisites/16_install_aws-iam.html",
	"title": "Install AWS IAM",
	"tags": [],
	"description": "",
	"content": "Amazon EKS uses IAM to provide authentication to your Kubernetes cluster through the AWS IAM authenticator for Kubernetes. You can configure the stock kubectl client to work with Amazon EKS by installing the AWS IAM authenticator for Kubernetes and modifying your kubectl configuration file to use it for authentication.\nTo install aws-iam-authenticator on Cloud9\nDownload the Amazon EKS-vended aws-iam-authenticator binary from Amazon S3:\ncurl -o aws-iam-authenticator https://amazon-eks.s3.us-west-2.amazonaws.com/1.15.10/2020-02-22/bin/linux/amd64/aws-iam-authenticator chmod +x ./aws-iam-authenticator sudo mv ./aws-iam-authenticator /usr/local/bin Test that the aws-iam-authenticator binary works:\naws-iam-authenticator help"
},
{
	"uri": "/20_weaveworks_prerequisites/17_add_the_role.html",
	"title": "Add the Required IAM Role",
	"tags": [],
	"description": "",
	"content": "Creating and using EKS clusters in AWS requires specific IAM roles for the user creating and accessing EKS clusters.\n Switch to the AWS Console (You can open the console from the \u0026ldquo;Team Dashboard\u0026rdquo;) Under Services, select EC2 Click Running Instances Select the instance named \u0026ldquo;aws-cloud9-\u0026hellip;\u0026rdquo; by clicking the check box to the left of the name On Actions pull down, select Instance Settings -\u0026gt; Attach/Replace IAM Role In the IAM role pull down, select TeamRoleInstanceProfile To the right, click Apply  "
},
{
	"uri": "/70_workshop_7_multicloud/10_prerequisites/17_add_the_role.html",
	"title": "Add the Required IAM Role",
	"tags": [],
	"description": "",
	"content": "Creating and using EKS clusters in AWS requires specific IAM roles for the user creating and accessing EKS clusters.\n Switch to the AWS Console (You can open the console from the \u0026ldquo;Team Dashboard\u0026rdquo;) Under Services, select EC2 Click Running Instances Select the instance named \u0026ldquo;aws-cloud9-\u0026hellip;\u0026rdquo; by clicking the check box to the left of the name On Actions pull down, select Instance Settings -\u0026gt; Attach/Replace IAM Role In the IAM role pull down, select modernization-admin To the right, click Apply  "
},
{
	"uri": "/70_workshop_7_multicloud/10_prerequisites/18_workspaceiam.html",
	"title": "Update IAM settings for your Workspace",
	"tags": [],
	"description": "",
	"content": " Cloud9 normally manages IAM credentials dynamically. This isn\u0026rsquo;t currently compatible with the EKS IAM authentication, so we will disable it and rely on the IAM role instead.\n  Return to your workspace and click the gear icon (in top right corner), or click to open a new tab and choose \u0026ldquo;Open Preferences\u0026rdquo; Select AWS SETTINGS Turn off AWS managed temporary credentials Close the Preferences tab   To ensure temporary credentials aren\u0026rsquo;t already in place we will also remove any existing credentials file:\nrm -vf ${HOME}/.aws/credentials We should configure our aws cli with our current region as default.\nIf you are at an AWS event, ask your instructor which AWS region to use.\n # install jq sudo yum install jq export ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account) export AWS_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r \u0026#39;.region\u0026#39;) Check if AWS_REGION is set to desired region\ntest -n \u0026#34;$AWS_REGION\u0026#34; \u0026amp;\u0026amp; echo AWS_REGION is \u0026#34;$AWS_REGION\u0026#34; || echo AWS_REGION is not set Let\u0026rsquo;s save these into bash_profile\necho \u0026#34;export ACCOUNT_ID=${ACCOUNT_ID}\u0026#34; | tee -a ~/.bash_profile echo \u0026#34;export AWS_REGION=${AWS_REGION}\u0026#34; | tee -a ~/.bash_profile aws configure set default.region ${AWS_REGION} aws configure get default.region Validate the IAM role Use the GetCallerIdentity CLI command to validate that the Cloud9 IDE is using the correct IAM role.\nJust run this to try it out yourself.\naws sts get-caller-identity Here is a script that will validate you have the right role.\naws sts get-caller-identity --query Arn | grep modernization-admin -q \u0026amp;\u0026amp; echo \u0026#34;IAM role valid\u0026#34; || echo \u0026#34;IAM role NOT valid\u0026#34; If the IAM role is not valid, DO NOT PROCEED. Go back and confirm the steps on this page.\n"
},
{
	"uri": "/70_workshop_7_multicloud/10_prerequisites/19_direnv.html",
	"title": "Install direnv",
	"tags": [],
	"description": "",
	"content": "direnv takes care of exporting environment variables automatically based on the directory we\u0026rsquo;re in.\nTo install direnv, run the following commands.\ncurl -sfL https://direnv.net/install.sh | bash echo \u0026#34;eval \u0026#39;$(direnv hook bash)\u0026#39;\u0026#34; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc"
},
{
	"uri": "/30_workshop_03_grc/150_iam-groups/20_create-iam-roles.html",
	"title": "Create IAM Roles",
	"tags": [],
	"description": "",
	"content": "We are going to create 3 roles:\n a k8sAdmin role which will have adminx rights in our EKS cluster a k8sDev role which will gives access to developers namespace in our EKS cluster a k8sInteg role which will gives access to integration namespace our EKS cluster  Create the roles:\nACCOUNT_ID=$(aws sts get-caller-identity --output text --query \u0026#39;Account\u0026#39;) POLICY=$(echo -n \u0026#39;{\u0026#34;Version\u0026#34;:\u0026#34;2012-10-17\u0026#34;,\u0026#34;Statement\u0026#34;:[{\u0026#34;Effect\u0026#34;:\u0026#34;Allow\u0026#34;,\u0026#34;Principal\u0026#34;:{\u0026#34;AWS\u0026#34;:\u0026#34;arn:aws:iam::\u0026#39;; echo -n \u0026#34;$ACCOUNT_ID\u0026#34;; echo -n \u0026#39;:root\u0026#34;},\u0026#34;Action\u0026#34;:\u0026#34;sts:AssumeRole\u0026#34;,\u0026#34;Condition\u0026#34;:{}}]}\u0026#39;) echo ACCOUNT_ID=$ACCOUNT_ID echo POLICY=$POLICY aws iam create-role \\ --role-name k8sAdmin \\ --description \u0026#34;Kubernetes administrator role (for AWS IAM Authenticator for Kubernetes).\u0026#34; \\ --assume-role-policy-document \u0026#34;$POLICY\u0026#34; \\ --output text \\ --query \u0026#39;Role.Arn\u0026#39; aws iam create-role \\ --role-name k8sDev \\ --description \u0026#34;Kubernetes developer role (for AWS IAM Authenticator for Kubernetes).\u0026#34; \\ --assume-role-policy-document \u0026#34;$POLICY\u0026#34; \\ --output text \\ --query \u0026#39;Role.Arn\u0026#39; aws iam create-role \\ --role-name k8sInteg \\ --description \u0026#34;Kubernetes role for integration namespace in quick cluster.\u0026#34; \\ --assume-role-policy-document \u0026#34;$POLICY\u0026#34; \\ --output text \\ --query \u0026#39;Role.Arn\u0026#39; In this example, the assume-role-policy allows the root account to assume the role. We are going to allow specific groups to also be able to assume thoses roles.\nCheck the official documentation for more information.\n Because the above roles are only used to authenticate within the k8s cluster, they don\u0026rsquo;t necessary need to have AWS permissions. We will only uses them to allow some IAM groups to assume this role in order to have acess to EKS kubernetes cluster.\n"
},
{
	"uri": "/30_workshop_03_grc/125_create_policy_templates/20_creating-contraint-template.html",
	"title": "Create Constraint Templates",
	"tags": [],
	"description": "",
	"content": " We are going to create 2 templates:\n allowed-repos this will determine what image repos can ben used in production require-labels - this policy requires that namespaces must have labels that match a regex value  Let\u0026rsquo;s download some samples for the above Run the following commands to download some policy template examples to your local git repo:\nmkdir opa/templatescurl https://raw.githubusercontent.com/open-policy-agent/gatekeeper/master/library/general/allowedrepos/template.yaml -o opa/templates/allowed-repos.yamlcurl https://github.com/open-policy-agent/gatekeeper/blob/master/library/general/requiredlabels/template.yaml -o opa/templates/require-labels.yaml Explore the require-labels template Lets understand what\u0026rsquo;s happening and what each part of this template is doing.\nIf we look at require-labels.yaml\nWe can see this is a ConstraintTemplate object. If you remember this crd was created when we first deplopyed opa-gatekeeper.\nOkay, the metadata has the name of the template which is standard stuff, let\u0026rsquo;s look at the next part, we can see inside the spec that this template is going to create another crd!!\nspec: crd: spec: names: kind: K8sRequiredLabels validation: # Schema for the `parameters` field openAPIV3Schema: properties: message: type: string labels: type: array items: type: object properties: key: type: string allowedRegex: type: string Remember, this is only a template, so that means that on its own, its not going to do anything, it needs to be consumed by something else. So every constraint template needs to create constraint objects so it can be used (and it does this by creating crd\u0026rsquo;s). Hopefully that makes sense but don\u0026rsquo;t worry we will show some examples shortly.\n The above example shows the outline for the crd, in this case it\u0026rsquo;s called K8sRequiredLabels it also contains the parameters of the constraint, in this case its labels which is an array of items you can add (key + allowedRegex).\nThere is also a message string which can be used add different messages to the template.\nThis allows us to create simple constraints with just a few parameters and re-use this template.\nThe next part of the constraints template is targets. This is where the rego code (policy rules) are added..\nTargets You can only have 1 target per constraint\nThis contains the rego policy you should use to define what rules are applied.\nLet\u0026rsquo;s take a look:\ntargets: - target: admission.k8s.gatekeeper.sh rego: | package k8srequiredlabels get_message(parameters, _default) = msg { not parameters.message msg := _default } get_message(parameters, _default) = msg { msg := parameters.message } violation[{\u0026#34;msg\u0026#34;: msg, \u0026#34;details\u0026#34;: {\u0026#34;missing_labels\u0026#34;: missing}}] { provided := {label | input.review.object.metadata.labels[label]} required := {label | label := input.parameters.labels[_].key} missing := required - provided count(missing) \u0026gt; 0 def_msg := sprintf(\u0026#34;you must provide labels: %v\u0026#34;, [missing]) msg := get_message(input.parameters, def_msg) } violation[{\u0026#34;msg\u0026#34;: msg}] { value := input.review.object.metadata.labels[key] expected := input.parameters.labels[_] expected.key == key # do not match if allowedRegex is not defined, or is an empty string expected.allowedRegex != \u0026#34;\u0026#34; not re_match(expected.allowedRegex, value) def_msg := sprintf(\u0026#34;Label \u0026lt;%v: %v\u0026gt; does not satisfy allowed regex: %v\u0026#34;, [key, value, expected.allowedRegex]) msg := get_message(input.parameters, def_msg) } As we can see in the rego policy, we\u0026rsquo;ve set 2 violations, one flags if required labels are missing, the other flags up if the label exists but does not match the regex we specified in the contraint.\nThings to note - when configuring rules you can add as many and configure them how you want but you must have the following format on at least one of them for this to work and flag as a violation:\n violation[{\u0026#34;msg\u0026#34;: msg, \u0026#34;details\u0026#34;: {}}] { # rule body } As we discussed earlier, this is just a contraint template, its not a constraint and it does nothing when deployed on it\u0026rsquo;s own.\nExplore the allowed-repos template The format for this look svery similar, we have metadata and name, then inside the spec we are defining a crd called K8sAllowedRepos.\nThe accepted parameters are an array of repos which include item values.\napiVersion: templates.gatekeeper.sh/v1beta1 kind: ConstraintTemplate metadata: name: k8sallowedrepos spec: crd: spec: names: kind: K8sAllowedRepos validation: # Schema for the `parameters` field openAPIV3Schema: properties: repos: type: array items: type: string Next let\u0026rsquo;s look at the targets:\ntargets: - target: admission.k8s.gatekeeper.sh rego: | package k8sallowedrepos violation[{\u0026#34;msg\u0026#34;: msg}] { container := input.review.object.spec.containers[_] satisfied := [good | repo = input.parameters.repos[_] ; good = startswith(container.image, repo)] not any(satisfied) msg := sprintf(\u0026#34;container \u0026lt;%v\u0026gt; has an invalid image repo \u0026lt;%v\u0026gt;, allowed repos are %v\u0026#34;, [container.name, container.image, input.parameters.repos]) } violation[{\u0026#34;msg\u0026#34;: msg}] { container := input.review.object.spec.initContainers[_] satisfied := [good | repo = input.parameters.repos[_] ; good = startswith(container.image, repo)] not any(satisfied) msg := sprintf(\u0026#34;container \u0026lt;%v\u0026gt; has an invalid image repo \u0026lt;%v\u0026gt;, allowed repos are %v\u0026#34;, [container.name, container.image, input.parameters.repos]) } For this we\u0026rsquo;ve created 2 rule violations. Bot of which checks the provided container image and compares the beginning of it to the repos in our parameters.\nIf the image we use does not match what we have in our contraint, then it will trigger a violation.\nLets check this in to git and next we can look at constraints and start using these templates.\ngit add opa/templatesgit commit -m \u0026#34;adding opa templates\u0026#34;git push"
},
{
	"uri": "/30_workshop_03_grc/140_test-policy-contraints/20_deploy-test-apps.html",
	"title": "Deploy Test Apps",
	"tags": [],
	"description": "",
	"content": " Now that we havwe defined our contraint templates and deployed some contraints, let\u0026rsquo;s see if they work!\nTest image repos in prod namespace Earlier we define that all pods in the production namespace must only use images from xxxx repo. Let\u0026rsquo;s deploy a pod using a different unauthorized repo.\nHere\u0026rsquo;s an example:\napiVersion: v1 kind: Pod metadata: name: opa namespace: production labels: owner: me.agilebank.demo spec: containers: - name: opa image: openpolicyagent/opa:0.9.2 args: - \u0026#34;run\u0026#34; - \u0026#34;--server\u0026#34; - \u0026#34;--addr=localhost:8080\u0026#34; resources: limits: cpu: \u0026#34;100m\u0026#34; memory: \u0026#34;30Mi\u0026#34; Let\u0026rsquo;s add this to our git repo\nmkdir example-apps curl https://weaveworks-gitops.awsworkshop.io/30_workshop_03_grc/deploy.files/pod-unauthorized-repo-example.yaml -o example-apps/pod-unauthorized-repo.yaml git add example-apps/pod-unauthorized-repo.yaml git commit -m \u0026#34;adding pod to test allowed repos\u0026#34; git push Check what happened to the pod, run the following:\nkubectl get pods opa -n production You should see it does not get deployed.\nIf we explore flux logs we should be able to see what happened and get the following error:\n Error from server ([denied by prod-repo-is-openpolicyagent] container  has an invalid image repo , allowed repos are [\"only-this-repo\"]): error when creating \"pod-unauthorized-repo.yaml\": admission webhook \"validation.gatekeeper.sh\" denied the request: [denied by prod-repo-is-openpolicyagent] container  has an invalid image repo , allowed repos are [\"only-this-repo\"]  To get the logs run:\nkubectl logs \u0026lt;flux-pod\u0026gt; -n fluxcd If you run the following command you will get the same error:\nkubectl create -f ./example-apps/pod-unauthorized-repo.yaml Lets test namespaces Download this manifest and try to create this namespace:\nmkdir namespaces curl https://weaveworks-gitops.awsworkshop.io/30_workshop_03_grc/deploy.files/bad-namespace-example.yaml -o namespaces/bad-namespace.yaml It should look like this:\napiVersion: v1 kind: Namespace metadata: name: test-namespace Lets run this manually for now so we can see it fail:\nkubectl create -f ./namespaces/bad-namespace.yaml  Error from server ([denied by all-must-have-owner] All namespaces must have an `owner` label that points to your company username): error when creating \"bad-namespace.yaml\": admission webhook \"validation.gatekeeper.sh\" denied the request: [denied by all-must-have-owner] All namespaces must have an `owner` label that points to your company username  Let\u0026rsquo;s make sure our policy allows namespaces to be created when the rules are met. Edit the manifest with the following:\napiVersion: v1 kind: Namespace metadata: name: test-namespace labels: owner: testuser.agilebank.demo Now let\u0026rsquo;s check this in to git:\ngit add \u0026#34;namespaces/bad-namespace.yaml\u0026#34; git commit -m \u0026#34;fixing bad namespace with label\u0026#34; git push You see that this has nwo been created.\n namespace/test-namespace created  Summary We\u0026rsquo;ve proven now that the policies we\u0026rsquo;ve defined through OPA Gatekeeper Contraint templates have worked.\n We created contraint templates which then created our contraint crds K8sAllowedRepos and K8sRequiredLabels.\n This allowed us to declaritively created K8sAllowedRepos and K8sRequiredLabels objects and parameterise them. We can create more adn add different values and point to different namespaces or even different resources such as services, secrets etc\n  "
},
{
	"uri": "/30_workshop_03_grc/120_deploy_opa_gatekeeper/20_intro-to-opa.html",
	"title": "Intro to OPA",
	"tags": [],
	"description": "",
	"content": " What is OPA? Open Policy Agent (OPA) is an open source, general-purpose policy engine that enables unified, context-aware policy enforcement across the entire stack.\nIt allows you to express policies in a high level, declaritive way using rego.\nHere\u0026rsquo;s a link which you can run through to get a ful understanding on how opa and rego code works: OPA\nOPA is platform/service agnostic, it can be used in your jenkins pipelines, terraform, kubernetes and much more\u0026hellip;\nThis is great as it gives us a unified way to enforce policy as code across all of our stacks.\nOPA in Kubernetes Admission Controllers Kubernetes uses built in Admission Controllers to intercept requests sent to the api server to validate or even mutate the request before they persist in etcd.\nSome examples of this are:\n NamespaceLifecycle - this prevents objects being created in namespaces when they are set to be deleted, also deletes all objects from namespaces when they are set to be deleted.\n PodSecurityPolicy - applied a security plicy to all pods in the cluster outside of the control plane. If you don\u0026rsquo;t have a policy and this is enabled, then no pods can be created!!\n  The downside of using PodSecurityPolicy is that you can only have 1 thwta applied to everything, maybe you would like to have different policies for different namespaces?\nKubernetes has 2 special admission controllers: MutatingAdmissionWebhook and ValidatingAdmissionWebhook.\nThis allows us to decouple Admission Control from the api server and create our own so any time certain requests are made to the api server, we can now intercept them.\nOPA Gatekeeper This is where OPA Gatekeeper comes in to play.\nWe want to be able to intercept Create \u0026amp; Update requests sent to the api server and validate them on a set of given rules.\nWhen we deploy OPA Gatekeeper, we create a ValidatingWebhookConfiguration which will intercept the requests and send them to the gatekeeper-controler, this will then query against a set of defined rules.\nWe will dicsuss later on how we define these rules.\nOverview of OPA Gatekeeper Architecture  Configure Replication - this is for auditing what has already been deployed to check for pre-existing violations Authz Webhook - this is used by the api server to query opa for authz decisions AdmissionController - this is the ValidatingAdmissionWebhook, we set up a webhook config to leverage this controller Policy/Template crd\u0026rsquo;s - the opa gatekeeper-controller will watch for these objects which wil define what actions the api server will take (see above)  Reference docs OPA Gatekeeper\n"
},
{
	"uri": "/05_introduction/20_about_weaveworks.html",
	"title": "About Weaveworks",
	"tags": [],
	"description": "",
	"content": " Weaveworks Weaveworks makes it fast and simple for developers and DevOps teams to build and operate powerful containerized applications. We minimize the complexity of operating workloads in Kubernetes by providing automated continuous delivery pipelines, observability and monitoring.\nWeaveworks’ mission is to minimize complexities in operating workloads and provide a developer centric operating model for cloud native applications. Our goal is to drive cloud native transformation for your developers and portability, flexibility, choice and stability for your business.\nOne of the first members of the Cloud Native Computing Foundation, Weaveworks also contributes to several open source projects, including Weave Net, Weave Scope, Weave Cortex, Weave Flux, Firekube and Flagger.\nWeaveworks and AWS Weaveworks is an APN Advanced Technology Partner and AWS Containers Competency Partner. In addition Weaveworks is the EKS certified Advanced partner that built eksctl.\nWeaveworks Pioneered GitOps Learn more about GitOps, a set of modern best practices for deploying and managing applications with cloud native tools and cloud services.\nDon’t know where to start? Our experienced team of Customer Reliability Engineers can provide the highest level of care, advice and coaching. Our expertise is rooted in working and supporting live production systems at scale. Get started with our EKS + GitOps Quickstart package.\nFor further reading we recommend:  Whitepaper: Implementing a Kubernetes Strategy in your Organization Whitepaper: Automating Kubernetes with GitOps Whitepaper: Guide to a product ready Kubernetes cluster  Checklist: Production Readiness for Kubernetes  Case Study: Fidelity Case Study: Mettle  For hands on instructions, we recommend: Tutorial: A practical guide to GitOps \n"
},
{
	"uri": "/40_workshop_4_hipo-teams/20_create-eks-cluster.html",
	"title": "Create EKS Cluster",
	"tags": [],
	"description": "",
	"content": " Create EKS Cluster For this exercise we will need to create an EKS cluster. We will be using the Cloud9 terminal window to do this.\nNot all AWS regions have all EKS capabilities enabled. The config file below assumes you are running the workshop from us-west-2 (Oregon). If you are using a different region make sure you replace us-west-2 in the config with your region.\n Create a new file in the root of your environment called cluster.yaml and paste in the following as the contents:\napiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: gitopsworkshop region: us-west-2 nodeGroups: - name: ng-1 instanceType: m5.large desiredCapacity: 1 iam: withAddonPolicies: albIngress: true appMesh: true xRay: true cloudWatch: true In the terminal window, execute the following:\n# make sure we are in the root environment folder cd ~/environment # Create the cluster eksctl create cluster -f cluster.yaml This command will take around 10-15 minutes to create our cluster, so now would be a good time to take a break or ask a question.\nWhen the command completes check that you can access the cluster using kubectl by executing the following command in the terminal window:\nkubectl get nodes You should see 2 nodes listed.\nIf you are using your own AWS account, you will need permissions to create EKS clusters plus admin rights within your EKS cluster to configure configuration rules and install agents. Ensure you have authority within your organization to do this in your tenant.\n "
},
{
	"uri": "/30_workshop_03_grc/130_create_policy_contraints/creating-contraints.html",
	"title": "Create OPA Constraints",
	"tags": [],
	"description": "",
	"content": "Now that we have our ContraintsTemplate configured and deployed into the cluster, we can now start creating the constraints.\nGoing back to our templates, we defined a crd called K8sRequiredLabels with a set of fields and values we could use.\nHere\u0026rsquo;s an example of what we could do with this:\napiVersion: constraints.gatekeeper.sh/v1beta1 kind: K8sRequiredLabels metadata: name: all-must-have-owner spec: match: kinds: - apiGroups: [\u0026#34;\u0026#34;] kinds: [\u0026#34;Namespace\u0026#34;] parameters: message: \u0026#34;All namespaces must have an `owner` label that points to your company username\u0026#34; labels: - key: owner allowedRegex: \u0026#34;^[a-zA-Z]+.agilebank.demo$\u0026#34; Looking at spec.match We are saying only apply this constraint to Namespace objects.\nNote as well the fields we defined previously kind: K8sRequiredLabels, parameters. This is where we are able to pass unique values in to here.\n In this case, we\u0026rsquo;ve added a sepcific message for this contraint as well as the labels we require whenver a namespace is created or updated and the regex of that label.\nLet\u0026rsquo;s take a look at anoother example :\napiVersion: constraints.gatekeeper.sh/v1beta1 kind: K8sAllowedRepos metadata: name: prod-repo-is-openpolicyagent spec: match: kinds: - apiGroups: [\u0026#34;\u0026#34;] kinds: [\u0026#34;Pod\u0026#34;] namespaces: - \u0026#34;production\u0026#34; parameters: repos: - \u0026#34;only-this-repo\u0026#34; Remember we created a template to only allow certain repos based on a given set of rules.\nThe above shows an exampe of how we can use this in our cluster.\nIn this case we have set this to apply to all pods in the production namespace and we have added a parameter.repos of \u0026ldquo;only this repo\u0026rdquo; - feel free to changes this to test it out.\nLet\u0026rsquo;s grab these example templates and check them in to git!\nmkdir opa/contraintscurl https://weaveworks-gitops.awsworkshop.io/30_workshop_03_grc/deploy.files/allowed-repos-policy.yaml -o opa/constraints/allowed-repos.yamlcurl https://weaveworks-gitops.awsworkshop.io/30_workshop_03_grc/deploy.files/require-labels-policy.yaml -o opa/constraints/require-labels.yaml Once you ar happy these have downloaded, lets check them in to git:\ngit add \u0026#34;opa/contraints/\u0026#34;git commit -m \u0026#34;adding test constraints\u0026#34;git push"
},
{
	"uri": "/40_workshop_4_hipo-teams/60_install-pod-info/20_create_namespace.html",
	"title": "Create the namespace",
	"tags": [],
	"description": "",
	"content": "All the PodInfo components will be live in the apps namespace in our EKS cluster. We will be managing all namespaces via GitOps. This includes the actual creation of the namespace and also the resources contained with it.\nCreate a file within the namespaces folder called apps.yaml.\nPaste the contents of the following into the new file:\n--- apiVersion: v1 kind: Namespace metadata: labels: name: apps appmesh.k8s.aws/sidecarInjectorWebhook: enabled name: apps Notice that the namespace has included a label called appmesh.k8s.aws/sidecarInjectorWebhook with a value of enabled. This tells the App Mesh Injector that pods created in this namespace should have the App Mesh sidecar automatically injected into them.\n Your repository structure should be:\n. ├── amazon-cloudwatch │ ├── cwagent-fluentd-quickstart.yaml │ └── cwagent-prometheus-eks.yaml ├── appmesh-system │ ├── appmesh-controller.yaml │ ├── appmesh-inject.yaml │ ├── appmesh-prometheus.yaml │ └── crds.yaml ├── namespaces │ ├── amazon-cloudwatch.yaml │ ├── appmesh-system.yaml │ └── apps.yaml └── README.md Add and then commit the apps.yaml file and push the the changes to your GitHub repo.\nFlux will now see that the desired state has changed in Git and will apply the namespace to our cluster. This will take up to 1 minute to apply.\nCheck that that the namespace has been created by running the following command:\nkubectl get ns You should see the apps namespace listed. For example:\nNAME STATUS AGE amazon-cloudwatch Active 25m appmesh-system Active 25m apps Active 1s default Active 84m flux Active 33m kube-node-lease Active 84m kube-public Active 84m kube-system Active 84m If you don\u0026rsquo;t see it listed given a few minutes longer and try again.\n"
},
{
	"uri": "/25_workshop_2_ha-dr/20_create_two_clusters.html",
	"title": "Create Two EKS Clusters",
	"tags": [],
	"description": "",
	"content": " Create Two EKS Clusters In order to make this exercise easier, you should use two terminal windows. In Cloud9, you can open a second terminal window by clicking the \u0026ldquo;+\u0026rdquo; icon on the tab bar.\nIn each terminal, we are going to use the eksctl command to create a cluster, and give them specific names. I suggest simple names, like \u0026ldquo;east\u0026rdquo; and \u0026ldquo;west\u0026rdquo; or something similar. Optionally, you can add the -r region_id option to the command line in order to place the cluster in a specific AWS region.\nNot all AWS regions have all EKS capabilities enabled\n In the first terminal window, execute:\neksctl create cluster -n ha-dr-east --alb-ingress-access and in the second terminal window, execute:\neksctl create cluster -n ha-dr-west --alb-ingress-access By running these eksctl create cluster commands, we will:\n Create two m5.large worker nodes; we have found that this instance type suits most use-cases and is good value for the cost Use the official AWS EKS AMI Create the cluster in the us-west-2 region Create a dedicated VPC Use a static AMI resolver Enable full access for alb-ingress-controller  As we covered in the webinar and as most of you know, EKS solves some major pain points with managing Kubernetes. The onus of managing Kubernetes upgrades and patching is taken up by AWS, with zero downtime upgrades. Another benefit that EKS gives you is High Availability in a regional cluster. EKS runs the Kubernetes management infrastructure across multiple Availability Zones, and detects and replaces unhealthy control plane nodes.\nThe clusters typically take around 10-15 minutes to create, so we\u0026rsquo;ll move on to setting up our git repositories and have a checkin once we have set them up.\nWhen they have completed, you will have two node EKS clusters to work with.\nIf you are using your own AWS account, you will need permissions to create EKS clusters plus admin rights within your EKS cluster to configure configuration rules and install agents. Ensure you have authority within your organization to do this in your tenant.\n "
},
{
	"uri": "/50_workshop_5_accelerating_sdlc/20_enable_gitops.html",
	"title": "Enable GitOps",
	"tags": [],
	"description": "",
	"content": " Create Workshop Git Repository A GitOps workflow requires several components, including a Git repository that acts as the source of truth for your Kubernetes resources.\nWe will be using eksctl enable repo on our newly-built cluster to install GitOps operator components.\n"
},
{
	"uri": "/60_workshop_6_ml/20_enable_gitops.html",
	"title": "Enable GitOps",
	"tags": [],
	"description": "",
	"content": " Create Workshop Git Repository A GitOps workflow requires several components, including a Git repository that acts as the source of truth for your Kubernetes resources.\nWe will be using eksctl enable repo on our newly-built cluster to install GitOps operator components.\n"
},
{
	"uri": "/40_workshop_4_hipo-teams/30_gitops_enable_clusters/30_enable_gitops.html",
	"title": "Enable Your Cluster for GitOps",
	"tags": [],
	"description": "",
	"content": " To enable GitOps we will be installing the following to our EKS cluster:\n Flux Flux Helm Operator with Helm v3 support  In other workshops we have used eksctl enable repo to install these. However, for this workshop we will be using Helm to install them both as we want access to additional configuration options (specifically reduce the polling period).\n Base Setup Both Flux and Flux Helm Operator will reside in their own namespace. Run the following command to create the namespace:\nkubectl create ns flux Both Flux and Helm Operator are via helm charts from the Flux Chart Repository. Tell Helm about this repository by running the following command:\nhelm repo add fluxcd https://charts.fluxcd.io Flux Install Let\u0026rsquo;s install Flux using its Helm Chart making sure you replace yourname/my-eks-config.git with your repo details:\nhelm upgrade -i flux fluxcd/flux --wait --namespace flux --set git.url=git@github.com:yourname/my-eks-config.git --set git.pollInterval=1m Let us go through the specified arguments one by one:\n \u0026ndash;git-url: this points to a Git URL where the configuration for your cluster will be stored. This will contain config for the workloads and infrastructure later on. This should be the GitHub repo you created earlier \u0026ndash;git-pollInterval: indicates how often Flux should try check the desired state in Git.  There are more arguments and options, please refer to the Flux documentation which details all the flags.\nWhen Flux is installed you will see a message similar to this:\nNAME: flux LAST DEPLOYED: Wed May 20 09:50:15 2020 NAMESPACE: flux STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Get the Git deploy key by either (a) running kubectl -n flux logs deployment/flux | grep identity.pub | cut -d \u0026#39;\u0026#34;\u0026#39; -f2 or by (b) installing fluxctl through https://docs.fluxcd.io/en/latest/references/fluxctl#installing-fluxctl and running: fluxctl identity --k8s-fwd-ns flux As indicated by the message we need to add the Flux deploy key to our GitHub repo. Run the following command:\nkubectl -n flux logs deployment/flux | grep identity.pub | cut -d \u0026#39;\u0026#34;\u0026#39; -f2 Copy the output from this command (which will start with ssh-rsa).\nGo to the Settings for your git repo and then click Deploy keys.\nClick the Add deploy button, enter a title of Workshop, paste the contents of the key into Key box and make sure you tick Allow write acess:\nClick Add key.\nHelm Operator Install Install the CRDs that are needed for HelmRelease:\nkubectl apply -f https://raw.githubusercontent.com/fluxcd/helm-operator/master/deploy/crds.yaml And then install Helm Operator using the helm chart:\nhelm upgrade -i helm-operator fluxcd/helm-operator --wait --namespace flux --set git.ssh.secretName=flux-git-deploy --set helm.versions=v3"
},
{
	"uri": "/40_workshop_4_hipo-teams/40_install-app-mesh/20_install_appmesh_crds.html",
	"title": "Install App Mesh CRDs",
	"tags": [],
	"description": "",
	"content": "For every namespace in our EKS cluster we will create a folder in our repo that will contain the declarations of every resource we want to live in that namespace. This becomes the desired state of that namespace.\nThe controller requires CRDs to be installed. We have a couple of options to do this:\n Apply the CRDs directly (using kubectl) Apply via GitOps by adding the CRDs to our repo  As we are using GitOps we will be going with option 2!\nFirstly create a appmesh-system folder in our repo:\n. ├── appmesh-system ├── namespaces │ └── appmesh-system.yaml └── README.md Now run the following commands to download the App Mesh CRDs into the new folder:\ncurl https://raw.githubusercontent.com/aws/eks-charts/v0.0.19/stable/appmesh-controller/crds/crds.yaml -o appmesh-system/crds.yaml Your folder structure should look like this now:\n. ├── appmesh-system │ └── crds.yaml ├── namespaces │ └── appmesh-system.yaml └── README.md Add and then commit the crds.yaml file and push the the changes to your GitHub repo.\nFlux will now see that the desired state of the appmesh-system namespace has changed in Git and will apply the CRDs to our cluster. This will take up to 1 minute to apply.\nCheck that that the CRDs has been created by running the following command:\nkubectl get crds You should see 3 App Mesh CRDs installed:\nNAME CREATED AT eniconfigs.crd.k8s.amazonaws.com 2020-05-20T10:45:46Z helmreleases.helm.fluxcd.io 2020-05-20T12:01:29Z meshes.appmesh.k8s.aws 2020-05-20T12:58:31Z virtualnodes.appmesh.k8s.aws 2020-05-20T12:58:31Z virtualservices.appmesh.k8s.aws 2020-05-20T12:58:32Z"
},
{
	"uri": "/70_workshop_7_multicloud/10_prerequisites/20_clusterctl.html",
	"title": "Install clusterctl",
	"tags": [],
	"description": "",
	"content": "The CluserAPI (CAPI) is a project of SIG Cluster Lifecycle to bring declarative, Kubernetes-style APIs to cluster creation, configuration, and management.\ncluserctl is the cli that we\u0026rsquo;ll use to bootstrap the CAPI controllers.\ncurl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/v0.3.6/clusterctl-linux-amd64 -o clusterctl chmod +x ./clusterctl sudo mv ./clusterctl /usr/local/bin/clusterctl Validate that it\u0026rsquo;s working\nclusterctl version"
},
{
	"uri": "/40_workshop_4_hipo-teams/50_install_container_insights/20_install_container_insights.html",
	"title": "Install Container Insights",
	"tags": [],
	"description": "",
	"content": "As every namespace in our EKS cluster has a folder in our repo that contains the declarations of the resources we want to live in that namespace, we will need a new folder for the amazon-cloudwatch namespace.\nCreate a amazon-cloudwatch folder in our repo:\n. ├── amazon-cloudwatch ├── appmesh-system │ ├── appmesh-controller.yaml │ ├── appmesh-inject.yaml │ ├── appmesh-prometheus.yaml │ └── crds.yaml ├── namespaces │ ├── amazon-cloudwatch.yaml │ └── appmesh-system.yaml └── README.md Now run the following commands to download the CloudWatch Agent resources into the new folder:\nThe command below assumes you are running the workshop from us-west-2 (Oregon). If you are using a different region make sure you replace us-west-2 in the command with your region.\n # Download container insights curl https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/daemonset/container-insights-monitoring/quickstart/cwagent-fluentd-quickstart.yaml | sed \u0026#34;s/{{cluster_name}}/gitopsworkshop/;s/{{region_name}}/us-west-2/\u0026#34; \u0026gt; amazon-cloudwatch/cwagent-fluentd-quickstart.yaml # Download the CloudWatch Agent for prometheus curl https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/prometheus-beta/k8s-deployment-manifest-templates/deployment-mode/service/cwagent-prometheus/prometheus-eks.yaml \u0026gt; amazon-cloudwatch/cwagent-prometheus-eks.yaml Your folder structure should look like this now:\n. ├── amazon-cloudwatch │ ├── cwagent-fluentd-quickstart.yaml │ └── cwagent-prometheus-eks.yaml ├── appmesh-system │ ├── appmesh-controller.yaml │ ├── appmesh-inject.yaml │ ├── appmesh-prometheus.yaml │ └── crds.yaml ├── namespaces │ ├── amazon-cloudwatch.yaml │ └── appmesh-system.yaml └── README.md Edit the cwagent-fluentd-quickstart.yaml and cwagent-prometheus-eks.yaml to delete the following (which should appear at the beginning of each file):\n# create amazon-cloudwatch namespace apiVersion: v1 kind: Namespace metadata: name: amazon-cloudwatch labels: name: amazon-cloudwatch --- Add and then commit the 2 files and push the the changes to your GitHub repo.\nFlux will now see that the desired state of the amazon-cloudwatch namespace has changed in Git and will apply the resources to our cluster. This will take up to 1 minute to apply.\nCheck that that the CRDs has been created by running the following command:\nkubectl get pods -n amazon-cloudwatch You should see the agent running:\nNAME READY STATUS RESTARTS AGE cwagent-prometheus-75dfcd47d7-h24pd 1/1 Running 0 21s"
},
{
	"uri": "/50_workshop_5_accelerating_sdlc/00_prerequisites.md/20_install_kubectl.html",
	"title": "Install kubectl",
	"tags": [],
	"description": "",
	"content": "The Kubernetes command-line tool, kubectl, allows you to run commands against Kubernetes clusters. You can use kubectl to deploy applications, inspect and manage cluster resources, and view logs.\nAt the terminal command prompt, enter the following two commands:\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl \u0026amp;\u0026amp; \\ chmod +x ./kubectl \u0026amp;\u0026amp; \\ sudo mv ./kubectl /usr/local/bin/kubectl This will install kubectl in your Cloud9 environment. To test to make sure the command is installed properly, execute the command:\nkubectl version --client You should see the kubectl version message.\n"
},
{
	"uri": "/60_workshop_6_ml/00_prerequisites.md/20_install_kubectl.html",
	"title": "Install kubectl",
	"tags": [],
	"description": "",
	"content": "The Kubernetes command-line tool, kubectl, allows you to run commands against Kubernetes clusters. You can use kubectl to deploy applications, inspect and manage cluster resources, and view logs.\nAt the terminal command prompt, enter the following two commands:\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl \u0026amp;\u0026amp; \\ chmod +x ./kubectl \u0026amp;\u0026amp; \\ sudo mv ./kubectl /usr/local/bin/kubectl This will install kubectl in your Cloud9 environment. To test to make sure the command is installed properly, execute the command:\nkubectl version --client You should see the kubectl version message.\n"
},
{
	"uri": "/25_workshop_2_ha-dr/50_add_yamls/20_podinfo_introduction.html",
	"title": "Introducing Podinfo",
	"tags": [],
	"description": "",
	"content": "Podinfo is a tiny web application made with Go that showcases best practices of running microservices in Kubernetes. It provides a simple web interface that allows you verify the operation of ingresses and/or service meshes. Podinfo was written by Stefan Prodan of Weaveworks, and is freely available on github. There are many options, and URLs are available to provide a wealth of information. More information on Podinfo is available here.\nFor our workshop, we will be using podinfo to show how load balancing and high availability can be simply implemented with ingresses and Kubernetes.\nOur configuration will enable internal Kubernetes load balancing and scaling. Optionally, AWS Route 53 can be used to balance between your actual clusters in different regions.\n"
},
{
	"uri": "/20_weaveworks_prerequisites.html",
	"title": "Prerequisites for all GitOps Workshop Modules",
	"tags": [],
	"description": "",
	"content": " Setup All the Required Tools \u0026amp; Permissions To complete these workshop modules, you will first need to install eksctl. Instructions for doing this can be found at eksctl.io. eksctl (pronounced \u0026ldquo;eks control\u0026rdquo;) is a simple CLI tool for creating clusters on EKS. It is written in Go, uses CloudFormation, was created by Weaveworks and it welcomes contributions from the community.\nIf you are using your own AWS account, you will need permissions to create EKS clusters plus admin rights within your EKS cluster to configure configuration rules and install agents. Ensure you have authority within your organization to do this in your tenant.\n "
},
{
	"uri": "/70_workshop_7_multicloud/20_credentials.html",
	"title": "Set up credentials",
	"tags": [],
	"description": "",
	"content": " Set up credentials Ensure you have the right IAM profile before proceeding with steps\naws sts get-caller-identity --query Arn | grep modernization-admin -q \u0026amp;\u0026amp; echo \u0026#34;IAM role valid\u0026#34; || echo \u0026#34;IAM role NOT valid\u0026#34; Kubeconfig for EKS management cluster  You should see an eks cluster already provisioned when running\neksctl get clusters We\u0026rsquo;ll need kubeconfig credentials. You can get it with\neksctl utils write-kubeconfig --cluster EKS-YOURCLUSTERNAME  AWS Keypair  Open a new tab and navigate to AWS console \u0026gt; EC2 \u0026gt; Key Pairs  Add a new Key Pair named weaveworks-workshop and save the pem file In your Cloud9 tab, click File \u0026gt; Upload local files, then choose the Key Pair\u0026rsquo;s pem file that was downloaded. It should have the name weaveworks-workshop.pem   Set up environment variables  cd aws-gitops-multicloud, then run cp ../gitops-cluster-management/.envrc.example .envrc Open .envrc and start populating fields  CAPI_AWS_ACCESS_KEY_ID to your workshop AWS_ACCESS_KEY_ID CAPI_AWS_SECRET_ACCESS_KEY to your workshop AWS_SECRET_ACCESS_KEY GIT_USER to your github username GIT_REPO_NAME to your repo name aws-gitops-multicloud AWS_REGION to us-west-2 AWS_SSH_KEY_NAME to weaveworks-workshop that we created earlier we can leave AWS_CONTROL_PLANE_MACHINE_TYPE and AWS_NODE_MACHINE_TYPE as t3.large  Finally run direnv allow. Which will export these env vars whenever you\u0026rsquo;re in the git repo directory.  "
},
{
	"uri": "/10_aws_prerequisites/20_self_paced.html",
	"title": "Using your own account",
	"tags": [],
	"description": "",
	"content": " PLEASE NOTE THAT THESE INSTRUCTIONS ARE INCOMPLETE AS OF MAY 14, 2020. PLEASE DO NOT PERFORM THESE WORKSHOPS UNLESS AT AN AWS HOSTED EVENT UNTIL THIS WARNING IS REMOVED.\n Only complete this section if you are running the workshop on your own. If you are at an AWS hosted event (such as re:Invent, Kubecon, Immersion Day, etc), goto Start the workshop at an AWS event.\n Create an AWS account You are responsible for the cost of the AWS services used while running this workshop in your AWS account.\n Your account must have the ability to create new IAM roles and scope other IAM permissions.\n  If you don\u0026rsquo;t already have an AWS account with Administrator access: create one now by clicking here\n Once you have an AWS account, ensure you are following the remaining workshop steps as an IAM user with administrator access to the AWS account: Create a new IAM user to use for the workshop\n Enter the user details:  Attach the AdministratorAccess IAM Policy:  Click to create the new user:  Take note of the login URL and save:   Login Now logout and login as as the new workshop user.\nCreate keypair This workshop will provision several EC2 VMs, so we will need to make a key pair that the CloudFormation script expects.\n Within the AWS web console, navigate to EC2 services and click on the key pair left side menu.\n Click on the create key pair button and fill in these values\n name = eksworkshop fileformat = pem    The name must be eksworkshop or the CloudFormation stack creation will fail.\n  Click on the create key pair, and a PEM file will be downloaded and you will return to the keypair page.\n  We will not need the PEM file for the workshop, but we recommend you save this file to a safe place especially if this is your AWS account.\n Provision the workshop environment This workshop creates an AWS account and a Cloud9 environment using a CloudFormation script.\n First need to add keypair named “eksworkshop” in us west-2 (Oregon). Follow this deep link to load the CloudFormation script.\n The new web page should look like this:  Choose all the defaults and click the Next button on each page. On the last page, accept all terms at the last page the CloudFormation flow and then click the Create Stack button as shown below.\n The deployment process takes approximately 20 minutes to complete. You can monitor in the AWS console CloudFormation service page. When it is done you should see CREATE_COMPLETE for all stack and nested stacks as shown below.\n  While you are waiting \u0026hellip; You will be asked to come back to the AWS console to review that CloudFormation status, but for now head to GitOps for EKS Prerequisites section.\n"
},
{
	"uri": "/40_workshop_4_hipo-teams/50_install_container_insights/30_verify_container_insights.html",
	"title": "Verify metric collection",
	"tags": [],
	"description": "",
	"content": "Now that Container Insights with additional Prometheus metrics collection is setup we should start to see metrics appear in CloudWatch.\nGo to the AWS Console and open CloudWatch.\nNow click Metrics on the left pane. On the right portion of the page you should now see ContainerInsights and ContainerInsights/Prometheus listed:\n"
},
{
	"uri": "/30_workshop_03_grc/150_iam-groups/21_create-iam-groups.html",
	"title": "Create IAM Groups",
	"tags": [],
	"description": "",
	"content": " We want to have different IAM users which will be added to specific IAM groups in order to have different rights in the kubernetes cluster.\nWe will define 3 groups:\n k8sAdmin - users from this group will have admin rights on the kubernetes cluster k8sDev - users from this group will have full access only in the development namespace of the cluster k8sInteg - users from this group will have access to integration namespace.   In fact, users from k8sDev and k8sInteg groups will only have access to namespaces where we will define kubernetes RBAC access for their associated kubernetes role. We\u0026rsquo;ll see this but first, let\u0026rsquo;s creates the groups.\n Create k8sAdmin IAM Group The k8sAdmin Group will be allowed to assume the k8sAdmin IAM Role.\naws iam create-group --group-name k8sAdmin Let\u0026rsquo;s add a Policy on our group which will allow users from this group to assume our k8sAdmin Role:\nADMIN_GROUP_POLICY=$(echo -n \u0026#39;{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowAssumeOrganizationAccountRole\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:iam::\u0026#39;; echo -n \u0026#34;$ACCOUNT_ID\u0026#34;; echo -n \u0026#39;:role/k8sAdmin\u0026#34; } ] }\u0026#39;) echo ADMIN_GROUP_POLICY=$ADMIN_GROUP_POLICY aws iam put-group-policy \\ --group-name k8sAdmin \\ --policy-name k8sAdmin-policy \\ --policy-document \u0026#34;$ADMIN_GROUP_POLICY\u0026#34; Create k8sDev IAM Group The k8sDev Group will be allowed to assume the k8sDev IAM Role.\naws iam create-group --group-name k8sDev Let\u0026rsquo;s add a Policy on our group which will allow users from this group to assume our k8sDev Role:\nDEV_GROUP_POLICY=$(echo -n \u0026#39;{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowAssumeOrganizationAccountRole\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:iam::\u0026#39;; echo -n \u0026#34;$ACCOUNT_ID\u0026#34;; echo -n \u0026#39;:role/k8sDev\u0026#34; } ] }\u0026#39;) echo DEV_GROUP_POLICY=$DEV_GROUP_POLICY aws iam put-group-policy \\ --group-name k8sDev \\ --policy-name k8sDev-policy \\ --policy-document \u0026#34;$DEV_GROUP_POLICY\u0026#34; Create k8sInteg IAM Group aws iam create-group --group-name k8sInteg Let\u0026rsquo;s add a Policy on our group which will allow users from this group to assume our k8sInteg Role:\nINTEG_GROUP_POLICY=$(echo -n \u0026#39;{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowAssumeOrganizationAccountRole\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:iam::\u0026#39;; echo -n \u0026#34;$ACCOUNT_ID\u0026#34;; echo -n \u0026#39;:role/k8sInteg\u0026#34; } ] }\u0026#39;) echo INTEG_GROUP_POLICY=$INTEG_GROUP_POLICY aws iam put-group-policy \\ --group-name k8sInteg \\ --policy-name k8sInteg-policy \\ --policy-document \u0026#34;$INTEG_GROUP_POLICY\u0026#34; You now should have your 3 groups\naws iam list-groups  { \"Groups\": [ { \"Path\": \"/\", \"GroupName\": \"k8sAdmin\", \"GroupId\": \"AGPAZRV3OHPJZGT2JKVDV\", \"Arn\": \"arn:aws:iam::xxxxxxxxxx:group/k8sAdmin\", \"CreateDate\": \"2020-04-07T13:32:52Z\" }, { \"Path\": \"/\", \"GroupName\": \"k8sDev\", \"GroupId\": \"AGPAZRV3OHPJUOBR375KI\", \"Arn\": \"arn:aws:iam::xxxxxxxxxx:group/k8sDev\", \"CreateDate\": \"2020-04-07T13:33:15Z\" }, { \"Path\": \"/\", \"GroupName\": \"k8sInteg\", \"GroupId\": \"AGPAZRV3OHPJR6GM6PFDG\", \"Arn\": \"arn:aws:iam::xxxxxxxxxx:group/k8sInteg\", \"CreateDate\": \"2020-04-07T13:33:25Z\" } ] }  "
},
{
	"uri": "/25_workshop_2_ha-dr/50_add_yamls/21_podinfo_deploy.html",
	"title": "Deploy a Podinfo Pod",
	"tags": [],
	"description": "",
	"content": "First, let\u0026rsquo;s create a namespace for our application (podinfo) to execute in. In each of the terminal sessions, execute:\nkubectl create namespace podinfo Yes, this can also be accomplished by placing a manifest in your git repository to create the namespace.\nYou can run:\nkubectl create namespace podinfo --dry-run -o yaml \u0026gt; podinfo-namespace.yaml Or paste this into a new file called podinfo-namespace.yaml.\napiVersion: v1 kind: Namespace metadata: creationTimestamp: null name: podinfo Add, commit, and push this change to your repository. You can call fluxctl sync --k8s-fwd-ns flux to get flux to deploy the changes without waiting for the set poll window.\nWe will be creating a number of files in your git repository for these next few steps. You can choose to create all the files first, then commit and push to the repository, or you can commit and push after each file creation. If you commit and push after each file creation, you can execute kubectl get pods -A to see what has happened.\n Create a file called podinfo-deployment.yaml in your git repository:\napiVersion: apps/v1 kind: Deployment metadata: name: podinfo namespace: podinfo spec: minReadySeconds: 3 revisionHistoryLimit: 5 progressDeadlineSeconds: 60 strategy: rollingUpdate: maxUnavailable: 0 type: RollingUpdate selector: matchLabels: app: podinfo template: metadata: annotations: prometheus.io/scrape: \u0026#34;true\u0026#34; prometheus.io/port: \u0026#34;9797\u0026#34; labels: app: podinfo spec: containers: - name: podinfod image: stefanprodan/podinfo:3.2.3 imagePullPolicy: IfNotPresent ports: - name: http containerPort: 9898 protocol: TCP - name: http-metrics containerPort: 9797 protocol: TCP - name: grpc containerPort: 9999 protocol: TCP command: - ./podinfo - --port=9898 - --port-metrics=9797 - --grpc-port=9999 - --grpc-service-name=podinfo - --level=info - --random-delay=false - --random-error=false env: - name: PODINFO_UI_COLOR value: \u0026#34;#34577c\u0026#34; livenessProbe: exec: command: - podcli - check - http - localhost:9898/healthz initialDelaySeconds: 5 timeoutSeconds: 5 readinessProbe: exec: command: - podcli - check - http - localhost:9898/readyz initialDelaySeconds: 5 timeoutSeconds: 5 resources: limits: cpu: 2000m memory: 512Mi requests: cpu: 100m memory: 64Mi"
},
{
	"uri": "/30_workshop_03_grc/150_iam-groups/22_create_iam_users.html",
	"title": "Create IAM Users",
	"tags": [],
	"description": "",
	"content": "In order to test our scenarios, we will create 3 users, one for each groups we created :\naws iam create-user --user-name PaulAdmin aws iam create-user --user-name JeanDev aws iam create-user --user-name PierreInteg Add users to associated groups:\naws iam add-user-to-group --group-name k8sAdmin --user-name PaulAdmin aws iam add-user-to-group --group-name k8sDev --user-name JeanDev aws iam add-user-to-group --group-name k8sInteg --user-name PierreInteg Check users are correctly added in their groups:\naws iam get-group --group-name k8sAdmin aws iam get-group --group-name k8sDev aws iam get-group --group-name k8sInteg For the sake of simplicity, in this chapter, we will save credentials to a file to make it easy to toggle back and forth between users. Never do this in production or with credentials that have priveledged access; It is not a security best practice to store credentials on the filesystem.\n Retrieve Access Keys for our fake users:\naws iam create-access-key --user-name PaulAdmin | tee /tmp/PaulAdmin.json aws iam create-access-key --user-name JeanDev | tee /tmp/JeanDev.json aws iam create-access-key --user-name PierreInteg | tee /tmp/PierreInteg.json ~/.aws/PaulAdmin_creds.sh export AWS_SECRET_ACCESS_KEY=$(jq .AccessKey.SecretAccessKey /tmp/PaulAdmin.json) export AWS_ACCESS_KEY_ID=$(jq .AccessKey.AccessKeyId /tmp/PaulAdmin.json) EoF cat ~/.aws/JeanDev_creds.sh export AWS_SECRET_ACCESS_KEY=$(jq .AccessKey.SecretAccessKey /tmp/JeanDev.json) export AWS_ACCESS_KEY_ID=$(jq .AccessKey.AccessKeyId /tmp/JeanDev.json) EoF cat ~/.aws/PierreInteg_creds.sh export AWS_SECRET_ACCESS_KEY=$(jq .AccessKey.SecretAccessKey /tmp/PierreInteg.json) export AWS_ACCESS_KEY_ID=$(jq .AccessKey.AccessKeyId /tmp/PierreInteg.json) EoF ``` -- Recap:\n PaulAdmin is in the k8sAdmin group and will be able to assume the k8sAdmin role. JeanDev is in k8sDev Group and will be able to assume IAM role k8sDev PierreInteg is in k8sInteg group and will be able to assume IAM role k8sInteg  "
},
{
	"uri": "/25_workshop_2_ha-dr/50_add_yamls/22_podinfo_hpa.html",
	"title": "Enable the Podinfo Pod to Scale Automatically",
	"tags": [],
	"description": "",
	"content": "A very useful feature in Kubernetes is to enable auto-scaling. This is accomplished by created a Horizontal Pod Autoscaler (HPA) object that is targeted at a specific pod. Each HPA has criteria that Kubernetes uses in order add pods to the running deployment. This HPA will scale the podinfo deployment to a minimum of 2 replicas, and a maximum of 4 replicas.\nCreate a file called podinfo-hpa.yaml in your git repository:\napiVersion: autoscaling/v2beta1 kind: HorizontalPodAutoscaler metadata: name: podinfo namespace: podinfo spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: podinfo minReplicas: 2 maxReplicas: 4 metrics: - type: Resource resource: name: cpu # scale up if usage is above # 99% of the requested CPU (100m) targetAverageUtilization: 99"
},
{
	"uri": "/22_workshop_1.html",
	"title": "Workshop Module 1: Introduction to GitOps on EKS",
	"tags": [],
	"description": "",
	"content": " Workshop Module 1: Introduction to GitOps on EKS In this module we\u0026rsquo;ll introduce you to the concepts of GitOps and how to create and operate an EKS cluster using the eksctl tool. This workshop is a prerequisite for all other workshops in this series.\nIf you are using your own AWS account, you will need permissions to create EKS clusters plus admin rights within your EKS cluster to configure configuration rules and install agents. Ensure you have authority within your organization to do this in your tenant.\n "
},
{
	"uri": "/25_workshop_2_ha-dr.html",
	"title": "Workshop Module 2: Using GitOps and EKS to Manage HA &amp; DR",
	"tags": [],
	"description": "",
	"content": " Welcome to the High Availability and Disaster Recovery Module 4 out of 10 enterprises are now running Kubernetes in a production environment today but many still consider security and policy concerns an overwhelming obstacle and bottleneck to innovation.\nGitOps operation practices give teams a head start as they rely on git’s strong correctness and security. Every pull request has a built-in and fully auditable trail. Many companies need to look beyond just compliance and seek a full GRC solution that’s integral to their workflows.\nThis workshops goes deep on the following topics:\n How to set up repeatable fleets of clusters with GitOps and eksctl How to set up multi-cluster and configure AWS services such as ELB and Route53 How to connect ingress and DNS using GitOps  Need help in getting started with EKS? Sign up for our EKS + GitOps Quickstart package.\nGitOps - an operating model for cloud native  An operating model for Kubernetes and other cloud native technologies, providing a set of best practices that unify deployment, management and monitoring for containerized clusters and applications. A path towards a developer experience for managing applications; where end-to-end CICD pipelines and git workflows are applied to both operations, and development.  Weaveworks Weaveworks makes it fast and simple for developers and DevOps teams to build and operate powerful containerized applications. We minimize the complexity of operating workloads in Kubernetes by providing automated continuous delivery pipelines, observability and monitoring.\nWeaveworks’ mission is to minimize complexities in operating workloads and provide a developer centric operating model for cloud native applications. Our goal is to drive cloud native transformation for your developers and portability, flexibility, choice and stability for your business.\nOne of the first members of the Cloud Native Computing Foundation, Weaveworks also contributes to several open source projects, including Weave Net, Weave Scope, Weave Cortex, Weave Flux, Firekube and Flagger.\nWeaveworks and AWS Weaveworks is an APN Advanced Technology Partner and AWS Containers Competency Partner. In addition Weaveworks is the EKS certified Advanced partner that built eksctl.\nWeaveworks Pioneered GitOps Learn more about GitOps, a set of modern best practices for deploying and managing applications with cloud native tools and cloud services.\nDon’t know where to start? Our experienced team of Customer Reliability Engineers can provide the highest level of care, advice and coaching. Our expertise is rooted in working and supporting live production systems at scale. Get started with our EKS + GitOps Quickstart package.\nFor further reading we recommend:  Whitepaper: Implementing a Kubernetes Strategy in your Organization Whitepaper: Automating Kubernetes with GitOps Whitepaper: Guide to a product ready Kubernetes cluster  Checklist: Production Readiness for Kubernetes  Case Study: Fidelity Case Study: Mettle  For hands on instructions, we recommend: Tutorial: A practical guide to GitOps \n"
},
{
	"uri": "/25_workshop_2_ha-dr/50_add_yamls/23_podinfo_service.html",
	"title": "Expose the Podinfo Pod Service",
	"tags": [],
	"description": "",
	"content": "In order for any Kubernetes object, including other pods, to access the podinfo pod, we have to expose on the Kubernetes network the network ports that should be accessed. When using the Application Load Balancer on EKS, the service type should be NodePort. In more complex architectures, there are other load balancing methods that can be used.\nYou can create a NodePort Service yaml (as specified by the ALB ingress controller) by running the following command and adding the manifest to your repository as podinfo-service.yaml:\nkubectl create service nodeport podinfo --namespace podinfo --tcp=9898:9898 --dry-run -o yaml \u0026gt; podinfo-service.yaml Alternatively, you can copy and paste the service as defined below to a new file called podinfo-service.yaml:\napiVersion: v1 kind: Service metadata: name: podinfo namespace: podinfo spec: type: NodePort selector: app: podinfo ports: - name: http port: 9898 protocol: TCP"
},
{
	"uri": "/25_workshop_2_ha-dr/50_add_yamls/24_podinfo_ingress.html",
	"title": "Connect the Podinfo Service to the Ingress Controller",
	"tags": [],
	"description": "",
	"content": "The final step is to inform the ALB Ingress Controller that a service needs to be exposed externally from the cluster. The ingress controller watches for the creation of Ingress objects, and then creates the routing required to reach the object externally.\nIn the Ingress object definition, ther eare annotations to indicate how the Ingress Controller should handle this service, as well as which controller to utilize. As there may be several different ingress controllers in a single Kubernetes cluster, this ability is key to managing access to multiple applications and microservices.\nTher is also a path parameter in this definition. The path parameter designates the external URL (URI) that would be required to reach this service. Each path for each specific ingress controller must be unique. For simplicity, we have mapped the root URI (\u0026ldquo;/\u0026rdquo;) to the service directly.\nCreate a file called podinfo-ingress.yaml in your git repository containing:\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: podinfo-ingress namespace: podinfo annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing spec: rules: - http: paths: - path: /* backend: serviceName: podinfo servicePort: 9898"
},
{
	"uri": "/30_workshop_03_grc/150_iam-groups/30_create-k8s-rbac.html",
	"title": "Configure Kubernetes RBAC",
	"tags": [],
	"description": "",
	"content": " You can refere to intro to RBAC module to understand the basic of Kubernetes RBAC.\nCreate kubernetes namespaces development namespace will be accessible for IAM users from k8sDev group integration namespace will be accessible for IAM users from k8sInteg group\nmkdir integration cat \u0026lt;\u0026lt; EOF \u0026gt; integration/integration-ns.yaml apiVersion: v1 kind: Namespace metadata: name: integration labels: owner: me.agilebank.demo EOF mkdir development cat \u0026lt;\u0026lt; EOF \u0026gt; development/development-ns.yaml apiVersion: v1 kind: Namespace metadata: name: development labels: owner: me.agilebank.demo EOF Configuring access to development namespace We create a kubernetes role and rolebinding in the development namespace giving full access to the kubernetes user dev-user\nmkdir development/roles cat \u0026lt;\u0026lt; EOF \u0026gt; ./development/roles/dev-role.yaml kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: dev-role namespace: development rules: - apiGroups: - \u0026#34;\u0026#34; - \u0026#34;apps\u0026#34; - \u0026#34;batch\u0026#34; - \u0026#34;extensions\u0026#34; resources: - \u0026#34;configmaps\u0026#34; - \u0026#34;cronjobs\u0026#34; - \u0026#34;deployments\u0026#34; - \u0026#34;events\u0026#34; - \u0026#34;ingresses\u0026#34; - \u0026#34;jobs\u0026#34; - \u0026#34;pods\u0026#34; - \u0026#34;pods/attach\u0026#34; - \u0026#34;pods/exec\u0026#34; - \u0026#34;pods/log\u0026#34; - \u0026#34;pods/portforward\u0026#34; - \u0026#34;secrets\u0026#34; - \u0026#34;services\u0026#34; verbs: - \u0026#34;create\u0026#34; - \u0026#34;delete\u0026#34; - \u0026#34;describe\u0026#34; - \u0026#34;get\u0026#34; - \u0026#34;list\u0026#34; - \u0026#34;patch\u0026#34; - \u0026#34;update\u0026#34; --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: dev-role-binding namespace: development subjects: - kind: User name: dev-user roleRef: kind: Role name: dev-role apiGroup: rbac.authorization.k8s.io EOF The role we define will give full access to everything in that namespace. It is a Role, and not a ClusterRole, so it is going to be applied only in the development namespace.\n feel free to adapt or duplicate to any namespace you prefer.\n Configuring access to integration namespace We create a kubernetes role and rolebinding in the integration namespace for full access with the kubernetes user integ-user\nmkdir integration/roles cat \u0026lt;\u0026lt; EOF \u0026gt; ./integration/roles/integ-role.yaml kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: integ-role namespace: integration rules: - apiGroups: - \u0026#34;\u0026#34; - \u0026#34;apps\u0026#34; - \u0026#34;batch\u0026#34; - \u0026#34;extensions\u0026#34; resources: - \u0026#34;configmaps\u0026#34; - \u0026#34;cronjobs\u0026#34; - \u0026#34;deployments\u0026#34; - \u0026#34;events\u0026#34; - \u0026#34;ingresses\u0026#34; - \u0026#34;jobs\u0026#34; - \u0026#34;pods\u0026#34; - \u0026#34;pods/attach\u0026#34; - \u0026#34;pods/exec\u0026#34; - \u0026#34;pods/log\u0026#34; - \u0026#34;pods/portforward\u0026#34; - \u0026#34;secrets\u0026#34; - \u0026#34;services\u0026#34; verbs: - \u0026#34;create\u0026#34; - \u0026#34;delete\u0026#34; - \u0026#34;describe\u0026#34; - \u0026#34;get\u0026#34; - \u0026#34;list\u0026#34; - \u0026#34;patch\u0026#34; - \u0026#34;update\u0026#34; --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: integ-role-binding namespace: integration subjects: - kind: User name: integ-user roleRef: kind: Role name: integ-role apiGroup: rbac.authorization.k8s.io EOF The role we define will give full access to everything in that namespace. It is a Role, and not a ClusterRole, so it is going to be applied only in the integration namespace.\nCheck this in to git! run the following to add our changes:\ngit add \u0026#34;integration/integration-ns.yaml\u0026#34; git add \u0026#34;development/development-ns.yaml\u0026#34; git commit -m \u0026#34;adding dev adn int namespaces\u0026#34; git add \u0026#34;integration/roles/\u0026#34; git add \u0026#34;development/roles/\u0026#34; git commit -m \u0026#34;adding int \u0026amp; dev roles\u0026#34; git push"
},
{
	"uri": "/30_workshop_03_grc.html",
	"title": "Workshop Module 3: Managing GRC for K8s on EKS",
	"tags": [],
	"description": "",
	"content": " Welcome to Managing governance, risk and compliance for Kubernetes on EKS Accelerate your journey to production ready Kubernetes on EKS by learning the practical techniques for reliably operating your software lifecycle using GitOps. In this virtual workshop the Weaveworks team will be sharing their expertise as users and contributors to Kubernetes and Prometheus, as well as pioneers of GitOps (operations by pull request).\nThis workshops goes deep on the following topics:Using a combination of instructor led demonstrations and hands-on exercises, the workshop will enable the attendee to go into detail on the following topics: * How to utilize IAM to configure RBAC and access control * How to create, assign and test IAM roles for AWS service accounts * How to deploy open policy agent * How to write simple and effective rego policies to achieve compliance\nNeed help in getting started with EKS? Sign up for our EKS + GitOps Quickstart package.\nGitOps - an operating model for cloud native  An operating model for Kubernetes and other cloud native technologies, providing a set of best practices that unify deployment, management and monitoring for containerized clusters and applications. A path towards a developer experience for managing applications; where end-to-end CICD pipelines and git workflows are applied to both operations, and development.  Weaveworks Weaveworks makes it fast and simple for developers and DevOps teams to build and operate powerful containerized applications. We minimize the complexity of operating workloads in Kubernetes by providing automated continuous delivery pipelines, observability and monitoring.\nWeaveworks’ mission is to minimize complexities in operating workloads and provide a developer centric operating model for cloud native applications. Our goal is to drive cloud native transformation for your developers and portability, flexibility, choice and stability for your business.\nOne of the first members of the Cloud Native Computing Foundation, Weaveworks also contributes to several open source projects, including Weave Net, Weave Scope, Weave Cortex, Weave Flux, Firekube and Flagger.\nWeaveworks and AWS Weaveworks is an APN Advanced Technology Partner and AWS Containers Competency Partner. In addition Weaveworks is the EKS certified Advanced partner that built eksctl.\nWeaveworks Pioneered GitOps Learn more about GitOps, a set of modern best practices for deploying and managing applications with cloud native tools and cloud services.\nDon’t know where to start? Our experienced team of Customer Reliability Engineers can provide the highest level of care, advice and coaching. Our expertise is rooted in working and supporting live production systems at scale. Get started with our EKS + GitOps Quickstart package.\nFor further reading we recommend:  Whitepaper: Implementing a Kubernetes Strategy in your Organization Whitepaper: Automating Kubernetes with GitOps Whitepaper: Guide to a product ready Kubernetes cluster  Checklist: Production Readiness for Kubernetes  Case Study: Fidelity Case Study: Mettle  For hands on instructions, we recommend: Tutorial: A practical guide to GitOps \n"
},
{
	"uri": "/30_workshop_03_grc/120_deploy_opa_gatekeeper/30_deploy-opa-gatekeeper.html",
	"title": "Deploy OPA gatekeeper",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s deploy opa gatekeeper using a prebuilt image.\nThis manifest will deplpy the following in to the cluster:\n gatekeeper-system namespace gatekeeper-admin service account config custom resource defintion ConstraintTemplate custom resource gatekeeper-webhook-service - clusterip service gatekeeper-controller-manager deployment secret - self signed certificate think about using something else, such as cert-manager + pki required roles ValidatingWebhookConfiguration - this basically creates a validating webhook whenever a resource is created or udpated in the cluster\ncd eksworkshop  make sure you have navigated into your git repo\nmkdir opa curl https://weaveworks-gitops.awsworkshop.io/30_workshop_03_grc/deploy.files/opa-gatekeper.yaml -o opa/opa-gatekeeper.yaml You should see the manifest now exists insde the opa folder. We\u0026rsquo;re no gonna let Flux install this and manage for us. Run the following to check in to git:\ngit add opa/opa-gatekeeper.yaml git commit -m \u0026#34;adding opa gatekeeper\u0026#34; git push Now let\u0026rsquo;s see what\u0026rsquo;s installed:\nkubectl get pods --namespace gatekeeper-system You should see the gatekeeper-controller-manager running now.\nLet\u0026rsquo;s see what else is there. Run the following to see what crd\u0026rsquo;s have been deployed:\nkubectl get crd You should see two, one is called config, this is mainly used for auditing and checking what has already been deployed in to the cluster before OPA and flagging any pre-existing violations.\nYou should see another one called constrainttemplates, let\u0026rsquo;s focus on this as this is what helps us define our policies.\n"
},
{
	"uri": "/70_workshop_7_multicloud/30_bootstrap_management_cluster.html",
	"title": "Bootstrap Management Cluster",
	"tags": [],
	"description": "",
	"content": " Bootstrap Management Cluster Namespaces Let\u0026rsquo;s create all the namespaces we need\nkubectl create namespace fluxcd kubectl create namespace mgmt-clusters Install Flux helm repo add fluxcd https://charts.fluxcd.io kubectl apply -f https://raw.githubusercontent.com/fluxcd/helm-operator/master/deploy/crds.yaml helm upgrade -i flux fluxcd/flux --wait \\  --namespace fluxcd \\  --set git.url=git@github.com:${GIT_USER}/${GIT_REPO_NAME}.git \\  --set git.path=\u0026#34;flux-mgmt\u0026#34; \\  --set git.timeout=120s \\  --set git.pollInterval=1m \\  --set syncGarbageCollection.enabled=true \\  --set rbac.create=true Get flux\u0026rsquo;s public key\nfluxctl identity --k8s-fwd-ns fluxcd  Copy printed public key and paste it in your git repo\u0026rsquo;s Settings \u0026gt; Deploy Keys \u0026gt; Add Deploy Key. Make sure to turn on write access. If no key shows up, try re-running fluxctl identity --k8s-fwd-ns fluxcd until it shows up.  Install Helm Operator helm upgrade helm-operator fluxcd/helm-operator \\  --force \\  -i \\  --wait \\  --namespace fluxcd \\  --set helm.versions=v3 Install CAPI export AWS_B64ENCODED_CREDENTIALS=$(cat \u0026lt;\u0026lt;EOF | base64 -w 0 [default] aws_access_key_id = ${CAPI_AWS_ACCESS_KEY_ID} aws_secret_access_key = ${CAPI_AWS_SECRET_ACCESS_KEY} EOF ) clusterctl init --infrastructure aws || true"
},
{
	"uri": "/40_workshop_4_hipo-teams/60_install-pod-info/30_create_podinfo.html",
	"title": "Create PodInfo",
	"tags": [],
	"description": "",
	"content": "We first need to install the 3 different instances of PodInfo:\n Frontend Backend v1 Backend v2  Create a apps folder in our repo:\n. ├── amazon-cloudwatch │ ├── cwagent-fluentd-quickstart.yaml │ └── cwagent-prometheus-eks.yaml ├── appmesh-system │ ├── appmesh-controller.yaml │ ├── appmesh-inject.yaml │ ├── appmesh-prometheus.yaml │ └── crds.yaml ├── apps ├── namespaces │ ├── amazon-cloudwatch.yaml │ ├── appmesh-system.yaml │ └── apps.yaml └── README.md Now download the yaml file for the deployments and services:\ncurl https://weaveworks-gitops.awsworkshop.io/40_workshop_4_hipo-teams/60_install-pod-info/deploy.files/1-podinfo.yaml -o apps/1-podinfo.yaml Add and then commit the 1-podinfo.yaml file and push the the changes to your GitHub repo.\nFlux will now see that the desired state of the apps namespace has changed in Git and will apply the resources to our cluster. This will take up to 1 minute to apply.\nCheck that that the deployment and services has been created by running the following command:\nkubectl get pods,svc -n apps You should see the pods running and services:\nNAME READY STATUS RESTARTS AGE pod/backend-podinfo-v1-5454d76d7f-66cg7 2/2 Running 0 56s pod/backend-podinfo-v2-74f967bf8b-4lb28 2/2 Running 0 56s pod/frontend-podinfo-54f9995b4f-r668p 2/2 Running 0 56s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/backend-podinfo-v1 ClusterIP 10.100.52.43 \u0026lt;none\u0026gt; 9898/TCP,9999/TCP 56s service/backend-podinfo-v2 ClusterIP 10.100.34.160 \u0026lt;none\u0026gt; 9898/TCP,9999/TCP 56s service/frontend-podinfo ClusterIP 10.100.73.93 \u0026lt;none\u0026gt; 9898/TCP,9999/TCP 56s You\u0026rsquo;ll see that there are 2 containers running in each of the PodInfo pods. This is denoted by the 2\u0026frasl;2 in the READY column. If you look at the 1-podinfo.yaml file you\u0026rsquo;ll see that there is only 1 container declared (i.e. podinfo). The AWS App Mesh Injector has injected a sidecar container into the deployment when Flux applied the yaml. You can verify this by doing a kubectl describe on one of the pods.\n "
},
{
	"uri": "/60_workshop_6_ml/30_deploy_appdev_profile.html",
	"title": "Deploy Application Development Profile",
	"tags": [],
	"description": "",
	"content": " Deploy Application Development Profile Add app-dev profile\n"
},
{
	"uri": "/40_workshop_4_hipo-teams/30_gitops_enable_clusters.html",
	"title": "GitOps Enable Clusters",
	"tags": [],
	"description": "",
	"content": " GitOps Enable Your EKS Cluster gitops is a way to do Kubernetes application delivery. It works by using Git as a single source of truth for Kubernetes resources and everything else. With Git at the center of your delivery pipelines, you and your team can make pull requests to accelerate and simplify application deployments and operations tasks to Kubernetes.\nIt\u0026rsquo;s assumed that you have already completed Workshop 1: Introduction to GitOps on EKS and as such we will be speeding through some of the base Flux commands.\n"
},
{
	"uri": "/50_workshop_5_accelerating_sdlc/30_install_appmesh.html",
	"title": "Install App Mesh",
	"tags": [],
	"description": "",
	"content": " Install App Mesh In this section, we will be using an experimental feature of eksctl to install App Mesh.\nThe eksctl appmesh profile will deploy the App Mesh Kubernetes components along with monitoring and progressive delivery tooling on an EKS cluster.\nComponents that this profile will install include:\n Kubernetes custom resources: mesh, virtual nodes and virtual services CRD controller: keeps the custom resources in sync with the App Mesh control plane Admission controller: injects the Envoy sidecar and assigns pods to App Mesh virtual nodes Telemetry service: Prometheus instance that collects and stores Envoy\u0026rsquo;s metrics Progressive delivery operator: Flagger instance that automates canary releases on top of App Mesh  "
},
{
	"uri": "/40_workshop_4_hipo-teams/40_install-app-mesh/30_install_appmesh_controller.html",
	"title": "Install App Mesh Controller",
	"tags": [],
	"description": "",
	"content": "Now the CRDs have been installed we can install the controller that understands them.\nThe AWS App Mesh Controller is available as a Helm package from the EKS Chart Repository. We can use the Helm Operator along with this chart to install the controller.\nTo install a chart using the Helm Operator we need to define a HelmRelease. A HelmRelease is a way to declare the desired state of a Helm release. The HelmRelease can be applied to a cluster using GitOps.\nCreate a new file called appmesh-controller.yaml in the appmesh-system folder. Add the following as contents to the file:\n--- apiVersion: helm.fluxcd.io/v1 kind: HelmRelease metadata: name: appmesh-controller namespace: appmesh-system spec: releaseName: appmesh-controller chart: repository: https://aws.github.io/eks-charts/ name: appmesh-controller version: 0.6.1 This HelmRelease is declaring that we want to create a Helm release for a chart called appmesh-controller with a version 0.6.1 that is available in the EKS chart repository https://aws.github.io/eks-charts/. The name of the release is appmesh-controller.\nFor full details on the elements of the HelmRelease please head to the documentation.\n You folder structure should look like this now:\n. ├── appmesh-system │ ├── appmesh-controller.yaml │ └── crds.yaml ├── namespaces │ └── appmesh-system.yaml └── README.md Add and then commit the appmesh-controller.yaml file and push the the changes to your GitHub repo.\nFlux will now see that the desired state of the appmesh-system namespace has changed in Git and will apply the CRDs to our cluster. This will take up to 1 minute to apply.\nCheck that the App Mesh Controller is up \u0026amp; running by running the following command:\nkubectl get pods -n appmesh-system You should see the controller pod in a Running state:\nNAME READY STATUS RESTARTS AGE appmesh-controller-54dd6bdfd8-n8zlq 1/1 Running 0 43s"
},
{
	"uri": "/40_workshop_4_hipo-teams/40_install-app-mesh/40_install_appmesh_injector.html",
	"title": "Install App Mesh Injector",
	"tags": [],
	"description": "",
	"content": "For App Mesh to work each of your pods needs to have a sidecar. This sidecar handles the network routing and is the point in each of services that helps to enable functionality like the end-to-end observability.\nWe can manually add the sidecar to each of pods (via Deployments for example). However, this can become quite a maintenance burden and additionally this shouldn\u0026rsquo;t be a concern of application teams.\nLuckily, we can automatically inject the App Mesh sidecar into the pods. Kubernetes has a feature called Dynamic Admission Controllers that allow you to mutate the resources as they are applied to Kubernetes and this can be used to inject the sidecar\u0026hellip;..and for this purpose there is the App Mesh Injector.\nThe AWS App Mesh Injector is also available as a Helm package from the EKS Chart Repository. We will use the Helm Operator along with this chart to install the injector.\nCreate a new file called appmesh-injector.yaml in the appmesh-system folder. Add the following as contents to the file:\n--- apiVersion: helm.fluxcd.io/v1 kind: HelmRelease metadata: name: appmesh-inject namespace: appmesh-system spec: releaseName: appmesh-inject chart: repository: https://aws.github.io/eks-charts/ name: appmesh-inject version: 0.9.0 values: mesh: create: true name: apps This HelmRelease is declaring that we want to create a Helm release for a chart called appmesh-injector with a version 0.9.0 that is available in the EKS chart repository https://aws.github.io/eks-charts/. The name of the release is appmesh-injector.\nWe are also setting some values within the helm chart. This is equivalent to using a values.yaml value, see the Helm documentation for details of values. By setting these values in our HelmRelease we are stating that we want to create a new mesh called apps.\nYou folder structure should look like this now:\n. ├── appmesh-system │ ├── appmesh-controller.yaml │ ├── appmesh-inject.yaml │ └── crds.yaml ├── namespaces │ └── appmesh-system.yaml └── README.md Add and then commit the appmesh-inject.yaml file and push the the changes to your GitHub repo.\nFlux will now see that the desired state of the appmesh-system namespace has changed in Git and will apply the CRDs to our cluster. This will take up to 1 minute to apply.\nCheck that the App Mesh Injector is up \u0026amp; running by running the following command:\nkubectl get pods -n appmesh-system You should see the injector pod in a Running state:\nNAME READY STATUS RESTARTS AGE appmesh-controller-54dd6bdfd8-n8zlq 1/1 Running 0 30m appmesh-inject-55cdc99595-qm8pt 1/1 Running 0 17s"
},
{
	"uri": "/50_workshop_5_accelerating_sdlc/00_prerequisites.md/30_install_fluxctl.html",
	"title": "Install fluxctl",
	"tags": [],
	"description": "",
	"content": "We will be using Fluxctl, a CLI tool that is able to talk to Weave Flux to immediately apply and deploy changes we make to our repository.\ncurl -Ls https://fluxcd.io/install | sh \u0026amp;\u0026amp; \\ sudo mv $HOME/.fluxcd/bin/fluxctl /usr/local/bin/fluxctl Verify the installation:\nfluxctl version"
},
{
	"uri": "/60_workshop_6_ml/00_prerequisites.md/30_install_fluxctl.html",
	"title": "Install fluxctl",
	"tags": [],
	"description": "",
	"content": "We will be using Fluxctl, a CLI tool that is able to talk to Weave Flux to immediately apply and deploy changes we make to our repository.\ncurl -Ls https://fluxcd.io/install | sh \u0026amp;\u0026amp; \\ sudo mv $HOME/.fluxcd/bin/fluxctl /usr/local/bin/fluxctl Verify the installation:\nfluxctl version"
},
{
	"uri": "/25_workshop_2_ha-dr/30_create_git_repo.html",
	"title": "Set Up Your GIT Repository",
	"tags": [],
	"description": "",
	"content": "The most important ingredient using eksctl enable repo is the configuration of your repository (which will include your workload manifests, HelmReleases, etc). You can start with an empty repository and push that to Git, or use the one you intend to deploy to the cluster.\nThe main point of GitOps is to keep everything (config, alerts, dashboards, apps, literally everything) in Git and use it as a single source of truth. To keep your cluster configuration in Git, please go ahead and create an empty repository. On GitHub, for example, follow these steps..\nIn the Cloud9 environment, a user SSH key pair is not automatically created. Use these instructions to create an SSH key pair for use as your git repository deploy keys.\n Open a new terminal in your Cloud9 workspace and run the following commands.\nTo create a new ssh key, run the following. Substitute your github email address where specified.\nssh-keygen -t rsa -b 4096 -C \u0026#34;your_email@example.com\u0026#34; Start the ssh-agent in the background.\neval \u0026#34;$(ssh-agent -s)\u0026#34; Add your SSH private key to the ssh-agent and store your passphrase in the keychain. If you created your key with a different name, or if you are adding an existing key that has a different name, replace id_rsa in the command with the name of your private key file.\nssh-add ~/.ssh/id_rsa Then run:\ncat ~/.ssh/id_rsa.pub You can then create a deploy key in your repository by going to: Settings \u0026gt; Deploy keys. Click on Add deploy key, and check Allow write access. Then paste your public key and click Add key. This will allow us to clone this repo to our Cloud9 environment. Instructions to do so are here.\n"
},
{
	"uri": "/30_workshop_03_grc/150_iam-groups/31_configure-aws-auth.html",
	"title": "Configure Kubernetes Role Access",
	"tags": [],
	"description": "",
	"content": " Gives Access to our IAM Roles to EKS Cluster In order to gives access to the IAM Roles we defined previously to our EKS cluster, we need to add specific mapRoles to the aws-auth ConfigMap\nThe Advantage of using Role to access the cluster instead of specifying directly IAM users is that it will be easier to manage: we won\u0026rsquo;t have to update the ConfigMap each time we want to add or remove users, we will just need to add or remove users from the IAM Group and we just configure the ConfigMap to allow the IAM Role associated to the IAM Group.\nUpdate the aws-auth ConfigMap to allow our IAM roles The aws-auth ConfigMap from the kube-system namespace must be edited in order to allow or new arn Groups.\nThis file makes the mapping between IAM role and k8S RBAC rights. We can edit it manually:\nWe can edit it using eksctl :\neksctl create iamidentitymapping --cluster eksworkshop --arn arn:aws:iam::${ACCOUNT_ID}:role/k8sDev --username dev-user eksctl create iamidentitymapping --cluster eksworkshop --arn arn:aws:iam::${ACCOUNT_ID}:role/k8sInteg --username integ-user eksctl create iamidentitymapping --cluster eksworkshop --arn arn:aws:iam::${ACCOUNT_ID}:role/k8sAdmin --username admin --group system:masters  It can also be used to delete entries eksctl delete iamidentitymapping --cluster eksworkshop --arn arn:aws:iam::xxxxxxxxxx:role/k8sDev --username dev-user\n you should have the config map looking something like:\nkubectl get cm -n kube-system aws-auth -o yaml  apiVersion: v1 data: mapRoles: | - groups: - system:bootstrappers - system:nodes rolearn: arn:aws:iam::xxxxxxxxxx:role/eksctl-eksworkshop-eksctl-nodegro-NodeInstanceRole-14TKBWBD7KWFH username: system:node:{{EC2PrivateDNSName}} - rolearn: arn:aws:iam::xxxxxxxxxx:role/k8sDev username: dev-user - rolearn: arn:aws:iam::xxxxxxxxxx:role/k8sInteg username: integ-user - groups: - system:masters rolearn: arn:aws:iam::xxxxxxxxxx:role/k8sAdmin username: admin mapUsers: | [] kind: ConfigMap  We can leverage eksctl to get a list of all identity managed in our cluster. Example:\neksctl get iamidentitymapping --cluster eksworkshop  arn:aws:iam::xxxxxxxxxx:role/eksctl-quick-nodegroup-ng-fe1bbb6-NodeInstanceRole-1KRYARWGGHPTT system:node:{{EC2PrivateDNSName}} system:bootstrappers,system:nodes arn:aws:iam::xxxxxxxxxx:role/k8sAdmin admin system:masters arn:aws:iam::xxxxxxxxxx:role/k8sDev dev-user arn:aws:iam::xxxxxxxxxx:role/k8sInteg integ-user  Here we have created:\n a RBAC role for K8sAdmin, that we map to admin user and give access to system:masters kubernetes Groups (so that it has Full Admin rights) a RBAC role for k8sDev that we map on dev-user in development namespace a RBAC role for k8sInteg that we map on integ-user in integration namespace  We will see on next section how we can test it.\n"
},
{
	"uri": "/70_workshop_7_multicloud/31_add_yamls.html",
	"title": "Add YAMLs",
	"tags": [],
	"description": "",
	"content": " Add YAMLs In this section we\u0026rsquo;re going to deploy resources using GitOps.\nMake sure you\u0026rsquo;re in your repo\ncd aws-gitops-multicloud Create a Pod Create a pod using gitops\ncp ../gitops-cluster-management/examples/k8s/pod.yaml flux-mgmt/pod.yaml git add flux-mgmt git commit -m \u0026#39;deploy nginx pod\u0026#39; git push Shortly after you should find the pod in the default namespace\nkubectl get pod Create a HelmRelease cp ../gitops-cluster-management/examples/k8s/helm/redis.yaml flux-mgmt/redis.yaml git add flux-mgmt git commit -m \u0026#39;deploy redis helmrelease\u0026#39; git push Validate that it worked by checking if the helmrelease and the pod are running\nkubectl get hr kubectl get pod Deploy secret-copier custom operator cp -R ../gitops-cluster-management/examples/k8s/custom-operators/ flux-mgmt git add flux-mgmt git commit -m \u0026#39;deploy custom operators\u0026#39; git push Now any secret or deployment in the default namespace that has the labels secret-copier: \u0026quot;yes\u0026quot; will be copied across namespaces.\nValidate that it worked by checking if the example secret and deployment in your cluster are copied across all namespaces.\nkubectl get secret -A | grep copy-me kubectl get deployment -A | grep memcached For more info you can go through the shell-operator source code for secret-copier and deployment-copier in gitops-cluster-management/operators.\nCreate ec2 clusters Let\u0026rsquo;s create two EC2 clusters using CAPI.\ncp -R ../gitops-cluster-management/examples/k8s/clusters/ flux-mgmt Modify the two cluster files ec2-cluster-1.yaml and ec2-cluster-2.yaml as follows:\n AWSCluster.spec.region to us-west-2 AWSCluster.spec.sshKeyName to weaveworks-workshop AWSMachineTemplate.spec.template.spec.sshKeyName to weaveworks-workshop (There should be 2 machine templates per cluster. One for the control plane instances and one for the worker instances)  Finally, let\u0026rsquo;s push our changes to git\ngit add flux-mgmt git commit -m \u0026#39;create two ec2 clusters\u0026#39; git push We can monitor cluster creation\nkubectl get clusters -w kubectl get machines -w kubectl logs --tail 100 -f -n capa-system deploy/capa-controller-manager -c manager Simulate a disaster kubectl get machines kubectl delete machine \u0026lt;MACHINE-NAME\u0026gt; Watch what happens\nkubectl get machines -w CAPI should take care of destroying the AWS EC2 instance, and provision a new one to replace it.\nScale up your cluster You can scale up or down our cluster instances by increasing or decreasing the number of replicas for the control plane or worker nodes in our yaml files.\nLet\u0026rsquo;s bump up the replicas from 3 to 5 for the worker nodes.\ngit add flux-mgmt git commit -m \u0026#39;scale up\u0026#39; git push Watch what happens\nkubectl get machines -w"
},
{
	"uri": "/22_workshop_1/20_gitops_enable/31_set_up_git.html",
	"title": "Set Up Your GIT Repository",
	"tags": [],
	"description": "",
	"content": "The most important ingredient using eksctl enable repo is the configuration of your repository (which will include your workload manifests, HelmReleases, etc). You can start with an empty repository and push that to Git, or use the one you intend to deploy to the cluster.\nThe main point of GitOps is to keep everything (config, alerts, dashboards, apps, literally everything) in Git and use it as a single source of truth. To keep your cluster configuration in Git, please go ahead and create an empty repository. On GitHub, for example, follow these steps.\nIn the Cloud9 environment, a user SSH key pair is not automatically created. Use these instructions to create an SSH key pair for use as your git repository deploy keys.\n You will also want to add your ssh. Instructions to do so are here.\nThe pbcopy may not work on amazon linix, so instead just run:\ncat ~/.ssh/id_rsa.pub # just copy content to clip manually using mouse/trackpad"
},
{
	"uri": "/22_workshop_1/20_gitops_enable/32_enable_gitops.html",
	"title": "Enable Your Cluster for GitOps",
	"tags": [],
	"description": "",
	"content": "The following command will set up your cluster with:\n Flux, Flux Helm Operator with Helm v3 support,  and add their manifests to Git, so you can configure them through pull requests.\nRun this command from any directory in your file system. eksctl will clone your repository in a temporary directory that will be removed later.\nEKSCTL_EXPERIMENTAL=true \\ eksctl enable repo \\ --git-url git@github.com:example/my-eks-config \\ --git-email your@email.com \\ --cluster your-cluster-name \\ --region your-cluster-region Let us go through the specified arguments one by one:\n \u0026ndash;git-url: this points to a Git URL where the configuration for your cluster will be stored. This will contain config for the workloads and infrastructure later on. \u0026ndash;git-email: the email used to commit changes to your config repository. \u0026ndash;cluster: the name of your cluster. Use eksctl get cluster to see all clusters in your default region. \u0026ndash;region: the region of your cluster.  There are more arguments and options, please refer to the GitOps reference of eksctl which details all the flags and resulting directory structure.\n"
},
{
	"uri": "/25_workshop_2_ha-dr/40_gitops_enable_clusters/32_enable_gitops.html",
	"title": "Enable Your Clusters for GitOps",
	"tags": [],
	"description": "",
	"content": " Perform this command in each of your two terminal windows. Verify that you are connecting to your two clusters individually. The cluster names and regions are the information provided when you created the clusters.\n The following command will set up your cluster with:\n Flux, Flux Helm Operator with Helm v3 support,  and add their manifests to Git, so you can configure them through pull requests.\nRun this command from any directory in your file system. eksctl will clone your repository in a temporary directory that will be removed later.\nEKSCTL_EXPERIMENTAL=true \\ eksctl enable repo \\ --git-url git@github.com:your-github-username/your-repo-name \\ --git-email your@email.com \\ --cluster your-cluster-name \\ --region your-cluster-region Let us go through the specified arguments one by one:\n \u0026ndash;git-url: this points to a Git URL where the configuration for your cluster will be stored. This will contain config for the workloads and infrastructure later on. \u0026ndash;git-email: the email used to commit changes to your config repository. \u0026ndash;cluster: the name of your cluster. Use eksctl get cluster to see all clusters in your default region. \u0026ndash;region: the region of your cluster.  There are more arguments and options, please refer to the GitOps reference of eksctl which details all the flags and resulting directory structure.\n"
},
{
	"uri": "/22_workshop_1/20_gitops_enable/33_lets_check_flux.html",
	"title": "Flux and GitOps",
	"tags": [],
	"description": "",
	"content": "The command will take a while to run and it\u0026rsquo;s a good idea to scan the output. You will note a similar bit of information in the log like this one:\n [ℹ] Flux will only operate properly once it has write-access to the Git repository ... [ℹ] please configure git@github.com:YOURUSER/eks-quickstart-app-dev.git so that the following Flux SSH public key has write access to it ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC8msUDG9tEEWHKKJw1o8BpwfMkCvCepeUSMa9iTVK6Bmxeu2pA/ivBS8Qgx/Lg8Jnu4Gk2RbXYMt3KL3/lcIezLwqipGmVvLgBLvUccbBpeUpWt+SlW2LMwcMOnhF3n86VOYjaRPggoPtWfLhFIfnkvKOFLHPRYS3nqyYspFeCGUmOzQim+JAWokf4oGOOX4SNzRKjusboh93oy8fvWk8SrtSwLBWXOKu+kKXC0ecZJK7G0jW91qb40QvB+VeSAbfk8LJZcXGWWvWa3W0/woKzGNWBPZz+pGuflUjVwQG5GoOq5VVWu71gmXoXBS3bUNqlu6nDobd2LlqiXNViaszX  Copy the lines starting with ssh-rsa and give it read/write access to your repository. For example, in GitHub, by adding it as a deploy key. There you can easily do this in the Settings \u0026gt; Deploy keys \u0026gt; Add deploy key. Just make sure you check Allow write access as well.\nThe next time Flux syncs from Git, it will start updating the cluster and actively deploying.\nIf you run git pull next, you will see that eksctl has committed them to your config repository already.\nIn our case we are going to see these new arrivals (flux and helm operator) running in the cluster:\n $ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE flux flux-56b5664cdd-nfzx2 1/1 Running 0 11m flux flux-helm-operator-6bc7c85bb5-l2nzn 1/1 Running 0 11m flux memcached-958f745c-dqllc 1/1 Running 0 11m kube-system aws-node-l49ct 1/1 Running 0 14m kube-system coredns-7d7755744b-4jkp6 1/1 Running 0 21m kube-system coredns-7d7755744b-ls5d9 1/1 Running 0 21m kube-system kube-proxy-wllff 1/1 Running 0 14m  All of the cluster configuration can be easily edited in Git now. Welcome to a fully GitOps\u0026rsquo;d world!\n"
},
{
	"uri": "/22_workshop_1/20_gitops_enable/34_update_flux_for_demo.html",
	"title": "Speed up Flux for Demo",
	"tags": [],
	"description": "",
	"content": "By default Flux will poll git every 5m but for the sake of the demo, we want to speed things up a bit.\nLets edit the flux deployment and add the following argument into around lne 180 --git-poll-interval\nEdit the following file in your prefered way:\n./flux/flux-deployment.yaml175 # Serve /metrics endpoint at different port; 176 # make sure to set prometheus\u0026#39; annotation to scrape the port value. 177 - --listen-metrics=:3031 178 179 # Additional arguments 180 - --sync-garbage-collection 190 - --git-poll-interval=1m"
},
{
	"uri": "/30_workshop_03_grc/150_iam-groups/40_test-cluster-access.html",
	"title": "Test EKS access",
	"tags": [],
	"description": "",
	"content": " Automate assumerole with aws cli It is possible to automate the retrieval of temporary credentials for the assumed role by configuring the aws cli using .aws/configand .aws/credentials files. Examples we will define 3 profile:\nadd in ~/.aws/config: cat \u0026lt;\u0026lt; EoF \u0026gt;\u0026gt; ~/.aws/config [profile admin] role_arn=arn:aws:iam::${ACCOUNT_ID}:role/k8sAdmin source_profile=eksAdmin [profile dev] role_arn=arn:aws:iam::${ACCOUNT_ID}:role/k8sDev source_profile=eksDev [profile integ] role_arn=arn:aws:iam::${ACCOUNT_ID}:role/k8sInteg source_profile=eksInteg EoF create ~/.aws/credentials: cat \u0026lt;\u0026lt; EoF \u0026gt; ~/.aws/credentials [eksAdmin] aws_access_key_id=$(jq -r .AccessKey.AccessKeyId /tmp/PaulAdmin.json) aws_secret_access_key=$(jq -r .AccessKey.SecretAccessKey /tmp/PaulAdmin.json) [eksDev] aws_access_key_id=$(jq -r .AccessKey.AccessKeyId /tmp/JeanDev.json) aws_secret_access_key=$(jq -r .AccessKey.SecretAccessKey /tmp/JeanDev.json) [eksInteg] aws_access_key_id=$(jq -r .AccessKey.AccessKeyId /tmp/PierreInteg.json) aws_secret_access_key=$(jq -r .AccessKey.SecretAccessKey /tmp/PierreInteg.json) EoF Test this with dev profile: aws sts get-caller-identity --profile dev  { \"UserId\": \"AROAUD5VMKW75WJEHFU4X:botocore-session-1581687024\", \"Account\": \"xxxxxxxxxx\", \"Arn\": \"arn:aws:sts::xxxxxxxxxx:assumed-role/k8sDev/botocore-session-1581687024\" }  The assumed-role is k8sDev, so we achieved our goal\nWhen specifying the \u0026ndash;profile dev parameter we automatically ask for temporary credentials for the role k8sDev You can test this with integ and admin also.\nWith admin:\naws sts get-caller-identity --profile admin { \u0026#34;UserId\u0026#34;: \u0026#34;AROAUD5VMKW77KXQAL7ZX:botocore-session-1582022121\u0026#34;, \u0026#34;Account\u0026#34;: \u0026#34;xxxxxxxxxx\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:sts::xxxxxxxxxx:assumed-role/k8sAdmin/botocore-session-1582022121\u0026#34; } When specifying the \u0026ndash;profile admin parameter we automatically ask for temporary credentials for the role k8sAdmin\nUsing AWS profiles with Kubectl config file It is also possible to specify the AWS_PROFILE uses with the aws-iam-authenticator in the .kube/config file, so that it will use the appropriate profile.\nwith dev profile Create new KUBECONFIG file to test this:\nexport KUBECONFIG=/tmp/kubeconfig-dev \u0026amp;\u0026amp; eksctl utils write-kubeconfig eksworkshop cat $KUBECONFIG | sed \u0026#39;s/eksworkshop./eksworkshop-dev./g\u0026#39; \u0026gt; ~/.kube/dev cat \u0026gt;\u0026gt; $KUBECONFIG \u0026lt;\u0026lt;- EOM - name: AWS_PROFILE value: dev EOM rm /tmp/kubeconfig-integexport KUBECONFIG=~/.kube/dev We just added the --profile dev parameter to our kubectl config file, so that this will ask kubectl to use our IAM role associated to our dev profile.\nWith this configuration we should be able to interract with the development namespace, because it as our RBAC role defined.\nlet\u0026rsquo;s create a pod\nkubectl run nginx-dev --image=nginx -n development We can list the pods\nkubectl get pods -n development  NAME READY STATUS RESTARTS AGE nginx-dev 1/1 Running 0 28h  but not in other namespaces\nkubectl get pods -n integration   Error from server (Forbidden): pods is forbidden: User \"dev-user\" cannot list resource \"pods\" in API group \"\" in the namespace \"integration\"  Test with integ profile export KUBECONFIG=/tmp/kubeconfig-integ \u0026amp;\u0026amp; eksctl utils write-kubeconfig eksworkshop cat $KUBECONFIG | sed \u0026#39;s/eksworkshop./eksworkshop-integ./g\u0026#39; \u0026gt; ~/.kube/integ cat \u0026gt;\u0026gt; $KUBECONFIG \u0026lt;\u0026lt;- EOM - name: AWS_PROFILE value: integ EOM rm /tmp/kubeconfig-integexport KUBECONFIG=~/.kube/integ let\u0026rsquo;s create a pod\nkubectl run nginx-integ --image=nginx -n integration We can list the pods\nkubectl get pods -n integration  NAME READY STATUS RESTARTS AGE nginx-integ 1/1 Running 0 43s pre but not in other namespaces ``` kubectl get pods -n development ```  Error from server (Forbidden): pods is forbidden: User \"integ-user\" cannot list resource \"pods\" in API group \"\" in the namespace \"development\"  Test with admin profile export KUBECONFIG=/tmp/kubeconfig-admin \u0026amp;\u0026amp; eksctl utils write-kubeconfig eksworkshop cat $KUBECONFIG | sed \u0026#39;s/eksworkshop./eksworkshop-integ./g\u0026#39; \u0026gt; ~/.kube/admin cat \u0026gt;\u0026gt; $KUBECONFIG \u0026lt;\u0026lt;- EOM - name: AWS_PROFILE value: admin EOM rm /tmp/kubeconfig-adminexport KUBECONFIG=~/.kube/admin let\u0026rsquo;s create a pod in default namespace\nkubectl run nginx-admin --image=nginx  We can list the pods\nkubectl get pods   NAME READY STATUS RESTARTS AGE nginx-integ 1\u0026frasl;1 Running 0 43s \u0026gt;\nWe can list ALL pods in all namespaces\nkubectl get pods -A  NAMESPACE NAME READY STATUS RESTARTS AGE default nginx-admin 1\u0026frasl;1 Running 0 15s development nginx-dev 1\u0026frasl;1 Running 0 11m integration nginx-integ 1\u0026frasl;1 Running 0 4m29s kube-system aws-node-mzbh4 1\u0026frasl;1 Running 0 100m kube-system aws-node-p7nj7 1\u0026frasl;1 Running 0 100m kube-system aws-node-v2kg9 1\u0026frasl;1 Running 0 100m kube-system coredns-85bb8bb6bc-2qbx6 1\u0026frasl;1 Running 0 105m kube-system coredns-85bb8bb6bc-87ndr 1\u0026frasl;1 Running 0 105m kube-system kube-proxy-4n5lc 1\u0026frasl;1 Running 0 100m kube-system kube-proxy-b65xm 1\u0026frasl;1 Running 0 100m kube-system kube-proxy-pr7k7 1\u0026frasl;1 Running 0 100m \u0026gt;\nSwithing between different contexts It is possible to merge several kubernetes API access in the same KUBECONFIG file, or just tell Kubectl several file to lookup at once:\nexport KUBECONFIG=~/.kube/dev:~/.kube/integ:~/.kube/admin # then run the following to get a list of contexts kubectl config get-contexts kubectl config current-context kubectl config use-context my-cluster-name Conclusion In this module, we have seen how to configure EKS to provide finer access to users combining IAM Groups and Kubernetes RBAC. You\u0026rsquo;ll be able to create different groups depending on your needs, configure their associated RBAC access in your cluster, and simply add or remove users from the group to grand or remove them access to your cluster.\nUsers will only have to configure their aws cli in order to automatically retrievfe their associated rights in your cluster.\n"
},
{
	"uri": "/40_workshop_4_hipo-teams/60_install-pod-info/40_add_app_mesh.html",
	"title": "Create App Mesh Resources",
	"tags": [],
	"description": "",
	"content": "We are now going to the the following App Mesh resources for our PodInfo application:\n Virtual Nodes - for our existing PodInfo services (physical) and also for our new \u0026ldquo;virtual\u0026rdquo; backend service Virtual Service - defines a new \u0026ldquo;virtual\u0026rdquo; service for the backend that will distribute traffic between the existing v1 and v2 backend PodInfo services/pods  To do this download the yaml files into the apps folder:\n# Virtual nodes curl https://weaveworks-gitops.awsworkshop.io/40_workshop_4_hipo-teams/60_install-pod-info/deploy.files/2-podinfo-virtual-nodes.yaml -o apps/2-podinfo-virtual-nodes.yaml # Virtual Services curl https://weaveworks-gitops.awsworkshop.io/40_workshop_4_hipo-teams/60_install-pod-info/deploy.files/3-podinfo-virtual-services.yaml -o apps/3-podinfo-virtual-services.yaml # Placeholder services curl https://weaveworks-gitops.awsworkshop.io/40_workshop_4_hipo-teams/60_install-pod-info/deploy.files/4-podinfo-placeholder-services.yaml -o apps/4-podinfo-placeholder-services.yaml You folder structure should look similar to:\n. ├── amazon-cloudwatch │ ├── cwagent-fluentd-quickstart.yaml │ └── cwagent-prometheus-eks.yaml ├── appmesh-system │ ├── appmesh-controller.yaml │ ├── appmesh-inject.yaml │ ├── appmesh-prometheus.yaml │ └── crds.yaml ├── apps │ ├── 1-podinfo.yaml │ ├── 2-podinfo-virtual-nodes.yaml │ ├── 3-podinfo-virtual-services.yaml │ └── 4-podinfo-placeholder-services.yaml ├── namespaces │ ├── amazon-cloudwatch.yaml │ ├── appmesh-system.yaml │ └── apps.yaml └── README.md Add and then commit the 3 file and push the the changes to your GitHub repo.\nFlux will now see that the desired state of the apps namespace has changed in Git and will apply the resources to our cluster. This will take up to 1 minute to apply.\nCheck that that the deployment and services has been created by running the following command:\nkubectl get virtualservices,virtualnodes,svc -n apps You should see the new resources:\nvirtualservice.appmesh.k8s.aws/backend-podinfo.apps.svc.cluster.local 26s NAME AGE virtualnode.appmesh.k8s.aws/backend-podinfo 26s virtualnode.appmesh.k8s.aws/backend-podinfo-v1 26s virtualnode.appmesh.k8s.aws/backend-podinfo-v2 26s virtualnode.appmesh.k8s.aws/frontend-podinfo 26s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/backend-podinfo ClusterIP 10.100.39.139 \u0026lt;none\u0026gt; 9898/TCP 26s service/backend-podinfo-v1 ClusterIP 10.100.52.43 \u0026lt;none\u0026gt; 9898/TCP,9999/TCP 3m29s service/backend-podinfo-v2 ClusterIP 10.100.34.160 \u0026lt;none\u0026gt; 9898/TCP,9999/TCP 3m29s service/frontend-podinfo ClusterIP 10.100.73.93 \u0026lt;none\u0026gt; 9898/TCP,9999/TCP 3m29s"
},
{
	"uri": "/22_workshop_1/30_deploy_sample_app.html",
	"title": "Deploy Sock Shop Application (Optional)",
	"tags": [],
	"description": "",
	"content": " Shop Sock For our labs, we picked up the Sock Shop application. Sock Shop is a microservice sample application that Weaveworks initially developed for demoing and testing purposes. We made it open source so it can be used by other organizations for learning and demonstration purposes.\nShop Sock Architecture The architecture of the demo microserivces application was intentionally designed to provide as many microservices as possible. Please don\u0026rsquo;t use the Sock Shop application as a model for a well architected micro-services application, it was built for demonstration purposes. If you are just beginning to architect your own micro-services based cloud native application, Weaveworks would be happy to help recommend the correct architecture for your use case..\nFurthermore, Sock Shop is intentionally uses a number of different languages and technologies in order to demonstrate how Kubernetes works for a number of different technologies. Again, we\u0026rsquo;d recommend that you only consider new technologies based upon a need.\nAs seen in the image above, the microservices are roughly defined by the functions of an e-commerce site. Networks are specified, but due to technology limitations may not be implemented in some deployments.\nAll services communicate using REST over HTTP. This was chosen due to the simplicity of development and testing. Their API specifications are under development.\n Setup Kubernetes labels   "
},
{
	"uri": "/40_workshop_4_hipo-teams/50_install_container_insights/40_explore_container_insights.html",
	"title": "Explore Container Insights",
	"tags": [],
	"description": "",
	"content": "We will now explore some of the observability that Container Insights gives us. This is a very quick look at CloudWatch\u0026hellip;.it can do a lot!\nIn the AWS Console make sure you are in CloudWatch (which you will probably be from the last section).\nOn the left navigation pane click Resources under Container Insights.\nClick any of the items from the list of Resources and you will see a performance dashboard open giving you an insight into the performance:\nThere is a specific resource for Prometheus AppMesh from the left most dropdown box on the Performance Monitoring pane. This shows a dashboard specifically for App Mesh. We will return to this later.\nOn the left navigation pane click Metrics. Then click ContainerInsights under Custom Namespaces. You will then be shown a list of groups at different granularities. Click ClusterName, Namespace.\nThis will show a list of the metrics that you can add to the graph. Check the box next to these 2 metrics:\nThe graph will then be updated to display the metrics.\nNow click on Log Groups under Logs. You will see a number of log groups created by Container Insights for our cluster, these are prefixed with /aws/containerinsights/gitopsworkshop.\nClick /aws/containerinsights/gitopsworkshop/application. You will then see a list of the log streams for your pods. Select any log stream from the list and you will be taken to the logs:\nThis has been a brief look at what Container Insights and CloudWatch gives us for observability of our EKS cluster. CloudWatch enables you to do a lot more including creating alarms (a.k.a alerts), dashboards, end-to-end tracing and a lot more!\n "
},
{
	"uri": "/25_workshop_2_ha-dr/40_gitops_enable_clusters.html",
	"title": "GitOps Enable Clusters",
	"tags": [],
	"description": "",
	"content": " GitOps Enable the Two EKS Clusters With two terminal windows open, you\u0026rsquo;ll have to be careful when executing commands to ensure you are working with the correct cluster. In these exercises, you will be flipping back and forth between your two terminal windows fairly often. As this can get confusing, just be sure to verify which window you are using each time.\n"
},
{
	"uri": "/70_workshop_7_multicloud/40_gitops_enable_clusters.html",
	"title": "GitOps Enable Clusters",
	"tags": [],
	"description": "",
	"content": " GitOps Enable the Two EC2 Clusters With two terminal windows open, you\u0026rsquo;ll have to be careful when executing commands to ensure you are working with the correct cluster. In these exercises, you will be flipping back and forth between your two terminal windows fairly often. As this can get confusing, just be sure to verify which window you are using each time.\nLet\u0026rsquo;s create two new kubeconfigs for the two EC2 clusters we have. The admin kubeconfigs are stored as secrets on the management cluster.\nkubectl get secret | grep ec2-cluster Let\u0026rsquo;s write the kubeconfig files. (Replace SECRETNAME with your cluster\u0026rsquo;s secret name)\nkubectl --namespace=default get secret/SECRETNAME-1 -o jsonpath={.data.value} \\  | base64 --decode \\  \u0026gt; ./ec2-cluster-1.kubeconfig kubectl --namespace=default get secret/SECRETNAME-2 -o jsonpath={.data.value} \\  | base64 --decode \\  \u0026gt; ./ec2-cluster-2.kubeconfig Open a new terminal for each cluster and in each one point KUBECONFIG to the right file.\n# NOTE: don\u0026#39;t run this on the terminal tab we\u0026#39;re using for the management cluster # terminal for cluster 1 export KUBECONFIG=./ec2-cluster-1.kubeconfig # terminal for cluster 2 export KUBECONFIG=./ec2-cluster-1.kubeconfig GitOps Enable the Clusters Typically we would use a different repo for the target clusters. But for today we\u0026rsquo;ll reuse the same git repo under a different directory called flux-ec2.\n Install Flux\nkubectl create namespace fluxcd helm repo add fluxcd https://charts.fluxcd.io kubectl apply -f https://raw.githubusercontent.com/fluxcd/helm-operator/master/deploy/crds.yaml helm upgrade -i flux fluxcd/flux --wait \\ --namespace fluxcd \\ --set git.url=git@github.com:${GIT_USER}/${GIT_REPO_NAME}.git \\ --set git.path=\u0026#34;flux-ec2\u0026#34; \\ --set git.timeout=120s \\ --set git.pollInterval=1m \\ --set syncGarbageCollection.enabled=true \\ --set rbac.create=true  Get flux\u0026rsquo;s public key\nfluxctl identity --k8s-fwd-ns fluxcd  Copy printed public key and paste it in your git repo\u0026rsquo;s Settings \u0026gt; Deploy Keys \u0026gt; Add Deploy Key. Make sure to turn on write access. If no key shows up, try re-running fluxctl identity --k8s-fwd-ns fluxcd until it shows up.\n Install Helm Operator\nhelm upgrade helm-operator fluxcd/helm-operator \\ --force \\ -i \\ --wait \\ --namespace fluxcd \\ --set helm.versions=v3  Setup CNI on all ec2 clusters cp ../gitops-cluster-management/examples/cni/weavenet.yaml flux-ec2/weavenet.yaml git add flux-ec2 git commit -m \u0026#39;deploy cni to ec2 clusters\u0026#39; git push Deploy applications across clusters cp ../gitops-cluster-management/examples/k8s/pod.yaml flux-ec2/pod.yaml git add flux-ec2 git commit -m \u0026#39;deploy nginx pod to ec2 clusters\u0026#39; git push"
},
{
	"uri": "/40_workshop_4_hipo-teams/40_install-app-mesh.html",
	"title": "Install App Mesh",
	"tags": [],
	"description": "",
	"content": " Install App Mesh In this section we will be deploying components to our EKS cluster that will enable integration with AWS App Mesh.\nAWS App Mesh is a service mesh that provides application-level networking. It allows services to communicate across different types of compute infrastructure (e.g. EKS, EC2). It also provides end-to-end visibility of how your services communicate with each other.\nApp Mesh is made up of quite a few components including:\n Service mesh – A service mesh is a logical boundary for network traffic between the services that reside within it Virtual services – A virtual service is an abstraction of an actual service that is provided by a virtual node, directly or indirectly, by means of a virtual router. Virtual nodes – A virtual node acts as a logical pointer to a discoverable service, such as an Amazon ECS or Kubernetes service. For each virtual service, you will have at least one virtual node. Virtual routers and routes – Virtual routers handle traffic for one or more virtual services within your mesh. A route is associated to a virtual router. The route is used to match requests for the virtual router and to distribute traffic to its associated virtual nodes  For more information see the AWS App Mesh Docs.\n"
},
{
	"uri": "/50_workshop_5_accelerating_sdlc/00_prerequisites.md/40_install_kustomize.html",
	"title": "Install kustomize",
	"tags": [],
	"description": "",
	"content": "Kustomize lets you customize raw, template-free YAML files for multiple purposes, leaving the original YAML untouched and usable as is.\nWe will be using kustomize to create more targeted patches that make our code easier to factor, understand, and reuse.\nInstall kustomize for Linux:\ncurl --silent --location --remote-name \\ \u0026#34;https://github.com/kubernetes-sigs/kustomize/releases/download/kustomize/v3.2.3/kustomize_kustomize.v3.2.3_linux_amd64\u0026#34; \u0026amp;\u0026amp; \\ chmod a+x kustomize_kustomize.v3.2.3_linux_amd64 \u0026amp;\u0026amp; \\ sudo mv kustomize_kustomize.v3.2.3_linux_amd64 /usr/local/bin/kustomize Verify the install with:\nkustomize version"
},
{
	"uri": "/60_workshop_6_ml/00_prerequisites.md/40_install_kustomize.html",
	"title": "Install kustomize",
	"tags": [],
	"description": "",
	"content": "Kustomize lets you customize raw, template-free YAML files for multiple purposes, leaving the original YAML untouched and usable as is.\nWe will be using kustomize to create more targeted patches that make our code easier to factor, understand, and reuse.\nInstall kustomize for Linux:\ncurl --silent --location --remote-name \\ \u0026#34;https://github.com/kubernetes-sigs/kustomize/releases/download/kustomize/v3.2.3/kustomize_kustomize.v3.2.3_linux_amd64\u0026#34; \u0026amp;\u0026amp; \\ chmod a+x kustomize_kustomize.v3.2.3_linux_amd64 \u0026amp;\u0026amp; \\ sudo mv kustomize_kustomize.v3.2.3_linux_amd64 /usr/local/bin/kustomize Verify the install with:\nkustomize version"
},
{
	"uri": "/60_workshop_6_ml/40_kubeflow.html",
	"title": "Kubeflow",
	"tags": [],
	"description": "",
	"content": "In this section, we will deploy Kubeflow\n"
},
{
	"uri": "/50_workshop_5_accelerating_sdlc/40_progressive_delivery.html",
	"title": "Progressive Delivery",
	"tags": [],
	"description": "",
	"content": " Progressive Delivery In this section, we will create a Canary deployment using Flagger to incrementally introduce traffic to new versions of your services. Documentation for Flagger can be found here.\nWe will demonstrate a successful Canary deployment and how an unsuccessful Canary deployment automatically gets rolled back without impacting application uptime.\n"
},
{
	"uri": "/40_workshop_4_hipo-teams/30_gitops_enable_clusters/40_verify_flux_op.html",
	"title": "Verify Flux and Helm are Operational",
	"tags": [],
	"description": "",
	"content": "Now we have installed Flux and Helm Operator we need to verify that they are both operational.\nRun the following command in your Cloud9 terminal:\nkubectl get pods -n flux You should see that there are 3 pods and the status should be Running:\nNAME READY STATUS RESTARTS AGE flux-5884545b8c-thfxh 1/1 Running 0 22m flux-memcached-97fc488-zwn8w 1/1 Running 0 22m helm-operator-6489b5cc6b-5pdvl 1/1 Running 0 7m21s If the status isn\u0026rsquo;t running then don\u0026rsquo;t proceed with the workshop until it\u0026rsquo;s fixed. Please ask for help.\n "
},
{
	"uri": "/40_workshop_4_hipo-teams.html",
	"title": "Workshop Module 4: How to Build High Performing Team Operations",
	"tags": [],
	"description": "",
	"content": " Welcome to How to Build High Performing Team Operations Many modern enterprises embrace DevOps methodologies - automating and streamlining the integration and deployment processes - in order to increase development speed and reliability. GitOps takes DevOps principles one step further while leveraging best practices from managing application infrastructure as code.\nGitOps allows development teams to use the tools that they are already familiar with - Git. Updates and features can be released more rapidly without knowing internals of Kubernetes or other operational procedures.\nOperators can enable a self service model for developers while reducing overhead on lifecycle management of workloads and clusters.\nIn this workshop module we will learn:\n Minimize K8s steep learning curve with GitOps for EKS clusters Install a sample app, release and rollback versions via PR with Flux \u0026amp; Helm Operator Monitor and observe effectively with App Mesh, Prometheus and Cloudwatch  Need help in getting started with EKS? Sign up for our EKS + GitOps Quickstart package.\nGitOps - an operating model for cloud native GitOps can be summarized as these two things:\n An operating model for Kubernetes and other cloud native technologies, providing a set of best practices that unify deployment, management and monitoring for containerized clusters and applications. A path towards a developer experience for managing applications; where end-to-end CICD pipelines and git workflows are applied to both operations, and development.  Weaveworks Weaveworks makes it fast and simple for developers and DevOps teams to build and operate powerful containerized applications. We minimize the complexity of operating workloads in Kubernetes by providing automated continuous delivery pipelines, observability and monitoring.\nWeaveworks’ mission is to minimize complexities in operating workloads and provide a developer centric operating model for cloud native applications. Our goal is to drive cloud native transformation for your developers and portability, flexibility, choice and stability for your business.\nOne of the first members of the Cloud Native Computing Foundation, Weaveworks also contributes to several open source projects, including Weave Net, Weave Scope, Weave Cortex, Weave Flux, Firekube and Flagger.\nWeaveworks and AWS Weaveworks is an APN Advanced Technology Partner and AWS Containers Competency Partner. In addition Weaveworks is the EKS certified Advanced partner that built eksctl.\nWeaveworks Pioneered GitOps Learn more about GitOps, a set of modern best practices for deploying and managing applications with cloud native tools and cloud services.\nDon’t know where to start? Our experienced team of Customer Reliability Engineers can provide the highest level of care, advice and coaching. Our expertise is rooted in working and supporting live production systems at scale. Get started with our EKS + GitOps Quickstart package.\nFor further reading we recommend:  Whitepaper: Implementing a Kubernetes Strategy in your Organization Whitepaper: Automating Kubernetes with GitOps Whitepaper: Guide to a product ready Kubernetes cluster Checklist: Production Readiness for Kubernetes Case Study: Fidelity Case Study: Mettle  For hands on instructions, we recommend: Tutorial: A practical guide to GitOps\n"
},
{
	"uri": "/22_workshop_1/30_deploy_sample_app/41_import_k8s_labels_annotations.html",
	"title": "Setup Kubernetes labels",
	"tags": [],
	"description": "",
	"content": " Explore metadata in pod definitions List all the Sock Shop pods running:\nkubectl get po -l product=sockshop --all-namespaces  Pick up a pod and a namespace (production or dev) and get the pods details, including the Labels and the Annotations.\nkubectl describe po \u0026lt;pod_name\u0026gt; -n \u0026lt;namespace\u0026gt; Grant viewer role to service accounts Those Labels and Annotations are centrally defined and managed in Kubernetes but we also want them available in Weaveworks for grouping and filtering purposes.\nThe OneAgent will use a pod service account to query for this metadata via the Kubernetes REST API.\nThe service accounts must be granted viewer role in order to have this access.\nIn the terminal, execute the following command to grant viewer role. This needs to be done for each namespace.\nkubectl create rolebinding serviceaccounts-view --clusterrole=view --group=system:serviceaccounts:production --namespace=production You can repeat the procedure for the dev namespace.\nkubectl create rolebinding serviceaccounts-view --clusterrole=view --group=system:serviceaccounts:dev --namespace=dev Wait\u0026hellip; Wait a few minutes 😀 seriously, let\u0026rsquo;s take a 10 minutes break here\n"
},
{
	"uri": "/25_workshop_2_ha-dr/50_add_yamls.html",
	"title": "Add YAMLs to Repo",
	"tags": [],
	"description": "",
	"content": " Add the YAML Files to your Repository In this section, we will be adding Kubernetes manifests to your git repository. Each one of the manifests installs specific features and applications.\nYou can either edit these files directly in your git repository using the \u0026ldquo;Create File\u0026rdquo; function in the git repository web user interface, or you can clone the repository into one of your Cloud9 terminal sessions, and edit them there. If you use the git clone method, do not forget to git add . and git commit -m \u0026quot;add commit message\u0026quot; your changes, and git push them to the repository.\nThe software agent, flux, will check for new commits to the git repository, and apply the changes to the manifests automatically to each of your clusters.\n"
},
{
	"uri": "/70_workshop_7_multicloud/50_cleanup.html",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": " Cleanup  Delete the ssh key we added to your github Delete the deploy key in https://github.com/YOURUSERNAME/gitops-cluster-management/settings/keys we added to your github  "
},
{
	"uri": "/60_workshop_6_ml/50_titanic_sample_application.html",
	"title": "Example application - Titanic Survival Prediction",
	"tags": [],
	"description": "",
	"content": " Titanic Survival Prediction In this section, we will build and deploy a sample application Titanic Survival Prediction\nThe sample runs a Spark ML pipeline to train a classfication model using random forest on AWS Elastic Map Reduce(EMR).\nTitanic: Machine Learning from Disaster copy of Jeffwan\u0026rsquo;s code for gitops workshop Also pipeline sample from kubeflow/pipeline\n"
},
{
	"uri": "/50_workshop_5_accelerating_sdlc/00_prerequisites.md/50_install_aws_iam_auth.html",
	"title": "Install aws iam authenticator",
	"tags": [],
	"description": "",
	"content": "Amazon EKS uses IAM to provide authentication to your Kubernetes cluster through the AWS IAM authenticator for Kubernetes. You can configure the stock kubectl client to work with Amazon EKS by installing the AWS IAM authenticator for Kubernetes and modifying your kubectl configuration file to use it for authentication.\nTo install aws-iam-authenticator on Cloud9\nDownload the Amazon EKS-vended aws-iam-authenticator binary from Amazon S3:\ncurl -o aws-iam-authenticator https://amazon-eks.s3.us-west-2.amazonaws.com/1.15.10/2020-02-22/bin/linux/amd64/aws-iam-authenticator Apply execute permissions to the binary:\nchmod +x ./aws-iam-authenticator And move it into a common directory:\nsudo mv ./aws-iam-authenticator /usr/local/bin Test that the aws-iam-authenticator binary works:\naws-iam-authenticator help"
},
{
	"uri": "/60_workshop_6_ml/00_prerequisites.md/50_install_aws_iam_auth.html",
	"title": "Install aws iam authenticator",
	"tags": [],
	"description": "",
	"content": "Amazon EKS uses IAM to provide authentication to your Kubernetes cluster through the AWS IAM authenticator for Kubernetes. You can configure the stock kubectl client to work with Amazon EKS by installing the AWS IAM authenticator for Kubernetes and modifying your kubectl configuration file to use it for authentication.\nTo install aws-iam-authenticator on Cloud9\nDownload the Amazon EKS-vended aws-iam-authenticator binary from Amazon S3:\ncurl -o aws-iam-authenticator https://amazon-eks.s3.us-west-2.amazonaws.com/1.15.10/2020-02-22/bin/linux/amd64/aws-iam-authenticator Apply execute permissions to the binary:\nchmod +x ./aws-iam-authenticator And move it into a common directory:\nsudo mv ./aws-iam-authenticator /usr/local/bin Test that the aws-iam-authenticator binary works:\naws-iam-authenticator help"
},
{
	"uri": "/40_workshop_4_hipo-teams/50_install_container_insights.html",
	"title": "Install Container Insights",
	"tags": [],
	"description": "",
	"content": " Install Container Insights In the previous section we setup Prometheus to collect the metrics from App Mesh. Ideally we want to have the metrics available in Cloudwatch. We can use the Container Insights which recently added support for collecting prometheus metrics.\nWe will follow a GitOps methodology to install Container Insights.\n"
},
{
	"uri": "/40_workshop_4_hipo-teams/40_install-app-mesh/50_install_appmesh_prometheus.html",
	"title": "Install Prometheus for App Mesh",
	"tags": [],
	"description": "",
	"content": "We also want to collect the metrics that App Mesh publishes and so we will also deploy Prometheus. We can use the metrics to create alerts or to help enable progressive delivery (covered in a later workshop).\nPrometheus for App Mesh is also available as a Helm package from the EKS Chart Repository. We will use the Helm Operator along with this chart to install the injector.\nCreate a new file called appmesh-prometheus.yaml in the appmesh-system folder. Add the following as contents to the file:\n--- --- apiVersion: helm.fluxcd.io/v1 kind: HelmRelease metadata: name: appmesh-prometheus namespace: appmesh-system spec: releaseName: appmesh-prometheus chart: repository: https://aws.github.io/eks-charts/ name: appmesh-prometheus version: 0.3.0 Your folder structure should look like this now:\n. ├── appmesh-system │ ├── appmesh-controller.yaml │ ├── appmesh-inject.yaml │ ├── appmesh-prometheus.yaml │ └── crds.yaml ├── namespaces │ └── appmesh-system.yaml └── README.md Add and then commit the appmesh-prometheus.yaml file and push the the changes to your GitHub repo.\nFlux will now see that the desired state of the appmesh-system namespace has changed in Git and will apply the CRDs to our cluster. This will take up to 1 minute to apply.\nCheck that Prometheus is up \u0026amp; running by running the following command:\nkubectl get pods -n appmesh-system You should see the prometheus pod in a Running state:\nappmesh-controller-54dd6bdfd8-n8zlq 1/1 Running 0 45m appmesh-inject-55cdc99595-qm8pt 1/1 Running 0 15m appmesh-prometheus-5cb5d88d6-pbg6w 1/1 Running 0 64s"
},
{
	"uri": "/20_weaveworks_prerequisites/50_workspaceiam.html",
	"title": "Update IAM settings for your Workspace",
	"tags": [],
	"description": "",
	"content": " Cloud9 normally manages IAM credentials dynamically. This isn\u0026rsquo;t currently compatible with the EKS IAM authentication, so we will disable it and rely on the IAM role instead.\n  Return to your workspace and click the gear icon (in top right corner), or click to open a new tab and choose \u0026ldquo;Open Preferences\u0026rdquo; Select AWS SETTINGS Turn off AWS managed temporary credentials Close the Preferences tab   To ensure temporary credentials aren\u0026rsquo;t already in place we will also remove any existing credentials file:\nrm -vf ${HOME}/.aws/credentials We should configure our aws cli with our current region as default.\nIf you are at an AWS event, ask your instructor which AWS region to use.\n # install jq sudo yum install jq Then, export the ACCOUNT_ID and AWS_REGION as variables:\nexport ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account) \u0026amp;\u0026amp;\\ export AWS_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r \u0026#39;.region\u0026#39;) Check if AWS_REGION is set to desired region\ntest -n \u0026#34;$AWS_REGION\u0026#34; \u0026amp;\u0026amp; echo AWS_REGION is \u0026#34;$AWS_REGION\u0026#34; || echo AWS_REGION is not set Let\u0026rsquo;s save these into bash_profile\necho \u0026#34;export ACCOUNT_ID=${ACCOUNT_ID}\u0026#34; | tee -a ~/.bash_profile echo \u0026#34;export AWS_REGION=${AWS_REGION}\u0026#34; | tee -a ~/.bash_profile aws configure set default.region ${AWS_REGION} aws configure get default.region Validate the IAM role Use the GetCallerIdentity CLI command to validate that the Cloud9 IDE is using the correct IAM role.\nJust run this to try it out yourself.\naws sts get-caller-identity Here is a script that will validate you have the right role.\naws sts get-caller-identity --query Arn | grep TeamRole -q \u0026amp;\u0026amp; echo \u0026#34;IAM role valid\u0026#34; || echo \u0026#34;IAM role NOT valid\u0026#34; If the IAM role is not valid, DO NOT PROCEED. Go back and confirm the steps on this page.\n"
},
{
	"uri": "/40_workshop_4_hipo-teams/60_install-pod-info/50_verify_app_mesh.html",
	"title": "Verify App Mesh Resources",
	"tags": [],
	"description": "",
	"content": "When we created the VirtualNode and VirtualService resources in EKS the controller acted on these and updated AWS App Mesh.\nTo check that the mesh has been created we can use the AWS CLI. Run the following command:\naws appmesh list-virtual-routers --mesh-name=apps The output should be similar to below and you should see apps listed:\n{ \u0026#34;virtualRouters\u0026#34;: [ { \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:appmesh:us-west-2:1234567890:mesh/apps/virtualRouter/backend-podinfo-router-apps\u0026#34;, \u0026#34;meshName\u0026#34;: \u0026#34;apps\u0026#34;, \u0026#34;meshOwner\u0026#34;: \u0026#34;1234567890\u0026#34;, \u0026#34;resourceOwner\u0026#34;: \u0026#34;1234567890\u0026#34;, \u0026#34;virtualRouterName\u0026#34;: \u0026#34;backend-podinfo-router-apps\u0026#34; } ] } Lets check that the traffic to the backend service has been configured to be routed 50:50 between v1 and v2:\naws appmesh describe-route --mesh-name=apps --virtual-router-name=backend-podinfo-router-apps --route-name=podinfo-route Hopefully you will see something similar to the following:\n{ \u0026#34;route\u0026#34;: { \u0026#34;meshName\u0026#34;: \u0026#34;apps\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:appmesh:us-west-2:1234567890:mesh/apps/virtualRouter/backend-podinfo-router-apps/route/podinfo-route\u0026#34;, \u0026#34;createdAt\u0026#34;: 1590071971.866, \u0026#34;lastUpdatedAt\u0026#34;: 1590071971.866, \u0026#34;meshOwner\u0026#34;: \u0026#34;1234567890\u0026#34;, \u0026#34;resourceOwner\u0026#34;: \u0026#34;1234567890\u0026#34;, \u0026#34;uid\u0026#34;: \u0026#34;aaa9b597-ede4-4d7a-8ca0-57682865003f\u0026#34;, \u0026#34;version\u0026#34;: 1 }, \u0026#34;routeName\u0026#34;: \u0026#34;podinfo-route\u0026#34;, \u0026#34;spec\u0026#34;: { \u0026#34;httpRoute\u0026#34;: { \u0026#34;action\u0026#34;: { \u0026#34;weightedTargets\u0026#34;: [ { \u0026#34;virtualNode\u0026#34;: \u0026#34;backend-podinfo-v1-apps\u0026#34;, \u0026#34;weight\u0026#34;: 50 }, { \u0026#34;virtualNode\u0026#34;: \u0026#34;backend-podinfo-v2-apps\u0026#34;, \u0026#34;weight\u0026#34;: 50 } ] }, \u0026#34;match\u0026#34;: { \u0026#34;prefix\u0026#34;: \u0026#34;/\u0026#34; } } }, \u0026#34;status\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;ACTIVE\u0026#34; }, \u0026#34;virtualRouterName\u0026#34;: \u0026#34;backend-podinfo-router-apps\u0026#34; } } You can also check by going to the AWS Console and choosing AWS App Mesh from the Services menu.\n"
},
{
	"uri": "/50_workshop_5_accelerating_sdlc.html",
	"title": "Workshop Module 5: Accelerating the Software Development Lifecycle",
	"tags": [],
	"description": "",
	"content": " Welcome to Accelerate the Software Development Lifecycles \u0026ldquo;I feel the need - the need for speed.\u0026rdquo; Many companies have chosen Kubernetes as it speeds up application development and delivery. So how can teams truly accelerate their software delivery? Can you avoid the steep learning curve altogether?\nGitOps lets teams continuously deploy new versions to hundreds of EKS environments simultaneously. With a clear deployment pipeline any failures are instrumented and changes can be reverted immediately.\nIn this workshop we will cover: * Manage multiple EKS clusters, environments, workloads and teams with time saving cluster models * Install Flagger to increase deployment speed for advanced deployment patterns * Handle canary and progressive releases safely with rollbacks\nNeed help in getting started with EKS? Sign up for our EKS + GitOps Quickstart package.\nGitOps - an operating model for cloud native GitOps can be summarized as these two things: * An operating model for Kubernetes and other cloud native technologies, providing a set of best practices that unify deployment, management and monitoring for containerized clusters and applications. * A path towards a developer experience for managing applications; where end-to-end CICD pipelines and git workflows are applied to both operations, and development.\nWeaveworks Weaveworks makes it fast and simple for developers and DevOps teams to build and operate powerful containerized applications. We minimize the complexity of operating workloads in Kubernetes by providing automated continuous delivery pipelines, observability and monitoring.\nWeaveworks’ mission is to minimize complexities in operating workloads and provide a developer centric operating model for cloud native applications. Our goal is to drive cloud native transformation for your developers and portability, flexibility, choice and stability for your business.\nOne of the first members of the Cloud Native Computing Foundation, Weaveworks also contributes to several open source projects, including Weave Net, Weave Scope, Weave Cortex, Weave Flux, Firekube and Flagger.\nWeaveworks and AWS Weaveworks is an APN Advanced Technology Partner and AWS Containers Competency Partner. In addition Weaveworks is the EKS certified Advanced partner that built eksctl.\nWeaveworks Pioneered GitOps Learn more about GitOps, a set of modern best practices for deploying and managing applications with cloud native tools and cloud services.\nDon’t know where to start? Our experienced team of Customer Reliability Engineers can provide the highest level of care, advice and coaching. Our expertise is rooted in working and supporting live production systems at scale. Get started with our EKS + GitOps Quickstart package.\nFor further reading we recommend:  Whitepaper: Implementing a Kubernetes Strategy in your Organization Whitepaper: Automating Kubernetes with GitOps Whitepaper: Guide to a product ready Kubernetes cluster  Checklist: Production Readiness for Kubernetes  Case Study: Fidelity Case Study: Mettle  For hands on instructions, we recommend: Tutorial: A practical guide to GitOps \n"
},
{
	"uri": "/60_workshop_6_ml.html",
	"title": "Workshop Module 6:  Machine Learning on AWS EKS",
	"tags": [],
	"description": "",
	"content": " Welcome to Machine Learining on AWS EKS and MLops Data has proven to be a competitive differentiator for decades and the adoption of machine learning can now deliver even greater insights from data. Adopting machine learning requires a technology environment with processes and platforms that can deliver innovation at scale.\nEKS is a managed Kubernetes platform for scale. GitOps introduces processes and principles that align with machine learning operations (MLOps) best practices: Reproducibility, Reusability, Manageability and Automation.\nIn this workshop we will cover:\n Manage multiple EKS clusters, environments, workloads and teams with time saving cluster models Install Flagger to increase deployment speed for advanced deployment patterns Handle canary and progressive releases safely with rollbacks  Need help in getting started with EKS? Sign up for our EKS + GitOps Quickstart package.\nGitOps - an operating model for cloud native GitOps can be summarized as these two things: * An operating model for Kubernetes and other cloud native technologies, providing a set of best practices that unify deployment, management and monitoring for containerized clusters and applications. * A path towards a developer experience for managing applications; where end-to-end CICD pipelines and git workflows are applied to both operations, and development.\nWeaveworks Weaveworks makes it fast and simple for developers and DevOps teams to build and operate powerful containerized applications. We minimize the complexity of operating workloads in Kubernetes by providing automated continuous delivery pipelines, observability and monitoring.\nWeaveworks’ mission is to minimize complexities in operating workloads and provide a developer centric operating model for cloud native applications. Our goal is to drive cloud native transformation for your developers and portability, flexibility, choice and stability for your business.\nOne of the first members of the Cloud Native Computing Foundation, Weaveworks also contributes to several open source projects, including Weave Net, Weave Scope, Weave Cortex, Weave Flux, Firekube and Flagger.\nWeaveworks and AWS Weaveworks is an APN Advanced Technology Partner and AWS Containers Competency Partner. In addition Weaveworks is the EKS certified Advanced partner that built eksctl.\nWeaveworks Pioneered GitOps Learn more about GitOps, a set of modern best practices for deploying and managing applications with cloud native tools and cloud services.\nDon’t know where to start? Our experienced team of Customer Reliability Engineers can provide the highest level of care, advice and coaching. Our expertise is rooted in working and supporting live production systems at scale. Get started with our EKS + GitOps Quickstart package.\nFor further reading we recommend:  Whitepaper: Implementing a Kubernetes Strategy in your Organization Whitepaper: Automating Kubernetes with GitOps Whitepaper: Guide to a product ready Kubernetes cluster  Checklist: Production Readiness for Kubernetes  Case Study: Fidelity Case Study: Mettle  For hands on instructions, we recommend: Tutorial: A practical guide to GitOps \n"
},
{
	"uri": "/50_workshop_5_accelerating_sdlc/00_prerequisites.md/60_add_iam_role.html",
	"title": "Add the Required IAM Role",
	"tags": [],
	"description": "",
	"content": "Creating and using EKS clusters in AWS requires specific IAM roles for the user creating and accessing EKS clusters.\n Click the A button next to the Share button in the upper right hand corner of your Cloud9 workspace Click Manage EC2 Instance Make sure your aws-cloud9-* instance is selected On Actions pull down, select Instance Settings -\u0026gt; Attach/Replace IAM Role In the IAM role pull down, select TeamRoleInstanceProfile To the right, click Apply  "
},
{
	"uri": "/60_workshop_6_ml/00_prerequisites.md/60_add_iam_role.html",
	"title": "Add the Required IAM Role",
	"tags": [],
	"description": "",
	"content": "Creating and using EKS clusters in AWS requires specific IAM roles for the user creating and accessing EKS clusters.\n Click the A button next to the Share button in the upper right hand corner of your Cloud9 workspace Click Manage EC2 Instance Make sure your aws-cloud9-* instance is selected On Actions pull down, select Instance Settings -\u0026gt; Attach/Replace IAM Role In the IAM role pull down, select modernization-admin To the right, click Apply  "
},
{
	"uri": "/40_workshop_4_hipo-teams/60_install-pod-info.html",
	"title": "Install Sample App (PodInfo)",
	"tags": [],
	"description": "",
	"content": " Install a sample app (PodInfo) This this section, we will be deploying a sample application called PodInfo. We will be adding Kubernetes manifests to your git repository. Each one of the manifests installs specific features and applications.\nYou can either edit these files directly in your git repository using the \u0026ldquo;Create File\u0026rdquo; function in the git repository web user interface, or you can clone the repository into one of your Cloud9 terminal sessions, and edit them there. If you use the git clone method, do not forget to add, commit, and push your changes to the repository.\nThe software agent, flux, will check for new commits to the git repository, and apply the changes to the manifests automatically to each of your clusters.\n"
},
{
	"uri": "/25_workshop_2_ha-dr/60_test_ingresses.html",
	"title": "Test Ingress Settings",
	"tags": [],
	"description": "",
	"content": "In each of your terminal windows, execute:\nkubectl get ingress -A You should see output similar to this:  NAMESPACE NAME HOSTS ADDRESS PORTS AGE podinfo podinfo-ingress * e737fb32-podinfo-podinfoin-2c89-1571962006.us-east-2.elb.amazonaws.com 80 4d18h  Now, using your browser, access each of you ingresses with HTTP. From the above, the URL would be:  http://e737fb32-podinfo-podinfoin-2c89-1571962006.us-east-2.elb.amazonaws.com  You should see the podinfo index status page. Refreshing the browser several times will show you that the Served by will change as Kubernetes routes between your two podinfo pods.\n"
},
{
	"uri": "/40_workshop_4_hipo-teams/60_install-pod-info/60_test_routing.html",
	"title": "Test the routing",
	"tags": [],
	"description": "",
	"content": "Now lets test the routing of requests to the backend PodInfo services. According to the routing rules we created request should be split roughly 50:50 between v1 and v2.\nWe\u0026rsquo;ll need to exec onto the frontend container:\n# Get the frontend pod\u0026#39;s name export FRONTEND_NAME=$(kubectl get pods -n apps -l app=frontend-podinfo -o jsonpath=\u0026#39;{.items[].metadata.name}\u0026#39;) # Exec into the pod kubectl -n apps exec -it ${FRONTEND_NAME} -- sh We\u0026rsquo;ll use curl to make a request to the backend-podinfo.apps.svc.cluster.local virtual service:\ncurl backend-podinfo.apps.svc.cluster.local:9898 When we run this command AWS App Mesh will take into account the virtual routes we have configured and route the request accordingly. You should see output similar to this:\n{ \u0026#34;hostname\u0026#34;: \u0026#34;backend-podinfo-v2-74f967bf8b-57xfx\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;3.3.1\u0026#34;, \u0026#34;revision\u0026#34;: \u0026#34;b45cc753291a9bfb9604a596ae709dd775f52a98\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;#34577c\u0026#34;, \u0026#34;logo\u0026#34;: \u0026#34;https://raw.githubusercontent.com/stefanprodan/podinfo/gh-pages/cuddle_clap.gif\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;greetings from podinfo v3.3.1\u0026#34;, \u0026#34;goos\u0026#34;: \u0026#34;linux\u0026#34;, \u0026#34;goarch\u0026#34;: \u0026#34;amd64\u0026#34;, \u0026#34;runtime\u0026#34;: \u0026#34;go1.14.2\u0026#34;, \u0026#34;num_goroutine\u0026#34;: \u0026#34;8\u0026#34;, \u0026#34;num_cpu\u0026#34;: \u0026#34;2\u0026#34; } Notice that you can see which version of the backend has been called by looking at the hostname in the output. In the example above it has been routed to v2.\n We will now make requests in a loop to ensure that both versions of the backend service are being called. Whilst still exec\u0026rsquo;d into the container run the following command:\nwhile true; do curl backend-podinfo.apps.svc.cluster.local:9898; echo; sleep .5; done You should see from the output that hostname indicates that both v1 and v2 are being called. And its roughly even between the 2 versions.\nLeave this command running.\n"
},
{
	"uri": "/30_workshop_03_grc/150_iam-groups/70_cleanup.html",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Once you have completed this chapter, you can cleanup the files and resources you created by issuing the following commands:\nunset KUBECONFIG # we will remove the manifest from git that we no longer want in our cluster rm -rf development rm -rf integration git add .integration/ .development/ git commit -m \u0026#34;cleaning up iamgroups demo\u0026#34; git push # if flux garbage collection is not enabled, please also run these commands to tidy things up: kubectl delete ns development integration eksctl delete iamidentitymapping --cluster eksworkshop-eksctl --arn arn:aws:iam::${ACCOUNT_ID}:role/k8sAdmin eksctl delete iamidentitymapping --cluster eksworkshop-eksctl --arn arn:aws:iam::${ACCOUNT_ID}:role/k8sDev eksctl delete iamidentitymapping --cluster eksworkshop-eksctl --arn arn:aws:iam::${ACCOUNT_ID}:role/k8sInteg aws iam remove-user-from-group --group-name k8sAdmin --user-name PaulAdmin aws iam remove-user-from-group --group-name k8sDev --user-name JeanDev aws iam remove-user-from-group --group-name k8sInteg --user-name PierreInteg aws iam delete-group-policy --group-name k8sAdmin --policy-name k8sAdmin-policy aws iam delete-group-policy --group-name k8sDev --policy-name k8sDev-policy aws iam delete-group-policy --group-name k8sInteg --policy-name k8sInteg-policy aws iam delete-group --group-name k8sAdmin aws iam delete-group --group-name k8sDev aws iam delete-group --group-name k8sInteg aws iam delete-access-key --user-name PaulAdmin --access-key-id=$(jq -r .AccessKey.AccessKeyId /tmp/PaulAdmin.json) aws iam delete-access-key --user-name JeanDev --access-key-id=$(jq -r .AccessKey.AccessKeyId /tmp/JeanDev.json) aws iam delete-access-key --user-name PierreInteg --access-key-id=$(jq -r .AccessKey.AccessKeyId /tmp/PierreInteg.json) aws iam delete-user --user-name PaulAdmin aws iam delete-user --user-name JeanDev aws iam delete-user --user-name PierreInteg aws iam delete-role --role-name k8sAdmin aws iam delete-role --role-name k8sDev aws iam delete-role --role-name k8sInteg rm /tmp/*.json rm /tmp/kubeconfig* rm /tmp/credentials"
},
{
	"uri": "/25_workshop_2_ha-dr/70_adjust_hpa.html",
	"title": "Adjust HPA Settings",
	"tags": [],
	"description": "",
	"content": "A Horizontal Pod Autoscaler would normally be triggered by the thresholds set in the manifest. However, there are times when you know that the minimum (or maximum) number of pods should be adjusted. This is done very easily by editing the podinfo_hpa.yaml file you created earlier.\nEdit the file, and change the number of replicas to something different:  minReplicas: 3 maxReplicas: 6  Don\u0026rsquo;t forget to commit and push the changes. After a few minutes, check the clusters to see your changes reflected.\nkubectl get pods -n podinfo If you can generate enough traffic, you should also be able to see the number of replicas scale up.\n"
},
{
	"uri": "/50_workshop_5_accelerating_sdlc/00_prerequisites.md/70_workspace_iam.html",
	"title": "Update IAM settings for your Workspace",
	"tags": [],
	"description": "",
	"content": " Cloud9 normally manages IAM credentials dynamically. This isn\u0026rsquo;t currently compatible with the EKS IAM authentication, so we will disable it and rely on the IAM role instead.\n  Return to your workspace and click the gear icon (in top right corner), or click to open a new tab and choose \u0026ldquo;Open Preferences\u0026rdquo; Select AWS SETTINGS Turn off AWS managed temporary credentials If you\u0026rsquo;d like to turn on AutoSave, select Experimental and select your Auto-Save Files preference Close the Preferences tab   To ensure temporary credentials aren\u0026rsquo;t already in place we will also remove any existing credentials file:\nrm -vf ${HOME}/.aws/credentials We should configure our aws cli with our current region as default.\nIf you are at an AWS event, ask your instructor which AWS region to use.\n Install jq and export your ACCOUNT_ID and AWS_REGION:\nsudo yum install jqexport ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account) export AWS_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r \u0026#39;.region\u0026#39;) Verify AWS_REGION is set to desired region\ntest -n \u0026#34;$AWS_REGION\u0026#34; \u0026amp;\u0026amp; echo AWS_REGION is \u0026#34;$AWS_REGION\u0026#34; || echo AWS_REGION is not set Let\u0026rsquo;s save these into bash_profile:\necho \u0026#34;export ACCOUNT_ID=${ACCOUNT_ID}\u0026#34; | tee -a ~/.bash_profile echo \u0026#34;export AWS_REGION=${AWS_REGION}\u0026#34; | tee -a ~/.bash_profile aws configure set default.region ${AWS_REGION} aws configure get default.region Validate the IAM role Use the GetCallerIdentity CLI command to validate that the Cloud9 IDE is using the correct IAM role.\nJust run this to try it out yourself.\naws sts get-caller-identity Here is a script that will validate you have the right role.\naws sts get-caller-identity --query Arn | grep TeamRole -q \u0026amp;\u0026amp; echo \u0026#34;IAM role valid\u0026#34; || echo \u0026#34;IAM role NOT valid\u0026#34; If the IAM role is not valid, DO NOT PROCEED. Go back and confirm the steps on this page.\n"
},
{
	"uri": "/60_workshop_6_ml/00_prerequisites.md/70_workspace_iam.html",
	"title": "Update IAM settings for your Workspace",
	"tags": [],
	"description": "",
	"content": " Cloud9 normally manages IAM credentials dynamically. This isn\u0026rsquo;t currently compatible with the EKS IAM authentication, so we will disable it and rely on the IAM role instead.\n  Return to your workspace and click the gear icon (in top right corner), or click to open a new tab and choose \u0026ldquo;Open Preferences\u0026rdquo; Select AWS SETTINGS Turn off AWS managed temporary credentials If you\u0026rsquo;d like to turn on AutoSave, select Experimental and select your Auto-Save Files preference Close the Preferences tab   To ensure temporary credentials aren\u0026rsquo;t already in place we will also remove any existing credentials file:\nrm -vf ${HOME}/.aws/credentials We should configure our aws cli with our current region as default.\nIf you are at an AWS event, ask your instructor which AWS region to use.\n Install jq and export your ACCOUNT_ID and AWS_REGION:\nsudo yum install jqexport ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account) export AWS_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r \u0026#39;.region\u0026#39;) Verify AWS_REGION is set to desired region\ntest -n \u0026#34;$AWS_REGION\u0026#34; \u0026amp;\u0026amp; echo AWS_REGION is \u0026#34;$AWS_REGION\u0026#34; || echo AWS_REGION is not set Let\u0026rsquo;s save these into bash_profile:\necho \u0026#34;export ACCOUNT_ID=${ACCOUNT_ID}\u0026#34; | tee -a ~/.bash_profile echo \u0026#34;export AWS_REGION=${AWS_REGION}\u0026#34; | tee -a ~/.bash_profile aws configure set default.region ${AWS_REGION} aws configure get default.region Validate the IAM role Use the GetCallerIdentity CLI command to validate that the Cloud9 IDE is using the correct IAM role.\nJust run this to try it out yourself.\naws sts get-caller-identity Here is a script that will validate you have the right role.\naws sts get-caller-identity --query Arn | grep modernization-admin -q \u0026amp;\u0026amp; echo \u0026#34;IAM role valid\u0026#34; || echo \u0026#34;IAM role NOT valid\u0026#34; If the IAM role is not valid, DO NOT PROCEED. Go back and confirm the steps on this page.\n"
},
{
	"uri": "/40_workshop_4_hipo-teams/60_install-pod-info/70_update_routing.html",
	"title": "Update the routing",
	"tags": [],
	"description": "",
	"content": "We are happy that v2 of the backend service is behaving as expected and want to switch 100% of the traffic to it.\nEdit the apps/3-podinfo-virtual-services.yaml file so it looks like this:\napiVersion: appmesh.k8s.aws/v1beta1 kind: VirtualService metadata: name: backend-podinfo.apps.svc.cluster.local namespace: apps spec: meshName: apps virtualRouter: name: backend-podinfo-router routes: - name: podinfo-route http: match: prefix: / action: weightedTargets: - virtualNodeName: backend-podinfo-v2 weight: 100 We have removed the target for v1 and updated the target for v2 to be 100.\n Open a new terminal window in Cloud9 (make sure you leave the other terminal window running the query).\nUse the second terminal window to add and then commit 3-podinfo-virtual-services.yaml. Push the the changes to your GitHub repo. Flux will pick this change up and apply it.\nGo back to the original terminal window. In about a minutes you will see the output change and the hostname will always show v2.\n"
},
{
	"uri": "/70_workshop_7_multicloud.html",
	"title": "Workshop Module 7: Cluster Management using Operators",
	"tags": [],
	"description": "",
	"content": " Welcome to Codifying hybrid-cloud operations Kubernetes is the foundation for modern cloud native architectures today but it is just one of the building blocks. To unlock cloud native agility, platform teams require new components and methodologies in order to successfully operationalize the entire stack. Many enterprise IT teams encounter production challenges when it comes to managing multi-cluster Kubernetes across multiple regions or in a hybrid-cloud configuration.\nThis workshop goes deep on the following topics:\n How to use the operator pattern and tools such as the shell-operator and Custom Resource Definitions How to write simple controllers to manage large deployments and complex systems How to increase reliability and stability across the platform through GitOps best practices How to simplify managing multiple clusters using GitOps  Need help in getting started with EKS? Sign up for our EKS + GitOps Quickstart package.\nGitOps - an operating model for cloud native GitOps can be summarized as these two things: * An operating model for Kubernetes and other cloud native technologies, providing a set of best practices that unify deployment, management and monitoring for containerized clusters and applications. * A path towards a developer experience for managing applications; where end-to-end CICD pipelines and git workflows are applied to both operations, and development.\nWeaveworks Weaveworks makes it fast and simple for developers and DevOps teams to build and operate powerful containerized applications. We minimize the complexity of operating workloads in Kubernetes by providing automated continuous delivery pipelines, observability and monitoring.\nWeaveworks’ mission is to minimize complexities in operating workloads and provide a developer centric operating model for cloud native applications. Our goal is to drive cloud native transformation for your developers and portability, flexibility, choice and stability for your business.\nOne of the first members of the Cloud Native Computing Foundation, Weaveworks also contributes to several open source projects, including Weave Net, Weave Scope, Weave Cortex, Weave Flux, Firekube and Flagger.\nWeaveworks and AWS Weaveworks is an APN Advanced Technology Partner and AWS Containers Competency Partner. In addition Weaveworks is the EKS certified Advanced partner that built eksctl.\nWeaveworks Pioneered GitOps Learn more about GitOps, a set of modern best practices for deploying and managing applications with cloud native tools and cloud services.\nDon’t know where to start? Our experienced team of Customer Reliability Engineers can provide the highest level of care, advice and coaching. Our expertise is rooted in working and supporting live production systems at scale. Get started with our EKS + GitOps Quickstart package.\nFor further reading we recommend:  Whitepaper: Implementing a Kubernetes Strategy in your Organization Whitepaper: Automating Kubernetes with GitOps Whitepaper: Guide to a product ready Kubernetes cluster  Checklist: Production Readiness for Kubernetes  Case Study: Fidelity Case Study: Mettle  For hands on instructions, we recommend: Tutorial: A practical guide to GitOps \n"
},
{
	"uri": "/25_workshop_2_ha-dr/80_configure_route53.html",
	"title": "Configure Route53 (Optional)",
	"tags": [],
	"description": "",
	"content": "To see what your ingress controller and cluster look like, head to the console -\u0026gt; EC2 -\u0026gt; LoadBalancers.\nYou\u0026rsquo;ll see that there is a load balancer between the nodes of your kubernetes cluster, and it is assigned a host name. That is the same host name that appears s output from this command:\nkubectl get ingress -A To finish a complete HA solution, you could add a Route 53 DNS round robin (Simple) entry to point to your two Kubernetes clusters. This would complete the simplest high availability configuration for EKS.\nThere are other ingress available for EKS that provide additional features.\n"
},
{
	"uri": "/40_workshop_4_hipo-teams/60_install-pod-info/80_revert_routing.html",
	"title": "Revert the routing",
	"tags": [],
	"description": "",
	"content": "The support department is receiving calls from customers about errors they are getting after the new release. You want to put the routing back to a 50:50 split.\nThis can be done in 2 ways:\n Revert the last commit and force push - rewrites history Fix forward - by updating the file and committing/push  If you revert the change you will lose the history of the routing changes. Many companies prefer a Fix Forward approach because of this.\nWe will revert the change and push just to demonstrate this approach. Run the following commands from your second terminal window (make sure you leave the other terminal window running the query):\n# Undo last commit and don\u0026#39;t keep the changes git reset --hard HEAD~1 # Force push the changes so Flux picks them up git push -f Go back to the original terminal window. In about a minutes you will see the output change and the hostname will show v1 and v2.\nOnce you happy boht version are being called go back to the second terminal window and enter the following command:\ngit log --oneline You will see that there is no history of the change where 100% traffic went to v2.\n"
},
{
	"uri": "/40_workshop_4_hipo-teams/60_install-pod-info/90_app_mesh_perf.html",
	"title": "Performance Monitoring",
	"tags": [],
	"description": "",
	"content": "As we\u0026rsquo;ve made some request using App Mesh lets return to Container Insights to have a look at the App Mesh performnace monitoring.\nIn the AWS Console make sure you are in CloudWatch.\nOn the left navigation pane click Performance Monitoring under Container Insights.\nIn the left most dropdown box choose Prometheus AppMesh and then in the dropdown box next to this select our cluster gitopsworkshop.\nWhen you do this you will see perforamne metrics that are specific to App Mesh. For example:\nThis will show useful metrics such as request per second, number of meshed pods, error codes.\n"
},
{
	"uri": "/40_workshop_4_hipo-teams/40_install-app-mesh/90_verify_mesh.html",
	"title": "Verify the Service Mesh is Active",
	"tags": [],
	"description": "",
	"content": "When we installed the App Mesh Injector we asked it to create a new service mesh called apps. A service mesh is a logical boundary for network traffic between the services that reside within it.\nTo check that the mesh has been created we can use the AWS CLI. Run the following command:\naws appmesh list-meshes The output should be similar to below and you should see apps listed:\n{ \u0026#34;meshes\u0026#34;: [ { \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:appmesh:us-west-2:1234567890:mesh/apps\u0026#34;, \u0026#34;meshName\u0026#34;: \u0026#34;apps\u0026#34;, \u0026#34;meshOwner\u0026#34;: \u0026#34;1234567890\u0026#34;, \u0026#34;resourceOwner\u0026#34;: \u0026#34;1234567890\u0026#34; } ] } You can also check by going to the AWS Console and choosing AWS App Mesh from the Services menu:\nAnd we need to check out out cluster by running:\nkubectl get meshes -A You should see output similar to:\nNAME AGE apps 51m"
},
{
	"uri": "/100_cleanup.html",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": " AWS Cleanup In order to prevent charges to your account we recommend cleaning up the infrastructure that was created. If you plan to keep things running so you can examine the workshop a bit more please remember to do the cleanup when you are done. It is very easy to leave things running in an AWS account, forget about it, and then accrue charges.\n In the AWS console, go to CloudFormation, click ModernizationWorkshop-EKS stack and then Delete. You do not need to choose the stacks for the workshop with the Nested tag, they will automatically be deleted as the ModernizationWorkshop-EKS stack is.\nThis process will take 15-30 minutes, to be sure to verify that none of the ModernizationWorkshop-EKS stacks are listed in CloudFormation and you are done.\n Weaveworks Cleanup You have a 15 day trial, so keep using it to monitor and manage your infrastructure and applications.\nHere are some additional resources to checkout:\n Learn more about your tenant and install more OneAgents Add other users to your tenant YouTube Videos More Support resources   Cleanup   "
},
{
	"uri": "/50_workshop_5_accelerating_sdlc/10_create_cluster/01_create_eks_cluster.html",
	"title": "Create EKS Cluster",
	"tags": [],
	"description": "",
	"content": "Before creating the EKS cluster, please make sure that you have the right role.\naws sts get-caller-identity --query Arn | grep TeamRole -q \u0026amp;\u0026amp; echo \u0026#34;IAM role valid\u0026#34; || echo \u0026#34;IAM role NOT valid\u0026#34; If the IAM role is not valid, DO NOT PROCEED. Go back and confirm the steps on this page.\nOnce you have ensured your IAM role is valid, proceed to define your cluster configuration.\nCreate a cluster.yaml file in the root of your Cloud9 environment. Copy and paste the yaml below:\napiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: gitopssdlcworkshop region: us-west-2 nodeGroups: - name: default instanceType: m5.large desiredCapacity: 2 volumeSize: 120 iam: withAddonPolicies: appMesh: true xRay: true To create the EKS cluster, you will need to run the following command:\n# make sure we are in the root environment folder cd ~/environment # Create the cluster eksctl create cluster -f cluster.yaml The creation of the cluster typically takes about 20 minutes. You can monitor the progress of your cluster by navigating to AWS Console \u0026gt; Services \u0026gt; CloudFormation.\nSince we have some time, let\u0026rsquo;s review some Advanced Deployment Strategies.\n"
},
{
	"uri": "/60_workshop_6_ml/10_create_cluster/01_create_eks_cluster.html",
	"title": "Create EKS Cluster",
	"tags": [],
	"description": "",
	"content": "If an eks cluster has already been created for you:\nexport AWS_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r \u0026#39;.region\u0026#39;) export AWS_DEFAULT_REGION=$AWS_REGION export EKS_CLUSTER_NAME=$(eksctl get cluster -o json | jq -r \u0026#39;.[0][\u0026#34;name\u0026#34;]\u0026#39;) eksctl utils write-kubeconfig --name $EKS_CLUSTER_NAME Before creating the EKS cluster, please make sure that you have the right role.\naws sts get-caller-identity --query Arn | grep TeamRole -q \u0026amp;\u0026amp; echo \u0026#34;IAM role valid\u0026#34; || echo \u0026#34;IAM role NOT valid\u0026#34; If the IAM role is not valid, DO NOT PROCEED. Go back and confirm the steps on this page.\nOnce you have ensured your IAM role is valid, proceed to define your cluster configuration.\nTo create the EKS cluster, you will need to run the following commands:\nexport AWS_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r \u0026#39;.region\u0026#39;) export AWS_DEFAULT_REGION=$AWS_REGION export EKS_CLUSTER_NAME=mlops-c9 # Create KMS key for encrypted secrets support key_arn=$(aws --region $AWS_DEFAULT_REGION kms create-key | jq -r \u0026#39;.\u0026#34;KeyMetadata\u0026#34;[\u0026#34;Arn\u0026#34;]\u0026#39;) mkdir -p ${EKS_CLUSTER_NAME} pushd ${EKS_CLUSTER_NAME} # Generate ssh key pair for access to EC2 nodes ssh-keygen -b 4096 -f id_rsa -q -t rsa -N \u0026#34;\u0026#34; 2\u0026gt;/dev/null \u0026lt;\u0026lt;\u0026lt; y \u0026gt;/dev/null curl https://raw.githubusercontent.com/paulcarlton-ww/ml-workshop/master/resources/eks-template.yaml | \\  sed s/NAME/$EKS_CLUSTER_NAME/ | sed s/REGION/$AWS_DEFAULT_REGION/ | sed s#KEY#$key_arn# \u0026gt; eks.yaml eksctl create cluster --config-file eks.yaml popd The creation of the cluster typically takes about 20 minutes. You can monitor the progress of your cluster by navigating to AWS Console \u0026gt; Services \u0026gt; CloudFormation.\n"
},
{
	"uri": "/30_workshop_03_grc/120_deploy_opa_gatekeeper.html",
	"title": "Deploy OPA gatekeeper",
	"tags": [],
	"description": "",
	"content": " Manage Policies and Compliance using OPA Gatekeeper This workshop will run through the following:\n Intro to OPA\n what is OPA? How does Kubernetes controls what is created. How can we extend this with OPA. Overview of OPA gatekeeper  Deploying OPA Gatekeeper\n Download and install OPA Gatekeeper A run thorugh of its componnents and whats installed   "
},
{
	"uri": "/30_workshop_03_grc/125_create_policy_templates.html",
	"title": "Create Policy Constraint Templates",
	"tags": [],
	"description": "",
	"content": " Manage Policies and Compliance using OPA Gatekeeper This workshop will run through how to create Constraint Templates to use with OPA Gatekeeper. We will run how to write a template, what component of a template does and also run through some examplerego policies.\nAll policies are written in rego, for detailed information on how this works please visit here.\nRego is a declaritive query language that can be used to support document models such as JSON. Making it a great fit for Kubernetes!\n"
},
{
	"uri": "/30_workshop_03_grc/130_create_policy_contraints.html",
	"title": "Create Policy Constraints",
	"tags": [],
	"description": "",
	"content": " Create Policy Constraints We will now run through how to use the CRDs we defined in the ContraintTemplates to create policies in our cluster.\n"
},
{
	"uri": "/30_workshop_03_grc/140_test-policy-contraints.html",
	"title": "Test Policy Constraints",
	"tags": [],
	"description": "",
	"content": "In this workshop we are going to create some resources and attempt to deploy them in to our cluster.\nThese will be intentially violating policies we defined in our previous workshop.\n "
},
{
	"uri": "/30_workshop_03_grc/150_iam-groups.html",
	"title": "Using IAM Groups to manage Kubernetes access",
	"tags": [],
	"description": "",
	"content": "In this chapter, we\u0026rsquo;ll learn about how to simplify access to different parts of the kubernetes clusters depending on IAM Groups\n"
},
{
	"uri": "/50_workshop_5_accelerating_sdlc/20_enable_gitops/01_create_git_repo.html",
	"title": "Create Git Repository",
	"tags": [],
	"description": "",
	"content": "We will be using a GitHub repo to declare the desired state of our EKS cluster. This repo will eventually include your workload manifests.\nYou can start with an empty repository and push that to GitHub, or use the one you intend to deploy to the cluster. As a suggestion create a blank public repo on GitHub called sdlc-workshop and make sure you tick Initialize this Repository with a README:\nFor instructions on how to create a repository on GitHub follow these steps..\nIn the Cloud9 environment, a user SSH key pair is not automatically created. Use these instructions to create an SSH key pair for use as your git repository deploy keys.\n You will also want to add your ssh. Instructions to do so are here.\nTo create a new ssh key, run the following. Substitute your github email address where specified.\nssh-keygen -t rsa -b 4096 -C \u0026#34;your_email@example.com\u0026#34; Start the ssh-agent in the background.\neval \u0026#34;$(ssh-agent -s)\u0026#34; Add your SSH private key to the ssh-agent and store your passphrase in the keychain. If you created your key with a different name, or if you are adding an existing key that has a different name, replace id_rsa in the command with the name of your private key file.\nssh-add ~/.ssh/id_rsa Then run:\ncat ~/.ssh/id_rsa.pub You can then create a deploy key in your repository by going to: Settings \u0026gt; Deploy keys. Click on Add deploy key, and check Allow write access. Then paste your public key and click Add key. This will allow us to clone this repo to our Cloud9 environment.\n"
},
{
	"uri": "/60_workshop_6_ml/20_enable_gitops/01_create_git_repo.html",
	"title": "Create Git Repository",
	"tags": [],
	"description": "",
	"content": "Using your github account, create a repository called \u0026lsquo;\u0026lt;eks cluster name\u0026gt;-config\u0026rsquo; in your personal git organisation.\nFor instructions on how to create a repository on GitHub follow these steps..\nIn the Cloud9 environment, a user SSH key pair is not automatically created. Use these instructions to create an SSH key pair for use as your git repository deploy keys.\n You will also want to add your ssh. Instructions to do so are here.\nTo create a new ssh key, run the following. Substitute your github email address where specified.\nssh-keygen -t rsa -b 4096 -C \u0026#34;your_email@example.com\u0026#34; Start the ssh-agent in the background.\neval \u0026#34;$(ssh-agent -s)\u0026#34; Add your SSH private key to the ssh-agent and store your passphrase in the keychain. If you created your key with a different name, or if you are adding an existing key that has a different name, replace id_rsa in the command with the name of your private key file.\nssh-add ~/.ssh/id_rsa Then run:\ncat ~/.ssh/id_rsa.pub You can then create a deploy key in your repository by going to: Settings \u0026gt; Deploy keys. Click on Add deploy key, and check Allow write access. Then paste your public key and click Add key. This will allow us to clone this repo to our Cloud9 environment.\n"
},
{
	"uri": "/50_workshop_5_accelerating_sdlc/20_enable_gitops/02_eksctl_enable_repo.html",
	"title": "Eksctl Enable Repo",
	"tags": [],
	"description": "",
	"content": "Ensure you are in the root of your Cloud9 Environment and clone your repository.\ncd ~/environment export GH_USER=YOUR_GITHUB_USERNAMEexport GH_REPO=sdlc-workshopgit clone git@github.com:${GH_USER}/${GH_REPO} Navigate to your newly cloned repository:\ncd ${GH_REPO} Next, we will be running eksctl enable repo. This takes an existing EKS cluster and an empty repository and sets up a GitOps pipeline.\nexport EKSCTL_EXPERIMENTAL=true eksctl enable repo \\ --cluster=gitopssdlcworkshop \\ --region=us-west-2 \\ --git-user=\u0026#34;fluxcd\u0026#34; \\ --git-email=\u0026#34;${GH_USER}@users.noreply.github.com\u0026#34; \\ --git-url=\u0026#34;git@github.com:${GH_USER}/${GH_REPO}\u0026#34; eksctl enable repo will install FluxCD and the Helm Operator (with support for Helm v3).\nWhen the command finishes, you will be prompted to add Flux\u0026rsquo;s SSH public key to your GitHub repository.\nCopy the public key and create a deploy key with write access to your GitHub repository.\nYou can add a deploy key to your repository by going to: Settings \u0026gt; Deploy keys. Click on Add deploy key, and check Allow write access. Then paste the Flux public key and click Add key.\nOnce you have added the deploy key, Flux will pick up the changes in the repository and deploy them to the cluster.\n"
},
{
	"uri": "/60_workshop_6_ml/20_enable_gitops/02_eksctl_enable_repo.html",
	"title": "Eksctl Enable Repo",
	"tags": [],
	"description": "",
	"content": "Ensure you are in the root of your Cloud9 Environment and clone your repository.\nGITHUB_DIR=$PWD/src/github.com GIT_EMAIL=$(git config -f ~/.gitconfig --get user.email) GIT_ORG=$(git config -f ~/.gitconfig --get user.name) EKSCTL_EXPERIMENTAL=true \\  eksctl enable repo \\  --git-url git@github.com:$GIT_ORG/${EKS_CLUSTER_NAME}-config \\  --git-email $GIT_EMAIL \\  --cluster $EKS_CLUSTER_NAME \\  --region \u0026#34;$AWS_DEFAULT_REGION\u0026#34; ` \u0026lsquo;eksctl enable repo\u0026rsquo; will install FluxCD and the Helm Operator (with support for Helm v3).\nWhen the command finishes, you will be prompted to add Flux\u0026rsquo;s SSH public key to your GitHub repository.\nCopy the public key and create a deploy key with write access to your GitHub repository.\nYou can add a deploy key to your repository by going to: Settings \u0026gt; Deploy keys. Click on Add deploy key, and check Allow write access. Then paste the Flux public key and click Add key.\nOnce you have added the deploy key, Flux will pick up the changes in the repository and deploy them to the cluster.\n"
},
{
	"uri": "/60_workshop_6_ml/30_deploy_appdev_profile/01_app_dev.html",
	"title": "Deploy app-dev profile",
	"tags": [],
	"description": "",
	"content": "EKSCTL_EXPERIMENTAL=true eksctl enable profile app-dev \\  --git-url git@github.com:$GIT_ORG/${EKS_CLUSTER_NAME}-config \\  --git-email $GIT_EMAIL \\  --cluster $EKS_CLUSTER_NAME \\  --region \u0026#34;$AWS_DEFAULT_REGION\u0026#34; mkdir -p $GITHUB_DIR/$GIT_ORG cd $GITHUB_DIR/$GIT_ORG git clone git@github.com:$GIT_ORG/${EKS_CLUSTER_NAME}-config.git cd ${EKS_CLUSTER_NAME}-config Wait a minute or two for flux to deploy applications, verify with \u0026lsquo;kubectl get pods -A\u0026rsquo;\n"
},
{
	"uri": "/50_workshop_5_accelerating_sdlc/30_install_appmesh/01_install_app_mesh.html",
	"title": "Enable App Mesh eksctl Profile",
	"tags": [],
	"description": "",
	"content": "In the same terminal window that you exported your GH_USER and GH_REPO to, we will be enabling the appmesh eksctl profile.\nRun the eksctl profile command:\neksctl enable profile appmesh \\ --profile-revision=demo \\ --cluster=gitopssdlcworkshop \\ --region=us-west-2 \\ --git-user=\u0026#34;fluxcd\u0026#34; \\ --git-email=\u0026#34;${GH_USER}@users.noreply.github.com\u0026#34; \\ --git-url=\u0026#34;git@github.com:${GH_USER}/${GH_REPO}\u0026#34; Run the fluxctl sync command to install the App Mesh control plane on your cluster:\nfluxctl sync --k8s-fwd-ns flux Inspect the components that were installed:\nkubectl get helmreleases --all-namespaces You should see the following:\nNAMESPACE NAME RELEASE PHASE STATUS MESSAGE AGE appmesh-system appmesh-controller appmesh-controller Succeeded deployed Release was successful for Helm release \u0026#39;appmesh-controller\u0026#39; in \u0026#39;appmesh-system\u0026#39;. 19h appmesh-system appmesh-grafana appmesh-grafana Succeeded deployed Release was successful for Helm release \u0026#39;appmesh-grafana\u0026#39; in \u0026#39;appmesh-system\u0026#39;. 19h appmesh-system appmesh-inject appmesh-inject Succeeded deployed Release was successful for Helm release \u0026#39;appmesh-inject\u0026#39; in \u0026#39;appmesh-system\u0026#39;. 19h appmesh-system appmesh-prometheus appmesh-prometheus Succeeded deployed Release was successful for Helm release \u0026#39;appmesh-prometheus\u0026#39; in \u0026#39;appmesh-system\u0026#39;. 19h appmesh-system flagger flagger Succeeded deployed Release was successful for Helm release \u0026#39;flagger\u0026#39; in \u0026#39;appmesh-system\u0026#39;. 19h demo appmesh-gateway appmesh-gateway Succeeded deployed Release was successful for Helm release \u0026#39;appmesh-gateway\u0026#39; in \u0026#39;demo\u0026#39;. 19h kube-system metrics-server metrics-server Succeeded deployed Release was successful for Helm release \u0026#39;metrics-server\u0026#39; in \u0026#39;kube-system\u0026#39;. 19h Verify that the mesh was created successfully:\nkubectl describe mesh The mesh was created and is active if you see the following:\nName: appmesh API Version: appmesh.k8s.aws/v1beta1 Kind: Mesh Spec: Service Discovery Type: dns Status: Mesh Condition: Status: True Type: MeshActive"
},
{
	"uri": "/50_workshop_5_accelerating_sdlc/30_install_appmesh/02_sync_repo.html",
	"title": "Sync your repository and Kustomize the Profile",
	"tags": [],
	"description": "",
	"content": " eksctl enable profile pushed changes to our new repo. Let\u0026rsquo;s fetch them:\ngit pull origin master Kustomize the profile Kustomize lets you customize raw, template-free YAML files for multiple purposes, leaving the original YAML untouched and usable as is.\nWe will be using kustomize to create more targeted patches that make our code easier to factor, understand, and reuse.\nCreate kustomization files for base and flux manifests:\nfor dir in ./flux ./base; do ( pushd \u0026#34;$dir\u0026#34; \u0026amp;\u0026amp; kustomize create --autodetect --recursive ) done Create a kustomization file in the repo root:\ncat \u0026lt;\u0026lt; EOF | tee kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization bases: - base - flux EOF Create .flux.yaml file in the repo root:\ncat \u0026lt;\u0026lt; EOF | tee .flux.yaml version: 1 commandUpdated: generators: - command: kustomize build . EOF Verify the kustomization is valid by running a dry run apply:\nkubectl apply --dry-run -k . \u0026amp;\u0026amp; echo \u0026amp;\u0026amp; echo \u0026#34;config is ok :)\u0026#34; Apply your changes via git and call fluxctl sync to immediately apply the changes:\ngit add -A \u0026amp;\u0026amp; \\ git commit -m \u0026#34;init kustomization\u0026#34; \u0026amp;\u0026amp; \\ git push origin master \u0026amp;\u0026amp; \\ fluxctl sync --k8s-fwd-ns flux Flux is now configured to patch our manifests before applying them to the cluster.\n"
},
{
	"uri": "/60_workshop_6_ml/40_kubeflow/01_install_kubeflow.html",
	"title": "Deploy Kubeflow",
	"tags": [],
	"description": "",
	"content": "Install the kubeflow:\ncd $GITHUB_DIR/$GIT_ORG wget https://github.com/kubeflow/kfctl/releases/download/v1.0.2/kfctl_v1.0.2-0-ga476281_linux.tar.gz tar -zxvf kfctl_v1.0.2-0-ga476281_linux.tar.gz sudo mv kfctl /usr/bin rm kfctl_v1.0.2-0-ga476281_linux.tar.gz export AWS_CLUSTER_NAME=$EKS_CLUSTER_NAME export KF_NAME=${AWS_CLUSTER_NAME} export BASE_DIR=$GITHUB_DIR/$GIT_ORG/kubeflow-config export KF_DIR=${BASE_DIR}/${KF_NAME} mkdir -p ${KF_DIR} cd ${KF_DIR} export CONFIG_URI=https://raw.githubusercontent.com/kubeflow/manifests/v1.0-branch/kfdef/kfctl_aws.v1.0.2.yaml wget -O kfctl_aws.yaml $CONFIG_URI grep -v eksctl kfctl_aws.yaml \u0026gt; kfctl.yaml sed -i s/kubeflow-aws/${AWS_CLUSTER_NAME}/ kfctl.yaml sed -i s/roles:/enablePodIamPolicy\\:\\ true/ kfctl.yaml sed -i s/us-west-2/${AWS_DEFAULT_REGION}/ kfctl.yaml sed -i s/roles:/enablePodIamPolicy\\:\\ true/ kfctl.yaml export CONFIG_FILE=${KF_DIR}/kfctl.yaml kfctl apply -V -f ${CONFIG_FILE} In a few minutes or so you will see deployments in \u0026lsquo;kubeflow\u0026rsquo; namespace, verify using \u0026lsquo;kubectl get all -n kubeflow\u0026rsquo;\n"
},
{
	"uri": "/50_workshop_5_accelerating_sdlc/40_progressive_delivery/01_install_podinfo.html",
	"title": "Install Podinfo demo app",
	"tags": [],
	"description": "",
	"content": "In the /base/demo folder, you will find several components that will help us deploy our sample podinfo application.\nInstall the demo app by setting fluxcd.io/ignore to false in base/demo/namespace.yaml:\ncat \u0026lt;\u0026lt; EOF | tee base/demo/namespace.yaml apiVersion: v1 kind: Namespace metadata: name: demo annotations: fluxcd.io/ignore: \u0026#34;false\u0026#34; labels: appmesh.k8s.aws/sidecarInjectorWebhook: enabled EOF Push the changes to your git repository and apply using fluxctl:\ngit add -A \u0026amp;\u0026amp; \\ git commit -m \u0026#34;init demo\u0026#34; \u0026amp;\u0026amp; \\ git push origin master \u0026amp;\u0026amp; \\ fluxctl sync --k8s-fwd-ns flux Wait for Flagger to initialize the canary:\nkubectl get canary -n demo Find the ingress public address with:\nexport URL=\u0026#34;http://$(kubectl -n demo get svc/appmesh-gateway -ojson | jq -r \u0026#34;.status.loadBalancer.ingress[].hostname\u0026#34;)\u0026#34; echo $URL In a few minutes or so, podinfo will be accessible via the public URL.\nWhile we wait for podinfo to be accessible, let\u0026rsquo;s review some of the components that Flagger creates / uses in order to canary your deployments on the next page.\n"
},
{
	"uri": "/50_workshop_5_accelerating_sdlc/40_progressive_delivery/02_review_flagger.html",
	"title": "Review Flagger",
	"tags": [],
	"description": "",
	"content": "In base/demo/podinfo/canary.yaml, you\u0026rsquo;ll see the specifications for Flagger to automatically increase traffic to new versions of the application. In this workshop, we will be demonstrating a successful canary release, as well mimicking a failed canary release.\nBelow is the Canary configuration:\napiVersion: flagger.app/v1beta1 kind: Canary metadata: name: podinfo namespace: demo spec: provider: appmesh # the maximum time in seconds for the canary deployment # to make progress before it is rollback (default 600s) progressDeadlineSeconds: 60 # deployment reference targetRef: apiVersion: apps/v1 kind: Deployment name: podinfo # HPA reference (optional) autoscalerRef: apiVersion: autoscaling/v2beta1 kind: HorizontalPodAutoscaler name: podinfo service: # container port port: 9898 # container port name (optional) # can be http or grpc portName: http # App Mesh Gateway external domain hosts: - \u0026#34;*\u0026#34; # App Mesh reference meshName: \u0026#34;gitopssdlcworkshop\u0026#34; # App Mesh retry policy (optional) retries: attempts: 3 perTryTimeout: 1s retryOn: \u0026#34;gateway-error,client-error,stream-error\u0026#34; # define the canary analysis timing and KPIs analysis: # schedule interval (default 60s) interval: 10s # max number of failed metric checks before rollback threshold: 10 # max traffic percentage routed to canary # percentage (0-100) maxWeight: 50 # canary increment step # percentage (0-100) stepWeight: 5 # App Mesh Prometheus checks metrics: - name: request-success-rate # minimum req success rate (non 5xx responses) # percentage (0-100) thresholdRange: min: 99 interval: 1m - name: request-duration # maximum req duration P99 # milliseconds thresholdRange: max: 500 interval: 30s # testing (optional) webhooks: - name: acceptance-test type: pre-rollout url: http://flagger-loadtester.demo/ timeout: 30s metadata: type: bash cmd: \u0026#34;curl -sd \u0026#39;test\u0026#39; http://podinfo-canary.demo:9898/token | grep token\u0026#34; - name: load-test url: http://flagger-loadtester.demo/ timeout: 5s metadata: cmd: \u0026#34;hey -z 1m -q 10 -c 2 http://podinfo-canary.demo:9898/\u0026#34; Flagger will create the following Kubernetes resources on our behalf, as we have specified the TargetRef and AutoscalerRef:\n# generated Kubernetes objects deployment.apps/podinfo-primary horizontalpodautoscaler.autoscaling/podinfo-primary service/podinfo service/podinfo-canary service/podinfo-primary If you run kubectl get pods -n demo, you will notice that you have podinfo-primary pods running. The primary deployment is treated as the stable release of your app. All traffic is routed to this version and the target deployment is scaled to zero (kubectl get deployment podinfo -n demo). Flagger will detect changes to the target deployment (including secrets and configmaps) and will perform a canary analysis before promoting the new version as primary.\nFlagger will also create the following App Mesh resources on our behalf:\n# generated App Mesh objects (Kubernetes Custom Resources) virtualnode.appmesh.k8s.aws/podinfo virtualnode.appmesh.k8s.aws/podinfo-canary virtualnode.appmesh.k8s.aws/podinfo-primary virtualservice.appmesh.k8s.aws/podinfo.test virtualservice.appmesh.k8s.aws/podinfo-canary.test virtualnode.appmesh.k8s.aws defines the logical pointer to a Kubernetes workload, and virtualservice.appmesh.k8s.aws defines the routing rules for a workload inside the mesh. After the bootstrap, the podinfo deployment will be scaled to zero and the traffic to podinfo.demo will be routed to the primary pods. During the canary analysis, the podinfo-canary.demo address can be used to target directly the canary pods.\nYou can inspect these resources for yourself by running:\nkubectl get virtualservice -n demo\nand\nkubectl get virtualnodes -n demo\nYou can also view the Virtual Service, Virtual Route, and Virtual Nodes in the AWS Console under the App Mesh service.\nTo perform a canary analysis, Flagger uses the analysis definition to determine how long to run the canary for (running periodically until it reaches the maximum traffic weight or the number of iteration) before rolling out the new version as the primary. On each run, Flagger calls the webhooks, checks the metrics and if the failed checks threshold is reached, it will stop the analysis and roll back the canary. We will demonstrate this shortly.\nNavigate to the following url (or just echo $URL if you are in the same terminal window you were in before):\nexport URL=\u0026#34;http://$(kubectl -n demo get svc/appmesh-gateway -ojson | jq -r \u0026#34;.status.loadBalancer.ingress[].hostname\u0026#34;)\u0026#34; echo $URL You can leave this up as we complete this workshop.\n"
},
{
	"uri": "/50_workshop_5_accelerating_sdlc/40_progressive_delivery/03_canary_release.html",
	"title": "Automated Canary Promotion",
	"tags": [],
	"description": "",
	"content": "When you deploy a new podinfo version, Flagger gradually shifts traffic to the canary. At the same time, Flagger measures the requests success rate as well as the average response duration. Based on an analysis of these App Mesh provided metrics, a canary deployment is either promoted or rolled back.\nRun the following command to create a Kustomize patch for the podinfo deployment in overlays/podinfo.yaml:\nmkdir -p overlays \u0026amp;\u0026amp; \\ cat \u0026lt;\u0026lt; EOF | tee overlays/podinfo.yaml apiVersion: apps/v1 kind: Deployment metadata: name: podinfo namespace: demo spec: template: spec: containers: - name: podinfod image: stefanprodan/podinfo:3.1.1 env: - name: PODINFO_UI_LOGO value: https://eks.handson.flagger.dev/cuddle_bunny.gif EOF Add the patch to the kustomization file as such:\ncat \u0026lt;\u0026lt; EOF | tee kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization bases: - base - flux patchesStrategicMerge: - overlays/podinfo.yaml EOF Push the changes to your repository and apply using fluxctl:\ngit add -A \u0026amp;\u0026amp; \\ git commit -m \u0026#34;patch podinfo\u0026#34; \u0026amp;\u0026amp; \\ git push origin master \u0026amp;\u0026amp; \\ fluxctl sync --k8s-fwd-ns flux When Flagger detects that the deployment revision changed it will start a new rollout. You can monitor the traffic shifting with:\nkubectl -n demo get canaries --watch Watch Flagger logs:\nkubectl -n appmesh-system logs deployment/flagger -f | jq .msg As we watch the weight of traffic increase for upgraded podinfo, we can expect to see our site change accordingly. As Flagger shifts more traffic to the canary according to the policy in the Canary object, we see requests going to the new version of the app. When the canary analysis successfully completes, the new version 3.1.1 will be promoted as the primary.\n"
},
{
	"uri": "/50_workshop_5_accelerating_sdlc/40_progressive_delivery/04_canary_rollback.html",
	"title": "Automated Canary Rollback",
	"tags": [],
	"description": "",
	"content": "Now we will generate some failed requests to trigger an automated rollback.\nDuring the canary analysis, you can generate HTTP 500 errors and high latency to verify that Flagger pauses and rolls back the faulty version.\nTrigger another canary release:\ncat \u0026lt;\u0026lt; EOF | tee overlays/podinfo.yaml apiVersion: apps/v1 kind: Deployment metadata: name: podinfo namespace: demo spec: template: spec: containers: - name: podinfod image: stefanprodan/podinfo:3.1.2 env: - name: PODINFO_UI_LOGO value: https://eks.handson.flagger.dev/cuddle_bunny.gif EOF Push your changes and use fluxctl to sync:\ngit add -A \u0026amp;\u0026amp; \\ git commit -m \u0026#34;update podinfo\u0026#34; \u0026amp;\u0026amp; \\ git push origin master \u0026amp;\u0026amp; \\ fluxctl sync --k8s-fwd-ns flux Watch the canaries:\nkubectl -n demo get canaries --watch View Flagger logs with:\nkubectl -n appmesh-system logs deployment/flagger -f | jq .msg Exec into the tester pod:\nkubectl -n demo exec -it $(kubectl -n demo get pods -o name | grep -m1 flagger-loadtester | cut -d\u0026#39;/\u0026#39; -f 2) bash Generate HTTP 500 errors:\nhey -z 1m -c 5 -q 5 http://podinfo-canary.demo:9898/status/500 \u0026amp;\u0026amp; \\ hey -z 1m -c 5 -q 5 http://podinfo-canary.demo:9898/delay/1 When the number of failed checks reaches the canary analysis threshold, the traffic is routed back to the primary and the canary is scaled to zero.\nView Flagger logs with:\nkubectl -n appmesh-system logs deployment/flagger -f | jq .msg You should see the following:\nStarting canary analysis for podinfo.prod Advance podinfo.test canary weight 5 Advance podinfo.test canary weight 10 Advance podinfo.test canary weight 15 Halt podinfo.test advancement success rate 69.17% \u0026lt; 99% Halt podinfo.test advancement success rate 61.39% \u0026lt; 99% Halt podinfo.test advancement success rate 55.06% \u0026lt; 99% Halt podinfo.test advancement request duration 1.20s \u0026gt; 0.5s Halt podinfo.test advancement request duration 1.45s \u0026gt; 0.5s Rolling back podinfo.prod failed checks threshold reached 5 Canary failed! Scaling down podinfo.test You\u0026rsquo;ll see that your podinfo-primary pods are still up, but they are all versioned at 3.1.1.\n"
},
{
	"uri": "/60_workshop_6_ml/50_titanic_sample_application/01_aws_k8s-setup.html",
	"title": "AWS and Kubenetes Environment Setup",
	"tags": [],
	"description": "",
	"content": " AWS and Kubenetes Environment Setup cd $HOME/environment # Set default region export AWS_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r \u0026#39;.region\u0026#39;) # For ec2 client or cloud9 export AWS_DEFAULT_REGION=$AWS_REGION # I encountered issues with EMR in eu-west-2 but it works fine in eu-west-1. # Use this variable to set the region to run the EMR cluster in if you encounter issues in your local/default region export EMR_REGION=$AWS_REGION EKS_CLUSTER_LOWER=$(echo $EKS_CLUSTER_NAME | awk \u0026#39;{print tolower($0)}\u0026#39;) export BUCKET_NAME=${EKS_CLUSTER_LOWER}-tdata aws iam create-user --user-name mlops-user aws iam create-access-key --user-name mlops-user \u0026gt; $HOME/mlops-user.json export THE_ACCESS_KEY_ID=$(jq -r \u0026#39;.\u0026#34;AccessKey\u0026#34;[\u0026#34;AccessKeyId\u0026#34;]\u0026#39; $HOME/mlops-user.json) echo $THE_ACCESS_KEY_ID export THE_SECRET_ACCESS_KEY=$(jq -r \u0026#39;.\u0026#34;AccessKey\u0026#34;[\u0026#34;SecretAccessKey\u0026#34;]\u0026#39; $HOME/mlops-user.json) export ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account) aws iam create-policy --policy-name mlops-s3-access \\  --policy-document https://raw.githubusercontent.com/paulcarlton-ww/mlops-titanic/master/resources/s3-policy.json \u0026gt; s3-policy.json aws iam create-policy --policy-name mlops-emr-access \\  --policy-document https://raw.githubusercontent.com/paulcarlton-ww/mlops-titanic/master/resources/emr-policy.json \u0026gt; emr-policy.json aws iam create-policy --policy-name mlops-iam-access \\  --policy-document https://raw.githubusercontent.com/paulcarlton-ww/mlops-titanic/master/resources/iam-policy.json \u0026gt; iam-policy.json aws iam attach-user-policy --user-name mlops-user --policy-arn $(jq -r \u0026#39;.\u0026#34;Policy\u0026#34;[\u0026#34;Arn\u0026#34;]\u0026#39; s3-policy.json) aws iam attach-user-policy --user-name mlops-user --policy-arn $(jq -r \u0026#39;.\u0026#34;Policy\u0026#34;[\u0026#34;Arn\u0026#34;]\u0026#39; emr-policy.json) aws emr create-default-roles curl https://raw.githubusercontent.com/paulcarlton-ww/mlops-titanic/master/resources/kubeflow-aws-secret.yaml | \\  sed s/YOUR_BASE64_SECRET_ACCESS/$(echo -n \u0026#34;$THE_SECRET_ACCESS_KEY\u0026#34; | base64)/ | \\  sed s/YOUR_BASE64_ACCESS_KEY/$(echo -n \u0026#34;$THE_ACCESS_KEY_ID\u0026#34; | base64)/ | kubectl apply -f -;echo aws s3api create-bucket --bucket $BUCKET_NAME --region $AWS_DEFAULT_REGION --create-bucket-configuration LocationConstraint=$AWS_DEFAULT_REGION"
},
{
	"uri": "/60_workshop_6_ml/05_setup/01_git-config-ssh-key.html",
	"title": "Cloud9 setup",
	"tags": [],
	"description": "",
	"content": " Git setup You need to create and clone github repositories during this workshop so it is recommended that you setup your git config and ssh keys.\nvim ~/.gitconfig # Add your config Extend FileSystem size The default disk size for a cloud9 instance is 10gb but we need more than this for the tools needed to build ML projects. So we need to increase the size of the EBS volume and grow the Linux filesytem. To do this use the aws console to locate the EC2 instance by clicking on EBS volume used by your cloud9 instance and increase the size of the EBS volume from 10gb to 100gb\n Click the A button next to the Share button in the upper right hand corner of your Cloud9 workspace Click Manage EC2 Instance Make sure your aws-cloud9-* instance is selected  Locate the root filesystem\nClick on it\nClick on EBS ID value to open ebs volume in console\nOn Actions pull down, select Modify Volume and change the size from 10gb to 100gb\nThen in cloud9 terminal:\n# Grow filesystem sudo growpart /dev/xvda 1 sudo resize2fs /dev/xvda1 df -h Filesystem Size Used Avail Use% Mounted on devtmpfs 483M 60K 483M 1% /dev tmpfs 493M 0 493M 0% /dev/shm /dev/xvda1 99G 7.6G 91G 8% /"
},
{
	"uri": "/50_workshop_5_accelerating_sdlc/15_deployment_strategies/01_recreate.html",
	"title": "Recreate",
	"tags": [],
	"description": "",
	"content": " When upgrading pods using the recreate strategy, existing pods are terminated before new ones are created.\nTo use the recreate strategy, define the Deployment yaml as such:\napiVersion: apps/v1 kind: Deployment spec: replicas: 2 strategy: type: Recreate The Deployment as defined above will create two replica pods:\ngraph LR; A{Deployment} -- B(Replica 1, V1) A -- C(Replica 2, V1)  The recreate strategy will terminate both replicas. During this time, the service is down and will not be able to handle traffic:\ngraph LR; A{Deployment} -- |Terminating| B(Replica 1, V1) A -- |Terminating| C(Replica 2, V1)  The upgraded replicas will then move into the Pending and ContainerCreating. During this time, the service is unable to handle traffic:\ngraph LR; A{Deployment} --|Pending, ContainerCreating| B(Replica 1, V2) A -- |Pending, ContainerCreating| C(Replica 2, V2)  Once the upgraded replicas have come up successfully, the pods are ready to handle traffic.\ngraph LR; A{Deployment} -- B(Replica 1, V2) A -- C(Replica 2, V2)  Benefits of the Recreate Strategy  Avoids versioning issues Avoids database schema incompatibilities  Caveats for the Recreate Strategy  Involves downtime between V1 complete shutdown and V2 startup  Suitable Use Cases  Monolithic legacy applications Non production environments Workers in a queue based system  "
},
{
	"uri": "/60_workshop_6_ml/50_titanic_sample_application/02_kubeflow_ui.html",
	"title": "Kubeflow UI",
	"tags": [],
	"description": "",
	"content": "To enable the mlops-user to do a kubectl port-forward:\ngit clone git@github.com:paulcarlton-ww/mlops-titanic cd mlops-titanic aws iam attach-user-policy --user-name mlops-user --policy-arn arn:aws:iam::aws:policy/AdministratorAccess eksctl create iamidentitymapping --cluster $EKS_CLUSTER_NAME --group system:masters --username mlops-user --arn $(aws iam get-user --user-name mlops-user | jq -r \u0026#39;.\u0026#34;User\u0026#34;[\u0026#34;Arn\u0026#34;]\u0026#39;) A shell script is provided to generate the commands required to run kubectl port-forward on workstation\naws-titanic/get-port-forward-cmds.sh $EKS_CLUSTER_NAME Run the commands generated on a terminal on your workstation (not cloud9 session)\nAccess UI via: http://127.0.0.1:8080/\n"
},
{
	"uri": "/50_workshop_5_accelerating_sdlc/15_deployment_strategies/02_rolling_update.html",
	"title": "Rolling Update",
	"tags": [],
	"description": "",
	"content": " The RollingUpdate deployment strategy ensures upgraded pods successfully come up before terminating older versions.\nThe following demonstrates an example of the RollingUpdate strategy. When using RollingUpdate, you can specify maxUnavailable to specify how many pods can go down as an update takes place. You can also specify maxSurge to specify how many pods can be created in addition to the desired replica count.\napiVersion: apps/v1 kind: Deployment spec: replicas: 2 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 minReadySeconds: 10 In the above sample Deployment yaml, two replicas are specified as desired number of pods.\ngraph LR; A{Deployment} -- B(Replica 1, V1); style A color:red; style B color:blue; A -- C(Replica 2, V1);  The V1 replica pods remain up while an upgraded pod attempts to come up, moving through the pending and container creating states. This meets the maxUnavailable: 0 abd maxSurge: 1 specifications.\ngraph LR; A{Deployment} -- B(Replica 1, V1) A -- C(Replica 2, V1) A -- |Pending, ContainerCreating| D(Replica 1, V2)  Once the upgraded V2 pod comes up, one of the old V1 pods can be terminated.\ngraph LR; A{Deployment} -- |Terminating| B(Replica 1, V1) A -- C(Replica 2, V1) A -- D(Replica 1, V2)  The upgrade continues to replace all the old pods. The second V2 pod attempts coming up.\ngraph LR; A{Deployment} --B(Replica 2, V1) A -- C(Replica 1, V2) A -- |Pending, ContainerCreating| D(Replica 2, V2)  The remaining old V1 pod is then terminated.\ngraph LR; A{Deployment} -- |Terminating| B(Replica 2, V1) A -- C(Replica 1, V2) A -- D(Replica 2, V2)  Desired replica count is achieved with the upgraded pods.\ngraph LR; A{Deployment} -- C(Replica 1, V2) A -- D(Replica 2, V2)  Benefits of the RollingUpdate Strategy  Low risk due to readiness checks Gradual rollout with no downtime  Drawbacks of the RollingUpdate Strategy  Needs backwards compatibility between API versions and database migrations No control over the traffic during the rollout  Suitable Use Cases  Stateful applications Stateless microservices  "
},
{
	"uri": "/50_workshop_5_accelerating_sdlc/15_deployment_strategies/03_blue_green.html",
	"title": "Blue / Green",
	"tags": [],
	"description": "",
	"content": " In Blue / Green deployments, you will have two nearly identical environments, one of which is serving live traffic (Blue). New versions of your applications can be deployed as Green. Once any testing or quality assurance has been performed on the upgraded Green environment, traffic can be routed to the Green, and the blue will eventually be scaled down.\nBenefits of Blue/Green Deployment  Avoids versioning issues Instant rollout and rollback (while the blue deployment still exists)  Drawbacks of Blue/Green Deployment  Requires resource duplication Data synchronization between the two environments can lead to partial service interruption  Suitable Use Cases  Monolithic legacy applications Autonomous microservices  "
},
{
	"uri": "/60_workshop_6_ml/50_titanic_sample_application/03_build-model.html",
	"title": "Build Model",
	"tags": [],
	"description": "",
	"content": " Install sbt curl -s \u0026#34;https://get.sdkman.io\u0026#34; | bash source \u0026#34;$HOME/.sdkman/bin/sdkman-init.sh\u0026#34; sdk install java sdk install sbt Build Spark Jars sbt clean package aws s3api put-object --bucket $BUCKET_NAME --key emr/titanic/titanic-survivors-prediction_2.11-1.0.jar --body target/scala-2.11/titanic-survivors-prediction_2.11-1.0.jar  Note: EMR has all spark libariries and this project doesn\u0026rsquo;t reply on third-party library. We don\u0026rsquo;t need to build fat jars.\n The dataset Check Kaggle Titanic: Machine Learning from Disaster for more details about this problem. 70% training dataset is used to train model and rest 30% for validation.\nA copy of train.csv is included in this repository, it needs to be uploaded to S3.\naws s3api put-object --bucket $BUCKET_NAME --key emr/titanic/train.csv --body train.csv"
},
{
	"uri": "/60_workshop_6_ml/50_titanic_sample_application/04_build-pipeline.html",
	"title": "Build Pipeline",
	"tags": [],
	"description": "",
	"content": " Install See building a pipeline to install the Kubeflow Pipelines SDK. The following command will install the tools required\nwget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.shsource /home/ec2-user/.bashrc conda create --name mlpipeline python=3.7pip3 install --user kfp --upgrade rm Miniconda3-latest-Linux-x86_64.sh  Compiling the pipeline template cd aws-titanic mkdir -p build sed s/mlops-kubeflow-pipeline-data/$BUCKET_NAME/g titanic-survival-prediction.py | sed s/aws-region/$EMR_REGION/ \u0026gt; build/titanic-survival-prediction.py dsl-compile --py build/titanic-survival-prediction.py --output build/titanic-survival-prediction.tar.gz aws s3api put-object --bucket $BUCKET_NAME --key emr/titanic/titanic-survival-prediction.tar.gz --body build/titanic-survival-prediction.tar.gz"
},
{
	"uri": "/50_workshop_5_accelerating_sdlc/15_deployment_strategies/04_canary.html",
	"title": "Canary",
	"tags": [],
	"description": "",
	"content": " In Canary deployments, you will incrementally introduce upgraded versions of your services. Upgraded versions of your service are introduced as Canary deployments, where percentages of live traffic are routed to the upgraded services. Depending on the success / failure threshold, the rollout of the upgraded service will continue through to accepting 100% of live traffic in the instance of successful metrics / requests, or rolled back in the instance of failed requests.\nBenefits of Canary Deployments  Low impact as the new version is released only to a subset of users Controlled rollout with no downtime Fast rollback  Drawbacks of Canary Deployments  Typically requires a traffic management solution / service mesh (Envoy, Istio, Linkerd, App Mesh) Requires backwards compatibility between API versions and database migrations  Suitable Use Cases  User facing applications Stateless microservices  "
},
{
	"uri": "/60_workshop_6_ml/50_titanic_sample_application/05_run.html",
	"title": "Run Pipeline",
	"tags": [],
	"description": "",
	"content": " Download the pipeline tar to workstation Using the portforward access to the Kubeflow UI the upload from URL option does not work so it is necessary to download the file to your workstation. A shell script is provided to generate the commands required.\n./get-tar-cmds.sh Run experiment Now use the kubeflow UI to upload the pipeline file and run an experiment.\nCheck results Open the Kubeflow pipelines UI. Create a new pipeline, and then upload the compiled specification (.tar.gz file) as a new pipeline template.\nOnce the pipeline done, you can go to the S3 path specified in output to check your prediction results. There\u0026rsquo;re three columes, PassengerId, prediction, Survived (Ground True value)\n... 4,1,1 5,0,0 6,0,0 7,0,0 ... Find the result file name:\naws s3api list-objects --bucket $BUCKET_NAME --prefix emr/titanic/output Download it and analyse:\nexport RESULT_FILE=\u0026lt;result file\u0026gt; aws s3api get-object --bucket $BUCKET_NAME --key emr/titanic/output/$RESULT_FILE $HOME/$RESULT_FILE grep \u0026#34;,1,1\\|,0,0\u0026#34; $HOME/$RESULT_FILE | wc -l # To count correct results wc -l $RESULT_FILE # To count items in file"
},
{
	"uri": "/categories.html",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags.html",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]